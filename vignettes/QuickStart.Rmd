---
title: "Quick Start for NPH Sample Size and Power"
author: "Keaven M. Anderson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 2
    number_sections: true
bibliography: gsDesign.bib
vignette: |
  %\VignetteIndexEntry{Quick Start for NPH Sample Size and Power}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Overview

We provide simple examples for use of the **gsdmvn** package for deriving fixed and group sequential designs under non-proportional hazards.
The piecewise model for enrollment, failure rates, dropout rates and changing hazard ratio over time allow great flexibility in design assumptions. 
Users are encouraged to suggest features that would be of immediate and long-term interest to add.

Topics included here are:

- Packages required and how they are used.
- Specifying enrollment rates.
- Specifying failure and dropout rates with possibly changing hazard ratio over time.
- Deriving a fixed design with no interim analysis.
- Simple boundary specification for group sequential design.
- Deriving a group sequential design under non-proportional hazards.
- Displaying design properties.
- Design properties under alternate assumptions.
- Differences from **gsDesign**.
- Future enhancement priorities.

All of these items are discussed briefly to enable a quick start for early adopters while also suggesting the ultimate possibilities that the software enables.
Finally, while the final section provides current enhancement priorities, potential topic-related enhancements are discussed throughout the document.

# Packages Used

- The **gsdmvn** package is used here to implement group sequential distribution theory under non-proportional hazards and to derive a wide variety of boundary types for group sequential designs.
- The **gsDesign** package is used as a check for results under proportional hazards as well as a source from deriving bounds using spending functions.
- The **gsDesign2** package provides computations to compute expected event accumulation and average hazard ratio over time; these are key inputs to the group sequential distribution parameters.
- The **simtrial** package is used to verify design properties using simulation.

The **gsdmvn** package will likely will likely be incorporated eventually into the **gsDesign2** package, resulting in a fully featured design package.
However, features and implementation in **gsdmvn** will be allowed to change as needed during the agile rapid development phase.

```{r, message=FALSE, warning=FALSE}
#library(gsdmvn)
devtools::load_all()
library(gsDesign)
library(gsDesign2)
library(simtrial)
library(knitr)
library(dplyr)
library(gt)
```

# Enrollment Rates

Piecewise constant enrollment rates are input in a tabular format.
Here we assume enrollment will ramp-up with $25\%$, $50\%$, and $75\%$ of the final enrollment rate for $2$ months each followed by a steady state $100\%$ enrollment for another $6$ months.
The rates will be increased later to power the design appropriately.
However, the fixed enrollment rate periods will remain unchanged.

```{r}
enrollRates <- tibble(
  Stratum = "All", 
  duration = c(2, 2, 2, 6), 
  rate = (1:4) / 4)
enrollRates %>% gt()
```

# Failure and Dropout Rates

Constant failure and dropout rates are specified by study period and stratum; we consider a single stratum here.
A hazard ratio is provided for treatment/control hazard rate for each period and stratum.
The dropout rate for each period is assumed the same for each treatment group; this restriction could be eliminated in a future version, if needed.
Generally, we take advantage of the identity for an exponential distribution with median $m$, the corresponding failure rate $\lambda$ is

$$\lambda = \log(2) / m.$$

We consider a control group exponential time-to-event with a $12$ month median.
We assume a hazard ratio of $1$ for $4$ months, followed by a hazard ratio of $0.6$ thereafter.
Finally, we assume a low $0.001$ exponential dropout rate for both treatment groups.

```{r}
medianSurv <- 12
failRates <- tibble::tibble(
  Stratum = "All",
  duration = c(4, Inf),
  failRate = log(2) / medianSurv,
  hr = c(1, .6),
  dropoutRate = .001
)
failRates %>% gt()
```

# Fixed Design

Under the above enrollment, failure and dropout rate assumptions we now derive sample size for a trial targeted to complete in 36 months with no interim analysis, $90\%$ power and $2.5\%$ Type I error.
The parameter `upar = qnorm(1 - .025)` in this case is the upper bound for the single analysis, while the parameter `beta = 0.1` is the Type II error (1 - power).
The information fraction `IF = 1` at the final analysis is just a statement that the analysis is done with $100\%$ of the design planned endpoints.
Finally, `lpar = -Inf` just means there is no futility bound for the design.
The design can be derived under these assumptions.

```{r}
# Type I error
alpha <- .025
design <- gs_design_ahr(
  enrollRates = enrollRates,
  failRates = failRates,
  alpha = alpha,
  beta = .1,               # Type II error = 1 - power
  analysisTimes = 36,      # Planned trial duration
  IF = 1,                  # Single analysis at information-fraction of 1
  upar = qnorm(1 - alpha), # Final analysis bound
  lpar = -Inf              # No futility bound
  )
```

There are four components to the resulting design.

```{r}
names(design)
```

First, the enrollment rates for each period have been increased proportionately to size the trial for the desired properties; the duration for each enrollment rate has not changed.

```{r}
design$enrollRates %>% gt()
```

Second, the output `failRates` are the same as input. 

Third, for the output `bounds`, there are commonly 7 columns.

- `Analysis` is the index of the analysis. For the fixed design, we only have Analysis = 1.
- `Bound` specify the type of the bounds, either upper bound or lower bound. In this example, we do not have lower bound since there is not a futility bound.
- `Probability`. The power is in the `Probability` column on the row with the `Upper` bound and `H1` hypothesis. 
- `hypothesis` specifies whether it is under the null or alternative hypothesis.
- `Z` is the Z-statistics. In this example, the output bounds are just an upper bound of $\Phi^{-1}(1-\alpha)$ and a lower bound of $-\infty$.
- `~HR at bound` is the approximated hazard ratio at the bound, which is calculated as `exp(-Z / sqrt(info0))`, where `info0` is the statistical information under H0.
- `Nominal p` is calculated as `pnorm(-Z)`.

```{r}
design$bounds %>% gt()
```

Fourth, for the output `analysis`, it summarize each analysis per hypothesis. And there are 9 columns.

- `Analysis` is the index of the analysis. For the fixed design, we only have Analysis = 1.
- `Time` is the targeted time of the analysis.
- `N` is the sample size of the anslysis.
- `Events` is the number of events of the analysis.
- `AHR` is average hazard ratio used to compute the sample size based on the @Schoenfeld1981 approximation is in `AHR`. 
- `theta` is the natural parameter for the effect size as outlined in @JTBook and elsewhere in this package; in this case it is `-log(AHR)`.
- `info` is the statistical information at each analysis under the alternate (if the row of `hypothesis` is H1) and null hypothesis (if the row of `hypothesis` is H0), respectively.
- `IF` is the information fraction.
- `hypothesis` specifies whether it is under the null or alternative hypothesis.

```{r}
design$analysis %>% gt()
```

We note that the targeted `Events` are approximately what would be proposed with the @Schoenfeld1981 formula:

```{r}
gsDesign::nEvents(hr = (design$analysis %>% filter(hypothesis == "H1"))$AHR)
```

The difference is that `gs_design_ahr()` accounts for both a null and alternate hypothesis variance estimate for `theta = log(AHR)` at each analysis to yield a slightly more conservative event target due to slower accumulation of statistical information under the alternate hypothesis. 
See @LachinBook for this approach for fixed design; to our knowledge, this has not previously been extended to group sequential design. 
Due to simplicity, there may be reasons to allow approaches with a single variance estimate in future releases.

Finally, we note that with a shorter trial duration of $30$ months, we need both a larger sample size and targeted number of events due to a larger expected AHR at the time of analysis:

```{r}
gs_design_ahr(
  enrollRates = enrollRates,
  failRates = failRates,
  alpha = alpha,
  beta = .1,               # Type II error = 1 - power
  analysisTimes = 30,      # Planned trial duration, which is different from the previous design 36->30
  IF = 1,                  # single analysis at information-fraction of 1
  upar = qnorm(1 - alpha), # Final analysis bound
  lpar = -Inf              # No futility bound
  )$analysis %>% 
  gt() %>% 
  fmt_number(columns = 5:7, decimals = 4)
```

# Group Sequential Design

We will not go into detail for group sequential designs here.
In brief, however, a sequence of tests $Z_1, Z_2,\ldots, Z_K$ that follow a multivariate normal distribution are performed to test if a new treatment is better than control (@JTBook).
We assume $Z_k > 0$ is favorable for the experimental treatment.
Generally Type I error for this set of tests will be controlled under the null hypothesis of no treatment difference by a sequence of bounds $b_1, b_2,\ldots,b_K$ such that for a chosen Type I error $\alpha > 0$ we have

$$
  \alpha = 1 - P_0(\cap_{k=1}^K Z_k < b_k)
$$
Where $P_0()$ refers to a probability under the null hypothesis.
This is referred to as a non-binding bound since it is assumed the trial will not be stopped early for futility if some $Z_k$ is small.

## Simple Efficacy Bound Definition

@LanDeMets developed the spending function method for deriving group sequential bounds.
This involves use of a non-decreasing spending function $f(t)$ for $t\ge 0$ where $f(0)=0$ and $f(t)=\alpha$ for $t \ge 1$.
Suppose for $K>0$ analyses are performed when proportion $t_1< t_2 <\ldots t_K=1$ of some planned statistical information (e.g., proportion of planned events for a time-to-event endpoint trial for proportion of observations for a binomial or normal endpoint). 
Bounds through the first $k$ analyses $1\le k\le K$ are recursively defined by the spending function and the multivariate normal distribution to satisfy

$$
  f(t_k) = 1 - P_0(\cap_{j=1}^k Z_j < b_j).
$$
For this quick start, we will only illustrate this type of efficacy bound.

Perhaps the most common spending function for this approach is the @LanDeMets approximation to the O'Brien-Fleming bound with

$$
  f(t) = 2-2\Phi\left(\frac{\Phi^{-1}(1-\alpha/2)}{t^{1/2}}\right).
$$

```{r, echo=FALSE, fig.width=6.5}
ggplot(
  data = tibble(t = (0:50) / 50, `f(t)` = 2 - 2 * pnorm(qnorm(1 - .0125) / sqrt(t))),
  aes(x = t, y = `f(t)`)) +
  geom_line()
```

Suppose $K=3$ and $t_1=0.5$, $t_2 = 0.75$, $t_3 = 1$.
We can define bounds with the **gsDesign** group sequential design function `gsDesign()` and Lan-DeMets O'Brien-Fleming spending function for $\alpha = 0.025$.

```{r}
gsDesign(
  k = 3, 
  timing = c(0.5, 0.75, 1), 
  test.type = 1, 
  alpha = 0.025, 
  sfu = gsDesign::sfLDOF)$upper$bound
```

Now we can define a one-sided group sequential design under the same enrollment, failure and dropout assumptions used previously.

```{r}
design1s <- gs_design_ahr(
  enrollRates = enrollRates,
  failRates = failRates,
  analysisTimes = 36,  # Trial duration
  upper = gs_spending_bound,
  upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
  lower = gs_b,
  lpar = rep(-Inf, 3), # No futility bound
  IF = c(.5, .75, 1)
)
```

Bounds at the 3 analyses are as follows. 
Note that expected sample size at time of each data cutoff for analysis is also here in `N`. We filter on the upper bound so that lower bounds with `Z = -Inf` are not shown.

```{r}
summary_bound(design1s) %>%
  filter(Bound == "Efficacy") %>% 
  gt() %>% 
  fmt_number(columns = 3:6, decimals = 4)
```


## Two-Sided Testing

We will consider both symmetric and asymmetric 2-sided designs.

### Symmetric 2-sided bounds

For a symmetric design, we can again define a bound using `gsDesign::gsDesign()`.
For this example, the bound is identical to `b` above to the digits calculated.

```{r}
gsDesign::gsDesign(
  test.type = 2, 
  sfu = sfLDOF, 
  alpha = 0.025, 
  timing = c(.5, .75, 1))$upper$bound
```

Now we replicate with `gs_design_ahr()`:

```{r}
design2ss <- gs_design_ahr(
  enrollRates = enrollRates,
  failRates = failRates,
  analysisTimes = 36, # Trial duration
  IF = c(.5, .75, 1), # Information fraction at analyses
  upper = gs_spending_bound,
  upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
  lower = gs_spending_bound,
  lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
  h1_spending = FALSE
)
```


Design bounds are confirmed with: 

```{r, message=FALSE}
summary_bound(design2ss) %>% 
  gt() %>% 
  fmt_number(columns = 3:6, decimals = 4)
  #$bounds %>% kable(digits = c(0, 0, 4, 0, 4, 4, 4))
```

The bounds can be plotted easily:

```{r,fig.width=6.5}
ggplot(
  data = design2ss$analysis %>% 
    subset(hypothesis == "H1") %>% 
    left_join(design2ss$bounds %>% subset(hypothesis == "H1")), 
  aes(x = Events, y = Z, group = Bound)) +
  geom_line(aes(linetype = Bound)) +
  geom_point() +
  ggtitle("2-sided symmetric bounds with O'Brien-Fleming-like spending")
```

### Asymmetric 2-sided bounds

Asymmetric 2-sided designs are more common than symmetric since the objectives of the two bounds tend to be different.
There is often caution to analyze early for efficacy or to use other than a conservative bound; both of these principles have been used with the example designs so far.
Stopping when there is a lack of benefit for experimental treatment over control or for an overt indication of an unfavorable trend generally might be examined early and bounds be less stringent.
We will add an early futility analysis where if there is a nominal 1-sided p-value of $0.05$ in the wrong direction ($Z=\Phi^{-1}(0.05)$ after 30% or $50\%$ of events have accrued.
This might be considered a *disaster check*. After this point in time, there may not be a perceived need for further futility analysis.
For efficacy, we add an infinite bound at this first interim analysis.

```{r}
b2sa <- c(Inf, 
          gsDesign(k = 3, 
                   timing = c(0.5, 0.75, 1), 
                   test.type = 1, 
                   alpha = 0.025, 
                   sfu = gsDesign::sfLDOF)$upper$bound)   # Same efficacy bound as before
a2sa <- c(rep(qnorm(.05), 2), rep(-Inf, 2))               # Single futility analysis bound

design2sa <- gs_design_nph(
  enrollRates = enrollRates,
  failRates = failRates,
  analysisTimes = 36,                                     # Trial duration
  upper = gs_b, 
  upar = b2sa,                                            # Same efficacy bound as before
  lower = gs_b, 
  lpar = a2sa,                                            # Asymmetric 2-sided bound
  IF = c(.3, .5, .75, 1)
)
```

We now have a slightly larger sample size to account for the possibility of an early futility stop.

```{r}
design2sa$enrollRates %>% summarise(N = ceiling(sum(rate * duration)))
```

Bounds are now:

```{r}
design2sa$bounds %>%
  filter(abs(Z) < Inf) %>%
  gt()
  #kable(digits = c(0, 0, 2, 4, 2, 1, 2, 1, 1, 1))
```

We see that there does not need to be a (finite) stopping rule for each bound at each analysis.
That is, there is a futility bound only at the first 2 analyses and an efficacy bound only for the last 3 analyses.
We still have the targeted power. 
The efficacy bound has not changed from our first design.
The reason for not changing it is to address regulator concerns that such bounds are not always stopped for.
However, if the bound is obeyed, the Type I error can be seen to be slightly reduced as follows.
This price is generally acceptable for regulatory acceptance and operational flexibility that is enabled by controlling Type I error even if the futility bound is not obeyed.

```{r}
events <- (design2sa$bounds %>% filter(Bound == "Upper"))$Events
gs_power_nph(
  enrollRates = design1s$enrollRates,
  failRates = design2sa$failRates %>% mutate(hr = 1),
  upar = b2sa,
  lpar = a2sa,
  events = events,
  maxEvents = max(events)
) %>%
  # filter eliminates bounds that are infinite
  filter(abs(Z) < Inf) %>% 
  gt() %>% 
  fmt_number(columns = 3:10, decimals = 4)
  #kable(digits = c(0, 0, 4, 4, 1, 1, 2, 2, 1, 1))
```

# Confirmation by Simulation

We do a small simulation to approximate the boundary crossing probabilities just shown in the 2-sided asymmetric design `design2sa`.
First, we generate test statistics for each analysis for a number of simulated trials.

```{r}
fr <- simfix2simPWSurv(failRates = failRates)
nsim <- 200                                # Number of trial simulations
simresult <- NULL
N <- ceiling(design2sa$enrollRates %>% summarize(N = sum(rate / 2 * duration))) * 2
K <- max(design2sa$bounds$Analysis)
events <- ceiling(sort(unique(design2sa$bounds$Events)))

for (i in 1:nsim) {
  
  sim <- simPWSurv(
    n = as.numeric(N),
    enrollRates = design2sa$enrollRates,
    failRates = fr$failRates,
    dropoutRates = fr$dropoutRates
  )
  
  for (k in 1:K) {
    Z <- sim %>%
      cutDataAtCount(events[k]) %>%          # Cut simulation for analysis at targeted events
      tensurv(txval = "Experimental") %>%
      tenFH(rg = tibble(rho = 0, gamma = 0))
    
    simresult <- rbind(
      simresult,
      tibble(sim = i, k = k, Z = -Z$Z)       # Change sign for Z
    )
  }
}
```

Now we analyze the individual trials and summarize results. A larger simulation would be required to more accurately assess the asymptotic approximation for boundary crossing probabilities.

```{r}
bds <- tibble(
  k = sort(unique(design2sa$bounds$Analysis)),
  upper = (design2sa$bounds %>% filter(Bound == "Upper"))$Z,
  lower = (design2sa$bounds %>% filter(Bound == "Lower"))$Z
)

trialsum <- simresult %>%
  full_join(bds, by = "k") %>%
  filter(Z < lower | Z >= upper | k == K) %>%
  group_by(sim) %>%
  slice(1) %>%
  ungroup()

trialsum %>% 
  summarize(
    nsim = n(),
    "Early futility (%)" = 100 * mean(Z < lower),
    "Power (%)" = 100 * mean(Z >= upper)
)
```

```{r}
xx <- trialsum %>% mutate(Positive = (Z >= upper))
table(xx$k, xx$Positive) 
```

## Differences From **gsDesign**

The sample size computation from the **gsdmvn** package is slightly different than from the **gsDesign** package for proportional hazards model.
We demonstrate this with the bounds for the 1-sided test above under a proportional hazards model with an underlying hazard ratio of 0.7.

```{r}
design1sPH <- gs_design_nph(
  enrollRates = enrollRates,
  failRates = failRates %>% mutate(hr = .7),
  analysisTimes = 36,  # Trial duration
  upar = gsDesign(k = 3, timing = c(0.5, 0.75, 1), test.type = 1, alpha = 0.025, sfu = sfLDOF)$upper$bound,
  lpar = rep(-Inf, 3), # No futility bound
  IF = c(.5, .75, 1)
)
design1sPH$enrollRates %>% gt()
```

This results in total sample size:

```{r}
design1sPH$enrollRates %>%
  tail(1) %>%
  select(N) %>% 
  gt()
```

Now we derive a design with the same targeted properties using the **gsDesign** package.

```{r,warning=FALSE}
x <- gsSurv(
  k = 3,
  test.type = 1,
  alpha = .025,
  beta = .1,
  timing = c(.5, .75, 1),
  sfu = sfLDOF,
  lambda = log(2) / medianSurv,
  hr = .7,
  eta = .001, # Dropout rate
  gamma = c(.25, .5, .75, 1),
  R = c(2, 2, 2, 6),
  minfup = 24,
  T = 36
)
gsBoundSummary(x) %>% gt()
```

Enrollment rates are:

```{r}
x$gamma %>% kable()
```

# References
