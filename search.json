[{"path":"https://merck.github.io/gsdmvn/articles/DesignWithAverageHazardRatio.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Design Using Average Hazard Ratio","text":"Jennison Turnbull (2000)","code":""},{"path":"https://merck.github.io/gsdmvn/articles/DesignWithAverageHazardRatio.html","id":"scenarios","dir":"Articles","previous_headings":"","what":"Scenarios","title":"Design Using Average Hazard Ratio","text":"Expected enrollment duration 24 months piecewise constant enrollment rates escalating every 2 months month 6 enrollment assumed reached steady state. alternate scenarios, enrollment occur faster planned rate. scenarios, assume initial illustrations total enrollment 100 subjects. consider following failure rate assumptions: Control group exponential failure rate median 12 months. Constant hazard ratio 0.7 (experimental/control). Control group exponential failure rate median 10 months. Hazard ratio 1 6 months followed hazard ratio 0.575. Control group exponential failure rate median 10 months. Hazard ratio 1.5 4 months followed hazard ratio 0.5. Survival curves 3 scenarios shown :","code":"# 6 month ramp-up of enrollment, 24 months enrollment time target enroll24 <- tibble::tibble(Stratum = rep(\"All\",4),                             duration = c(rep(2,3), 18),                            rate = 1:4) # Set rates to enroll 100 subjects N <- sum(enroll24$duration * enroll24$rate) enroll24$rate <- enroll24$rate * 100 / N  # Enroll in 16 months, same ramp-up enroll16 <- tibble::tibble(Stratum = rep(\"All\",4),                             duration = c(rep(2,3), 12),                            rate = 1:4) # Set rates to enroll 100 subjects N <- sum(enroll16$duration * enroll16$rate) enroll16$rate <- enroll16$rate * 100 / N # Put these in a single tibble by scenario # We will use 16 month enrollment for delayed effect and crossing hazards # scenarios enrollRates <- rbind(enroll24 %>% mutate(Scenario = \"PH\"),                      enroll24 %>% mutate(Scenario = \"Delayed effect 1\"),                      enroll16 %>% mutate(Scenario = \"Delayed effect 2\"),                      enroll16 %>% mutate(Scenario = \"Crossing\") ) Month <- c(0,4,6,44) duration <- Month - c(0,Month[1:3]) control_rate <- log(2) / c(rep(16,4), rep(14, 4), rep(14, 4)) s <- tibble::tibble(Scenario = c(rep(\"PH\",4), rep(\"Delayed effect\", 4), rep(\"Crossing\", 4)),                     Treatment = rep(\"Control\", 12),                     Month = rep(Month, 3),                     duration = rep(duration, 3),                     rate = control_rate,                     hr = c(rep(.7, 4), c(1, 1, 1, .575), c(1.5,1.5, .5, .5)), ) s <- rbind(s,            s %>% mutate(Treatment = \"Experimental\",                         rate = rate * hr) ) %>%   group_by(Scenario, Treatment) %>%   mutate(Survival = exp(-cumsum(duration * rate))) ggplot(s, aes(x = Month, y = Survival, col = Scenario, lty = Treatment)) +    geom_line() +   scale_y_log10(breaks = (1:10) /10, lim=c(.1,1))+   scale_x_continuous(breaks = seq(0,42, 6))"},{"path":"https://merck.github.io/gsdmvn/articles/DesignWithAverageHazardRatio.html","id":"average-hazard-ratio","dir":"Articles","previous_headings":"","what":"Average Hazard Ratio","title":"Design Using Average Hazard Ratio","text":"","code":"# Durations to be used in common for all failure rate scenarios dur <- c(4,2,100) # Exponential failure, proportional hazards failRates <- rbind(tibble(Scenario = \"PH\", Stratum = \"All\",                            duration = dur, failRate = log(2) / 14,                           hr = 0.7, dropoutRate = .001),                    tibble(Scenario = \"Delayed effect 1\", Stratum = \"All\",                            duration = dur, failRate = log(2) / 11,                           hr = c(1, .6, .6), dropoutRate = .001),                    tibble(Scenario = \"Delayed effect 2\", Stratum = \"All\",                            duration = dur, failRate = log(2) / 11,                           hr = c(1, 1, .7), dropoutRate = .001),                    tibble(Scenario = \"Crossing\", Stratum = \"All\",                            duration = dur, failRate = log(2) / 11,                           hr = c(1.5, .6,.6), dropoutRate = .001) ) hr <- NULL for(g in c(\"PH\", \"Delayed effect 1\", \"Delayed effect 2\", \"Crossing\")){   hr <-      rbind(hr,         AHR(enrollRates = enrollRates %>% filter(Scenario == g),              failRates = failRates %>% filter(Scenario == g),             totalDuration = c(.001, seq(4, 44, 4))         ) %>%         mutate(Scenario = g)   ) } ggplot(hr, aes(x=Time, y=AHR, col = Scenario)) + geom_line() + scale_x_continuous(breaks = seq(0, 42, 6)) ggplot(hr, aes(x=Time, y=`Events`, col = Scenario)) + geom_line() + scale_x_continuous(breaks = seq(0, 42, 6))"},{"path":[]},{"path":"https://merck.github.io/gsdmvn/articles/DesignWithAverageHazardRatio.html","id":"fixed-design-using-ahr-and-logrank","dir":"Articles","previous_headings":"Sample Size and Events by Scenario","what":"Fixed design using AHR and logrank","title":"Design Using Average Hazard Ratio","text":"power fixed design 90% 2.5% one-sided Type error different scenarios consideration. Assuming delayed effect 1 primary scenario wish protect power, long trial optimize tradeoffs sample size, AHR events required? inform tradeoff looking sizing trial different assumed trial durations failure rates assumed relative enrollment rates. counts events required perhaps interesting 24 month trial requires almost twice events powered 90% compared trial 42 months duration. study, consider 36 month trial duration reasonable tradeoff time, sample size power presumed delayed effect 4 months followed hazard ratio 0.6 thereafter.","code":"ss_ahr_fixed <- NULL for(g in c(\"PH\", \"Delayed effect 1\",\"Delayed effect 2\", \"Crossing\")){     ss_ahr_fixed <-        rbind(ss_ahr_fixed,       gs_design_ahr(enrollRates = enrollRates %>% filter(Scenario == g),                     failRates = failRates %>% filter(Scenario == g),                     analysisTimes = 36,                     upper = gs_b, upar = qnorm(.975),                     lower = gs_b, lpar = -Inf,                     alpha = .025,                     beta = .1                     )$bounds %>% mutate(Scenario = g)       ) } ss_ahr_fixed %>% select(Time, N, Events, AHR, Scenario) %>%    gt() %>% fmt_number(columns=1:3,decimals = 0) %>% fmt_number(columns = 4, decimals = 3)  %>%   tab_header(title = \"Sample Size and Events Required by Scenario\",              subtitle = \"36 Month Trial duration, 2.5% One-sided Type 1 Error, 90% Power\") ss_ahr_fixed <- NULL g <- \"Delayed effect 1\" for(trialEnd in c(24,30,36,42)){     ss_ahr_fixed <-        rbind(ss_ahr_fixed,       gs_design_ahr(enrollRates = enrollRates %>% filter(Scenario == g),                     failRates = failRates %>% filter(Scenario == g),                     analysisTimes = trialEnd,                     upper = gs_b, upar = qnorm(.975),                     lower = gs_b, lpar = -Inf,                     alpha = .025,                     beta = .1                     )$bounds %>% mutate(Scenario = g)       ) } ss_ahr_fixed %>% select(Time, N, Events, AHR, Scenario) %>%    gt() %>% fmt_number(columns=1:3,decimals = 0) %>% fmt_number(columns = 4, decimals = 3) %>%   tab_header(title = \"Sample Size and Events Required by Trial Duration\",              subtitle = \"Delayed Effect of 4 Months, HR = 0.6 Thereafter; 90% Power\")"},{"path":"https://merck.github.io/gsdmvn/articles/DesignWithAverageHazardRatio.html","id":"alternate-hypothesis-mapping","dir":"Articles","previous_headings":"Sample Size and Events by Scenario","what":"Alternate hypothesis mapping","title":"Design Using Average Hazard Ratio","text":"Experimental version AHR(). different scenarios interest, can examine expected number events time periods interest. Recall alternate hypothesis assumes treatment effect (HR=1) 4 months HR = 0.6 thereafter. scenarios, wish base futility bound assumption plus number events first 4 months 4 months, can compute average hazard ratio alternate hazard ratio scenario 20 months follows. can see interim futility spending bound based alternate hypothesis can depend fairly heavily enrollment control failure rate. Note also time interim analysis, alternate hypothesis AHR can computed fashion based observed events time period. Note can quite different scenario HR; e.g., PH, assume HR=0.7 throughout, futility bound comparison, compute blinded AHR decreases analysis alternate hypothesis.","code":"AHRx <- function(enrollRates=tibble::tibble(Stratum=\"All\",                                            duration=c(2,2,10),                                            rate=c(3,6,9)),                 failRates=tibble::tibble(Stratum=\"All\",                                          duration=c(3,100),                                          failRate=log(2)/c(9,18),                                          hr=c(.9,.6),                                          dropoutRate=rep(.001,2)),                 totalDuration=30,                 ratio=1,                 simple=TRUE ){   # check input values   # check input enrollment rate assumptions   if(max(names(enrollRates)==\"Stratum\") != 1){stop(\"gsDesign2: enrollRates column names in `AHR()` must contain stratum\")}   if(max(names(enrollRates)==\"duration\") != 1){stop(\"gsDesign2: enrollRates column names in `AHR()` must contain duration\")}   if(max(names(enrollRates)==\"rate\") != 1){stop(\"gsDesign2: enrollRates column names in `AHR()' must contain rate\")}    # check input failure rate assumptions   if(max(names(failRates)==\"Stratum\") != 1){stop(\"gsDesign2: failRates column names in `AHR()` must contain stratum\")}   if(max(names(failRates)==\"duration\") != 1){stop(\"gsDesign2: failRates column names in `AHR()` must contain duration\")}   if(max(names(failRates)==\"failRate\") != 1){stop(\"gsDesign2: failRates column names in `AHR()` must contain failRate\")}   if(max(names(failRates)==\"hr\") != 1){stop(\"gsDesign2: failRates column names in `AHR()` must contain hr\")}   if(max(names(failRates)==\"dropoutRate\") != 1){stop(\"gsDesign2: failRates column names in `AHR()` must contain dropoutRate\")}    # check input trial durations   if(!is.numeric(totalDuration)){stop(\"gsDesign2: totalDuration in `AHR()` must be a non-empty vector of positive numbers\")}   if(!is.vector(totalDuration) > 0){stop(\"gsDesign2: totalDuration in `AHR()` must be a non-empty vector of positive numbers\")}   if(!min(totalDuration) > 0){stop(\"gsDesign2: totalDuration in `AHR()` must be greater than zero\")}   strata <- names(table(enrollRates$Stratum))   strata2 <- names(table(failRates$Stratum))   length(strata) == length(strata2)   for(s in strata){     if(max(strata2==s) != 1){stop(\"gsDesign2: Strata in `AHR()` must be the same in enrollRates and failRates\")}   }   # check input simple is logical   if(!is.logical(simple)){stop(\"gsDesign2: simple in `AHR()` must be logical\")}    # compute proportion in each group   Qe <- ratio / (1 + ratio)   Qc <- 1 - Qe    # compute expected events by treatment group, stratum and time period   rval <- NULL   for(td in totalDuration){     events <- NULL     for(s in strata){       # subset to stratum       enroll <- enrollRates %>% filter(Stratum==s)       fail <- failRates %>% filter(Stratum==s)       # Control events       enrollc <- enroll %>% mutate(rate=rate*Qc)       control <- eEvents_df(enrollRates=enrollc,failRates=fail,totalDuration=td,simple=FALSE)       # Experimental events       enrolle <- enroll %>% mutate(rate=rate*Qe)       fre <- fail %>% mutate(failRate=failRate*hr)       experimental <- eEvents_df(enrollRates=enrolle,failRates=fre,totalDuration=td,simple=FALSE)       # Combine control and experimental; by period recompute HR, events, information       events <-         rbind(control %>% mutate(Treatment=\"Control\"),               experimental %>% mutate(Treatment=\"Experimental\")) %>%         arrange(t, Treatment) %>% ungroup() %>% group_by(t) %>%         summarize(Stratum = s, info = (sum(1 / Events))^(-1),                   Events = sum(Events), HR = last(failRate) / first(failRate)         ) %>%         rbind(events)     }     rval <- rbind(rval,                   events %>%                     mutate(Time=td, lnhr = log(HR), info0 = Events * Qc * Qe) %>% # NEXT 2 lines are the only changes from AHR()                     ungroup() %>% group_by(Stratum, t) %>%                     summarize(Time = td, Events = sum(Events), HR=first(HR), lnhr=first(lnhr), info0 = sum(info0), info = sum(info)) %>% ungroup()     )   }    if(!simple) return(rval %>% select(c(\"Time\", \"Stratum\", \"t\", \"HR\", \"Events\", \"info\", \"info0\")) %>%                         group_by(Time, Stratum) %>% arrange(t, .by_group = TRUE))   return(rval %>%            group_by(Time) %>%            summarize(AHR = exp(sum(log(HR)*Events)/sum(Events)),                      Events = sum(Events),                      info = sum(info),                      info0 = sum(info0))   ) } events_by_time_period <- NULL for(g in c(\"PH\", \"Delayed effect 1\",\"Delayed effect 2\", \"Crossing\")){     events_by_time_period <-        rbind(events_by_time_period,       AHRx(enrollRates = enrollRates %>% filter(Scenario == g),           failRates = failRates %>% filter(Scenario == g),           totalDuration = c(12, 20, 28, 36), simple = FALSE) %>% mutate(Scenario = g)       ) } ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. ## `summarise()` has grouped output by 'Stratum'. You can override using the ## `.groups` argument. events_by_time_period %>% gt() # Time periods for each scenario were 0-4, 4-6, and 6+ # Thus H1 has HR as follows hr1 <- tibble(t = c(0, 4, 6), hr1 = c(1, .6, .6)) ahr_by_analysis <-   events_by_time_period %>%    full_join(hr1) %>%   group_by(Scenario, Time) %>%   summarize(AHR1 = exp(sum(Events * log(hr1))/ sum(Events))) ## Joining, by = \"t\" ## `summarise()` has grouped output by 'Scenario'. You can override using the ## `.groups` argument. ahr_by_analysis %>%    pivot_wider(names_from = Scenario, values_from = AHR1) %>%    gt() %>% fmt_number(columns=2:5, decimals = 3)"},{"path":"https://merck.github.io/gsdmvn/articles/DesignWithAverageHazardRatio.html","id":"group-sequential-design","dir":"Articles","previous_headings":"","what":"Group sequential design","title":"Design Using Average Hazard Ratio","text":"assume design delayed effect model delay long long-term average hazard ratio benefit strong. proportional hazards scenario, look power alternate scenarios. plan 36 month group sequential design delayed effect 1 scenario. Interim analyses planned 12, 20, 28 months. scenario, now wish compute adjusted expected futility bounds power implied.","code":"analysisTimes <- c(12, 20, 28, 36) upar <- list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL, theta=0) lpar <- list(sf = gsDesign::sfHSD, total_spend = .1, param = -2, timing = NULL, theta=NULL) NPHasymmetric <- gs_design_ahr(enrollRates = enrollRates,                                failRates = failRates,                                ratio = 1, alpha = .025, beta = 0.1,                                # Information fraction not required (but available!)                                analysisTimes = analysisTimes,                                # Function to enable spending bound                                upper = gs_spending_bound, lower = gs_spending_bound,                                # Spending function and parameters used                                upar = upar, lpar = lpar ) NPHasymmetric$bounds ## # A tibble: 8 × 11 ##   Analysis Bound  Time     N Events      Z  Probability   AHR theta  info info0 ##      <int> <chr> <dbl> <dbl>  <dbl>  <dbl>        <dbl> <dbl> <dbl> <dbl> <dbl> ## 1        1 Upper    12  231.   40.3  6.58  0.0000000236   0.7 0.357  9.82  10.1 ## 2        2 Upper    20  436.  125.   3.62  0.0492         0.7 0.357 30.5   31.1 ## 3        3 Upper    28  577.  236.   2.52  0.577          0.7 0.357 58.1   59.1 ## 4        4 Upper    36  769.  358.   1.99  0.900          0.7 0.357 88.2   89.4 ## 5        1 Lower    12  231.   40.3 -1.54  0.00391        0.7 0.357  9.82  10.1 ## 6        2 Lower    20  436.  125.  -0.265 0.0156         0.7 0.357 30.5   31.1 ## 7        3 Lower    28  577.  236.   0.899 0.0429         0.7 0.357 58.1   59.1 ## 8        4 Lower    36  769.  358.   1.99  0.100          0.7 0.357 88.2   89.4 xx <- NULL lparx <- lpar for(g in c(\"PH\", \"Delayed effect 1\",\"Delayed effect 2\", \"Crossing\")){   AHR1 <- (filter(ahr_by_analysis, Scenario == g))$AHR1   lparx$theta1 <- -log(AHR1)   yy <- gs_power_ahr(enrollRates = enrollRates %>% filter(Scenario == g),                failRates = failRates %>% filter(Scenario == g),                 events = NULL,                analysisTimes = c(12,20,28,36),                upper = gs_spending_bound,                upar = upar,                lower = gs_spending_bound,                lpar = lparx)   xx <- rbind(xx, yy %>% mutate(Scenario = g)) }"},{"path":"https://merck.github.io/gsdmvn/articles/DesignWithAverageHazardRatio.html","id":"weighted-logrank","dir":"Articles","previous_headings":"Group sequential design","what":"Weighted logrank","title":"Design Using Average Hazard Ratio","text":"","code":"ss_FH05_fixed <- NULL g <- \"Delayed effect\" # for(g in c(\"PH\", \"Delayed effect\", \"Crossing\")){ #     ss_FH05_fixed <-  #       rbind(ss_FH05_fixed,     gs_arm <- gsdmvn:::gs_create_arm(enrollRates, failRates %>% filter(Scenario == g),                                       ratio = 1,       # Randomization ratio                                      total_time = 44) # Total study duration     arm0 <- gs_arm[[\"arm0\"]]     arm1 <- gs_arm[[\"arm1\"]]     npsurvSS::size_two_arm(arm0, arm1, power = 0.9, alpha = 0.025,                             test = list(test=\"weighted logrank\", weight = \"FH_p.5_q0\"))          npsurvSS::size_two_arm(arm0, arm1, power = 0.9, alpha = 0.025,                         test = list(test=\"rmst difference\",                         # Milestone allow user to define RMST cutpoint                        milestone = 44)) # 42 selected since better than 40 or 44     gsdmvn:::gs_design_wlr(enrollRates = enrollRates,                             failRates = failRates %>% filter(Scenario == g),                             weight = function(x, arm0, arm1){                                       gsdmvn:::wlr_weight_fh(x, arm0, arm1,                                                               rho = 0, gamma = 0.5, tau = 4)},                            alpha = .025, beta = .1,                            upar = qnorm(.975),                            lpar = -Inf,                            analysisTimes = 44)$bounds %>% filter(Bound ==\"Upper\") # Ignore tau or (tau can be -1)  gsdmvn:::gs_design_wlr(enrollRates = enrollRates,                         failRates = failRates %>% filter(Scenario == g),                         weight = function(x, arm0, arm1){                                   gsdmvn:::wlr_weight_fh(x, arm0, arm1,                                                           rho = 0, gamma = 0.5)},                        alpha = .025, beta = .1,                        upar = qnorm(.975),                        lpar = -Inf,                        analysisTimes = 44)$bounds %>% filter(Bound ==\"Upper\") # MaxCombo    MC_test <- data.frame(rho = c(0, 0, .5), gamma = c(0, .5, .5), tau = -1,                       test = 1:3,                       Analysis = 1,                       analysisTimes = 44)    gs_design_combo(enrollRates,                failRates %>% filter(Scenario == g),                MC_test,                alpha = 0.025,                beta = 0.1,                ratio = 1,                binding = FALSE,                upper = gs_spending_combo,                upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),                lower = gs_spending_combo,                lpar = list(sf = gsDesign::sfLDOF, total_spend = .1)                ) }"},{"path":"https://merck.github.io/gsdmvn/articles/DesignWithAverageHazardRatio.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Design Using Average Hazard Ratio","text":"Jennison, Christopher, Bruce W. Turnbull. 2000. Group Sequential Methods Applications Clinical Trials. Boca Raton, FL: Chapman; Hall/CRC.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/DesignWithSpending.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Trial design with spending under NPH","text":"vignette covers implement designs trials spending assuming non-proportional hazards. primarily concerned practical issues implementation rather design strategies, ignore design strategy.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/DesignWithSpending.html","id":"scenario-for-consideration","dir":"Articles","previous_headings":"","what":"Scenario for consideration","title":"Trial design with spending under NPH","text":"set enrollment, failure dropout rates along assumptions enrollment duration times analyses.","code":"library(gsdmvn) library(dplyr) library(tibble) library(gsDesign) library(gsDesign2) analysisTimes <- c(18, 24, 30, 36) enrollRates <- tibble::tibble(   Stratum = \"All\",   duration = c(2, 2, 2, 6),   rate = c(8, 12, 16, 24) ) failRates <- tibble::tibble(   Stratum = \"All\",   duration = c(3, 100),   failRate = log(2) / c(8, 14),   hr = c(.9, .6),   dropoutRate = .001 )"},{"path":"https://merck.github.io/gsdmvn/articles/DesignWithSpending.html","id":"deriving-power-for-a-given-sample-size","dir":"Articles","previous_headings":"","what":"Deriving power for a given sample size","title":"Trial design with spending under NPH","text":"derive statistical information targeted analysis times. Now can examine power using gs_power_npe():","code":"xx <- gsDesign2::AHR(enrollRates = enrollRates, failRates = failRates, totalDuration = analysisTimes) Events <- ceiling(xx$Events) yy <- gs_info_ahr(enrollRates = enrollRates, failRates = failRates, events = Events) timing <- yy$info0 / max(yy$info0) d <- gsDesign::gsDesign(k = length(timing), test.type = 2, sfu = sfLDOF, alpha = .025, timing = timing) zz <- gs_power_npe(   theta = yy$theta, info = yy$info, info0 = yy$info0,   upper = gs_b, lower = gs_b,   upar = d$upper$bound,   lpar = d$lower$bound ) zz #> # A tibble: 8 × 9 #>   Analysis Bound     Z Probability theta theta1  info info0 info1 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  2.67   0.105     0.301  0.301  22.3  22.7  22.3 #> 2        2 Upper  2.36   0.307     0.346  0.346  28.4  29.0  28.4 #> 3        3 Upper  2.18   0.493     0.370  0.370  33.4  34.0  33.4 #> 4        4 Upper  2.07   0.627     0.385  0.385  37.5  38.0  37.5 #> 5        1 Lower -2.67   0.0000214 0.301  0.301  22.3  22.7  22.3 #> 6        2 Lower -2.36   0.0000299 0.346  0.346  28.4  29.0  28.4 #> 7        3 Lower -2.18   0.0000336 0.370  0.370  33.4  34.0  33.4 #> 8        4 Lower -2.07   0.0000353 0.385  0.385  37.5  38.0  37.5"},{"path":"https://merck.github.io/gsdmvn/articles/DesignWithSpending.html","id":"deriving-sample-size-to-power-a-trial","dir":"Articles","previous_headings":"","what":"Deriving sample size to power a trial","title":"Trial design with spending under NPH","text":"using fixed design, approximate sample size follows: inflate enrollment rates minx use fixed design, see achieves targeted power. power group sequential design final sample size bit lower: inflate bit overpowered. Now use gs_design_npe() inflate information proportionately power trial.","code":"K <- 4 minx <- ((qnorm(.025) / sqrt(zz$info0[K]) + qnorm(.1) / sqrt(zz$info[K])) / zz$theta[K])^2 minx #> [1] 1.875516 gs_power_npe(   theta = yy$theta[K], info = yy$info[K] * minx, info0 = yy$info0[K] * minx,   upar = qnorm(.975), lpar = -Inf ) %>%   filter(Bound == \"Upper\") #> # A tibble: 1 × 9 #>   Analysis Bound     Z Probability theta theta1  info info0 info1 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  1.96       0.898 0.385  0.385  70.3  71.3  70.3 zz <- gs_power_npe(   theta = yy$theta, info = yy$info * minx, info0 = yy$info0 * minx,   upper = gs_spending_bound, lower = gs_spending_bound,   upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),   lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL) ) zz #> # A tibble: 8 × 9 #>   Analysis Bound      Z Probability theta theta1  info info0 info1 #>      <int> <chr>  <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  2.67      0.233   0.301  0.301  41.8  42.7  41.8 #> 2        2 Upper  2.36      0.569   0.346  0.346  53.3  54.4  53.3 #> 3        3 Upper  2.18      0.778   0.370  0.370  62.7  63.8  62.7 #> 4        4 Upper  2.07      0.881   0.385  0.385  70.3  71.3  70.3 #> 5        1 Lower -0.739     0.00365 0.301  0.301  41.8  42.7  41.8 #> 6        2 Lower  0.159     0.0101  0.346  0.346  53.3  54.4  53.3 #> 7        3 Lower  0.746     0.0176  0.370  0.370  62.7  63.8  62.7 #> 8        4 Lower  1.16      0.0250  0.385  0.385  70.3  71.3  70.3 zz <- gs_power_npe(   theta = yy$theta, info = yy$info * minx * 1.2, info0 = yy$info0 * minx * 1.2,   upper = gs_spending_bound, lower = gs_spending_bound,   upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),   lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL) ) zz #> # A tibble: 8 × 9 #>   Analysis Bound      Z Probability theta theta1  info info0 info1 #>      <int> <chr>  <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  2.67      0.294   0.301  0.301  50.2  51.2  50.2 #> 2        2 Upper  2.36      0.660   0.346  0.346  64.0  65.3  64.0 #> 3        3 Upper  2.18      0.851   0.370  0.370  75.3  76.5  75.3 #> 4        4 Upper  2.07      0.931   0.385  0.385  84.4  85.5  84.4 #> 5        1 Lower -0.554     0.00365 0.301  0.301  50.2  51.2  50.2 #> 6        2 Lower  0.400     0.0101  0.346  0.346  64.0  65.3  64.0 #> 7        3 Lower  1.03      0.0176  0.370  0.370  75.3  76.5  75.3 #> 8        4 Lower  1.47      0.0250  0.385  0.385  84.4  85.5  84.4 theta <- yy$theta info <- yy$info info0 <- yy$info0 upper <- gs_spending_bound lower <- gs_spending_bound upar <- list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL) lpar <- list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL) alpha <- .025 beta <- .1 binding <- FALSE test_upper <- TRUE test_lower <- TRUE r <- 18 tol <- 1e-06  zz <- gs_design_npe(   theta = yy$theta, info = yy$info, info0 = yy$info0,   upper = gs_spending_bound, lower = gs_spending_bound,   upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),   lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL) ) zz #> # A tibble: 8 × 9 #>   Analysis Bound      Z Probability theta theta1  info info0 info1 #>      <int> <chr>  <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  2.67      0.252   0.301  0.301  44.5  45.4  44.5 #> 2        2 Upper  2.36      0.600   0.346  0.346  56.8  57.9  56.8 #> 3        3 Upper  2.18      0.804   0.370  0.370  66.8  67.9  66.8 #> 4        4 Upper  2.07      0.900   0.385  0.385  74.9  75.8  74.9 #> 5        1 Lower -0.678     0.00365 0.301  0.301  44.5  45.4  44.5 #> 6        2 Lower  0.238     0.0101  0.346  0.346  56.8  57.9  56.8 #> 7        3 Lower  0.839     0.0176  0.370  0.370  66.8  67.9  66.8 #> 8        4 Lower  1.26      0.0250  0.385  0.385  74.9  75.8  74.9"},{"path":"https://merck.github.io/gsdmvn/articles/NPEbackground.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Non-Proportional Effect Size in Group Sequential Design","text":"acronym NPES short non-proportional effect size. motivated primarily use designing time--event trial non-proportional hazards (NPH), simplified generalized concept . model likely useful rank-based survival tests beyond logrank test considered initially Tsiatis (1982). also useful situations treatment effect may vary time trial reason. generalize framework Chapter 2 Proschan, Lan, Wittes (2006) incorporate possibility treatment effect changing course trial systematic way. vignettes addresses distribution theory initial technical issues around computing boundary crossing probabilities bounds satisfying targeted boundary crossing probabilities applied generalize computational algorithms provided Chapter 19 Jennison Turnbull (2000) used compute boundary crossing probabilities well boundaries group sequential designs. Additional specifics around boundary computation, power sample size provided separate vignette.","code":""},{"path":[]},{"path":"https://merck.github.io/gsdmvn/articles/NPEbackground.html","id":"the-continuous-model-and-e-process","dir":"Articles","previous_headings":"The probability model","what":"The continuous model and E-process","title":"Non-Proportional Effect Size in Group Sequential Design","text":"consider simple example motivate distribution theory quite general applies across many situations. instance Proschan, Lan, Wittes (2006) immediately suggest paired observations, time--event binary outcomes endpoints theory applicable. assume given integer \\(N>0\\) \\(X_{}\\) independent, \\(=1,2,\\ldots\\). integer \\(K\\le N\\) assume perform analysis \\(K\\) times \\(0<n_1<n_2,\\ldots ,n_K = N\\) observations available analysis. Note confined \\(n\\le N\\), \\(N\\) can considered final planned sample size. Proschan, Lan, Wittes (2006) refer estimation E-process extend \\[\\hat{\\theta}_k = \\frac{\\sum_{=1}^{n_k} X_{}}{n_k}\\equiv \\bar X_{k}.\\] Proschan, Lan, Wittes (2006) used \\(\\delta\\) instead \\(\\theta\\) notation, stick closely notation Jennison Turnbull (2000) \\(\\theta\\) used. example, see \\(\\hat{\\theta}_k\\equiv\\bar X_k\\) represents sample average analysis \\(k\\), \\(1\\le k\\le K\\). survival endpoint, \\(\\hat\\theta_k\\) typically represent Cox model coefficient representing logarithm hazard ratio experimental vs control treatment \\(n_k\\) represent planned number events analysis \\(k\\), \\(1\\le k\\le K.\\) Denoting \\(t_k=n_k/N\\), assume real-valued function \\(\\theta(t)\\) \\(t\\ge 0\\) \\(1\\le k\\le K\\) \\[E(\\hat{\\theta}_k) =\\theta(t_k) =E(\\bar X_k).\\] models Proschan, Lan, Wittes (2006) Jennison Turnbull (2000) \\(\\theta(t)\\) equal constant \\(\\theta\\). assume \\(=1,2,\\ldots\\) \\[\\hbox{Var}(X_{})=1.\\] sample average variance assumption \\(1\\le k\\le K\\) \\[\\hbox{Var}(\\hat\\theta(t_k))=\\hbox{Var}(\\bar X_k) =  1/ n_k.\\] statistical information estimate \\(\\hat\\theta(t_k)\\) \\(1\\le k\\le K\\) case \\[ \\mathcal{}_k \\equiv \\frac{1}{\\hbox{Var}(\\hat\\theta(t_k))} = n_k.\\] now see \\(t_k\\), \\(1\\le k\\le K\\) -called information fraction analysis \\(k\\) \\(t_k=\\mathcal{}_k/\\mathcal{}_K.\\)","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbackground.html","id":"z-process","dir":"Articles","previous_headings":"The probability model","what":"Z-process","title":"Non-Proportional Effect Size in Group Sequential Design","text":"Z-process commonly used (e.g., Jennison Turnbull (2000)) used extend computational algorithm Chapter 19 Jennison Turnbull (2000) defining equivalently first second lines \\(k=1,\\ldots,K\\) \\[Z_{k} = \\frac{\\hat\\theta_k}{\\sqrt{\\hbox{Var}(\\hat\\theta_k)}}= \\sqrt{\\mathcal{}_k}\\hat\\theta_k= \\sqrt{n_k}\\bar X_k.\\] variance \\(1\\le k\\le K\\) \\[\\hbox{Var}(Z_k) = 1\\] expected value \\[E(Z_{k})= \\sqrt{\\mathcal{}_k}\\theta(t_{k})= \\sqrt{n_k}E(\\bar X_k) .\\]","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbackground.html","id":"b-process","dir":"Articles","previous_headings":"The probability model","what":"B-process","title":"Non-Proportional Effect Size in Group Sequential Design","text":"B-values mnemonic Brownian motion. \\(1\\le k\\le K\\) define \\[B_{k}=\\sqrt{t_k}Z_k\\] implies \\[ E(B_{k}) = \\sqrt{t_{k}\\mathcal{}_k}\\theta(t_k) = t_k \\sqrt{\\mathcal{}_K} \\theta(t_k) = \\mathcal{}_k\\theta(t_k)/\\sqrt{\\mathcal{}_K}\\] \\[\\hbox{Var}(B_k) = t_k.\\] example, \\[B_k=\\frac{1}{\\sqrt N}\\sum_{=1}^{n_k}X_i.\\] can useful think \\(B_k\\) sum independent random variables.","code":""},{"path":[]},{"path":"https://merck.github.io/gsdmvn/articles/NPEbackground.html","id":"conditional-independence-covariance-and-canonical-form","dir":"Articles","previous_headings":"The probability model","what":"Conditional independence, covariance and canonical form","title":"Non-Proportional Effect Size in Group Sequential Design","text":"assume independent increments B-process. , \\(1\\le j < k\\le K\\) \\[\\tag{1} B_k - B_j \\sim \\hbox{Normal} (\\sqrt{\\mathcal{}_K}(t_k\\theta(t_k)- t_j\\theta(t_j)), t_k-t_j)\\] independent \\(B_1,\\ldots,B_j\\). noted , given \\(1\\le k\\le K\\) example \\[B_j=\\sum_{=1}^{n_j}X_i / \\sqrt N.\\] independence sequence \\(X_i\\), \\(=1,2,\\ldots\\), immediately \\(1\\le j\\le k\\le K\\) \\[\\hbox{Cov}(B_j,B_k) = \\hbox{Var}(B_j) = t_j.\\] leads \\[\\hbox{Corr}(B_j,B_k)=\\frac{t_j}{\\sqrt{t_jt_k}}=\\sqrt{t_j/t_k}=\\hbox{Corr}(Z_j,Z_k)=\\hbox{Cov}(Z_j,Z_k)\\] covariance structure -called canonical form Jennison Turnbull (2000). example, \\[B_k=\\frac{1}{\\sqrt N}\\sum_{=1}^{n_k}X_i\\] \\[B_k-B_j=\\frac{1}{\\sqrt N}\\sum_{=n_j + 1}^{n_k}X_i\\] covariance obvious. assume independent increments B-process demonstrated simple example . , \\(1\\le j < k\\le K\\) \\[\\tag{1} B_k - B_j \\sim \\hbox{Normal} (\\mathcal{}_k\\theta(t_k)- \\mathcal{}_j\\theta(t_j), t_k-t_j)\\] independent \\(B_1,\\ldots,B_j\\). given \\(1\\le j\\le k\\le K\\) example \\[B_j=\\sum_{=1}^{n_j}X_i / (\\sqrt N\\sigma).\\] independence sequence \\(X_i\\), \\(=1,2,\\ldots\\), immediately \\(1\\le j\\le k\\le K\\) \\[\\hbox{Cov}(B_j,B_k) = \\hbox{Var}(B_j) = t_j/t_k =\\mathcal{}_j/\\mathcal{}_k.\\] leads \\[\\mathcal{}_j/\\mathcal{}_k=\\sqrt{t_j/t_k}=\\hbox{Corr}(B_j,B_k)=\\hbox{Corr}(Z_j,Z_k)=\\hbox{Cov}(Z_j,Z_k)\\] covariance structure -called canonical form Jennison Turnbull (2000). independence \\(B_j\\) \\[B_k-B_j=\\sum_{=n_j + 1}^{n_k}X_i/(\\sqrt N\\sigma)\\] obvious example.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbackground.html","id":"test-bounds-and-crossing-probabilities","dir":"Articles","previous_headings":"","what":"Test bounds and crossing probabilities","title":"Non-Proportional Effect Size in Group Sequential Design","text":"section define notation bounds boundary crossing probabilities group sequential design. also define algorithm computing bounds based targeted boundary crossing probability analysis. notation used elsewhere defining one- two-sided group sequential hypothesis testing. value \\(\\theta(t)>0\\) reflect positive benefit. \\(k=1,2,\\ldots,K-1\\), interim cutoffs \\(-\\infty \\le a_k< b_k\\le \\infty\\) set; final cutoffs \\(-\\infty \\le a_K\\leq b_K <\\infty\\) also set. infinite efficacy bound analysis means bound crossed analysis. Thus, \\(3K\\) parameters define group sequential design: \\(a_k\\), \\(b_k\\), \\(\\mathcal{}_k\\), \\(k=1,2,\\ldots,K\\).","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbackground.html","id":"notation-for-boundary-crossing-probabilities","dir":"Articles","previous_headings":"Test bounds and crossing probabilities","what":"Notation for boundary crossing probabilities","title":"Non-Proportional Effect Size in Group Sequential Design","text":"now apply distributional assumptions compute boundary crossing probabilities. use shorthand notation section \\(\\theta\\) represent \\(\\theta()\\) \\(\\theta=0\\) represent \\(\\theta(t)\\equiv 0\\) \\(t\\). denote probability crossing upper boundary analysis \\(k\\) without previously crossing bound \\[\\alpha_{k}(\\theta)=P_{\\theta}(\\{Z_{k}\\geq b_{k}\\}\\cap_{j=1}^{-1}\\{a_{j}\\le Z_{j}< b_{j}\\}),\\] \\(k=1,2,\\ldots,K.\\) Next, consider analogous notation lower bound. \\(k=1,2,\\ldots,K\\) denote probability crossing lower bound analysis \\(k\\) without previously crossing bound \\[\\beta_{k}(\\theta)=P_{\\theta}((Z_{k}< a_{k}\\}\\cap_{j=1}^{k-1}\\{ a_{j}\\le Z_{j}< b_{j}\\}).\\] symmetric testing analysis \\(k\\) \\(a_k= - b_k\\), \\(\\beta_k(0)=\\alpha_k(0),\\) \\(k=1,2,\\ldots,K\\). total lower boundary crossing probability trial denoted \\[\\beta(\\theta)\\equiv\\sum_{k=1}^{K}\\beta_{k}(\\theta).\\] Note can also set \\(a_k= -\\infty\\) analyses lower bound desired, \\(k=1,2,\\ldots,K\\). \\(k<K\\), can set \\(b_k=\\infty\\) upper bound desired. Obviously, \\(k\\), want either \\(a_k>-\\infty\\) \\(b_k<\\infty\\).","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbackground.html","id":"recursive-algorithms","dir":"Articles","previous_headings":"Test bounds and crossing probabilities","what":"Recursive algorithms","title":"Non-Proportional Effect Size in Group Sequential Design","text":"now provide small update algorithm Chapter 19 Jennison Turnbull (2000) numerical integration required compute boundary crossing probabilites previous section also identifying group sequential boundaries satisfying desired characteristics. key calculations conditional power identitity equation (1) allows building recursive numerical integration identities enable simple, efficient numerical integration. define \\[g_1(z;\\theta) = \\frac{d}{dz}P(Z_1\\le z) = \\phi\\left(z - \\sqrt{\\mathcal{}_1}\\theta(t_1)\\right)\\tag{2}\\] \\(k=2,3,\\ldots K\\) recursively define subdensity function \\[\\begin{align} g_k(z; \\theta) &= \\frac{d}{dz}P_\\theta(\\{Z_k\\le z\\}\\cap_{j=1}^{k-1}\\{a_j\\le Z_j<b_j\\}) \\\\  &=\\int_{a_{k-1}}^{b_{k-1}}\\frac{d}{dz}P_\\theta(\\{Z_k\\le z |Z_{k-1}=z_{k-1}\\})g_{k-1}(z_{k-1}; \\theta)dz_{k-1}\\\\  &=\\int_{a_{k-1}}^{b_{k-1}}f_k(z_{k-1},z;\\theta)g_{k-1}(z_{k-1}; \\theta)dz_{k-1}.\\tag{3}  \\end{align}\\] bottom line notation p. 347 Jennison Turnbull (2000). However, \\(f_k()\\) takes slightly different form. \\[\\begin{align} f_k(z_{k-1},z;\\theta) &=\\frac{d}{dz}P_\\theta(\\{Z_k\\le z |Z_{k-1}=z_{k-1}\\})\\\\  &=\\frac{d}{dz}P_\\theta(B_k - B_{k-1} \\le z\\sqrt{t_k}-z_{k-1}\\sqrt{t_{k-1}})\\\\  &=\\frac{d}{dz}\\Phi\\left(\\frac{z\\sqrt{t_k}-z_{k-1}\\sqrt{t_{k-1}}-\\sqrt{\\mathcal{}_K}(t_k\\theta(t_k)- t_{k-1}\\theta(t_{k-1}))}{\\sqrt{t_k-t_{k-1}}}\\right)\\\\  &=\\frac{\\sqrt{t_k}}{\\sqrt{t_k-t_{k-1}}}\\phi\\left(\\frac{z\\sqrt{t_k}-z_{k-1}\\sqrt{t_{k-1}}-\\sqrt{\\mathcal{}_K}(t_k\\theta(t_k)- t_{k-1}\\theta(t_{k-1}))}{\\sqrt{t_k-t_{k-1}}}\\right)\\\\  &=\\frac{\\sqrt{\\mathcal{}_k}}{\\sqrt{\\mathcal{}_k-\\mathcal{}_{k-1}}}\\phi\\left(\\frac{z\\sqrt{\\mathcal{}_k}-z_{k-1}\\sqrt{\\mathcal{}_{k-1}}-(\\mathcal{}_k\\theta(t_k)- \\mathcal{}_{k-1}\\theta(t_{k-1}))}{\\sqrt{\\mathcal{}_k-\\mathcal{}_{k-1}}}\\right).\\tag{3} \\end{align}\\] worked towards last line due comparability equation (19.4) p. 347 Jennison Turnbull (2000) assumes \\(\\theta(t_k)=\\theta\\) constant \\(\\theta\\); re-write equation slightly : \\[f_k(z_{k-1},z;\\theta) = \\frac{\\sqrt{\\mathcal{}_k}}{\\sqrt{\\mathcal{}_k-\\mathcal{}_{k-1}}}\\phi\\left(\\frac{z\\sqrt{\\mathcal{}_k}-z_{k-1}\\sqrt{\\mathcal{}_{k-1}}-\\theta(\\mathcal{}_k- \\mathcal{}_{k-1})}{\\sqrt{\\mathcal{}_k-\\mathcal{}_{k-1}}}\\right).\\tag{4}\\] really difference computational algorithm boundary crossing probabilities Jennison Turnbull (2000) algorithm. Using recursive approach can compute \\(k=1,2,\\ldots,K\\) \\[\\alpha_{k}(\\theta)=\\int_{b_k}^\\infty g_k(z;\\theta)dz\\tag{5}\\] \\[\\beta_{k}(\\theta)=\\int_{-\\infty}^{a_k} g_k(z;\\theta)dz.\\tag{6}\\]","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbackground.html","id":"deriving-spending-boundaries","dir":"Articles","previous_headings":"Test bounds and crossing probabilities","what":"Deriving spending boundaries","title":"Non-Proportional Effect Size in Group Sequential Design","text":"can now derive boundaries satisfying given boundary crossing probabilities using equations (2-6) . Suppose specified \\(b_1,\\ldots,b_{k-1}\\) \\(a_1,\\ldots,a_{k-1}\\) now wish derive \\(a_k\\) \\(b_k\\) equations (5) (6) hold. write upper bound function probability crossing wish derive. \\[\\pi_k(b;\\theta) = \\int_b^\\infty g_k(z;\\theta)dz\\] \\[\\pi_k^\\prime(b;\\theta) =\\frac{d}{db}\\pi_k(b;\\theta)= -g_k(b; \\theta).\\tag{7}\\] value \\(\\pi_k(b^{()};\\theta)\\) can use first order Taylor’s series expansion approximate \\[\\pi_k(b;\\theta)\\approx \\pi_k(b^{()};\\theta)+(b-b^{()})\\pi_k^\\prime(b^{()};\\theta)\\] set \\(b^{(+1)}\\) \\[\\alpha_k(\\theta)=\\pi_k(b^{()}; \\theta) + (b^{(+1)}-b^{()})\\pi^\\prime(b^{()};\\theta).\\] Solving \\(b^{(+1)}\\) \\[b^{(+)} = b^{()} + \\frac{\\alpha_k(\\theta) - \\pi_k(b^{()};\\theta)}{\\pi_k^\\prime(b^{()}; \\theta)}= b^{()} - \\frac{\\alpha_k(\\theta) - \\pi_k(b^{()};\\theta)}{g_k(b^{()}; \\theta)}\\tag{8}\\] iterate \\(|b^{(+1)}-b^{()}|<\\epsilon\\) tolerance level \\(\\epsilon>0\\) \\(\\pi_k(b^{(+1)};\\theta)-\\alpha_k(\\theta)\\) suitably small. simple starting value \\(k\\) \\[b^{(0)} = \\Phi^{-1}(1- \\alpha_k(\\theta)) + \\sqrt{\\mathcal{}_k}\\theta(t_k).\\tag{9}\\] Normally, \\(b_k\\) calculated \\(\\theta(t_k)=0\\) \\(k=1,2,\\ldots,K\\) simplifies . However, \\(a_k\\) computed analogously often use non-zero \\(\\theta\\) enable -called \\(\\beta\\)-spending.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbackground.html","id":"numerical-integration","dir":"Articles","previous_headings":"","what":"Numerical integration","title":"Non-Proportional Effect Size in Group Sequential Design","text":"numerical integration required compute boundary probabilities derive boundaries defined section 19.3 Jennison Turnbull (2000). single change replacement non-proportional effect size assumption equation (3) replacing equivalent equation (4) used constant effect size Jennison Turnbull (2000).","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbackground.html","id":"demonstrating-calculations","dir":"Articles","previous_headings":"Numerical integration","what":"Demonstrating calculations","title":"Non-Proportional Effect Size in Group Sequential Design","text":"walk perform basic calculations . basic scenario one interim analysis addition final analysis. target Type error \\(\\alpha=0.025\\) Type II error \\(\\beta = 0.1\\), latter corresponding target 90% power. assume power spending function \\(\\rho=2\\) bounds. , information fraction \\(t\\), cumulative spending \\(\\alpha \\times t^2\\) upper bound \\(\\beta \\times t^2\\) lower bound. Statistical information 1 first analysis 4 final analysis, leading information fraction \\(t_1= 1/4, t_2=1\\) interim final, respectively. assume \\(\\theta_1 = .5\\), \\(\\theta_3=1.5\\). Set overall study parameters Calculate interim bounds Set numerical integration grid next (final) analysis set table numerical integration continuation region can subsequently use compute boundary crossing probabilities bounds second interim analysis. begin null hypothesis. columns resulting table - z - \\(Z\\)-values grid; recall interim test statistic normally distributed variance 1 - w - weights numerical integration - h - weights w times normal density can used numerical integration; demonstrate use probability crossing bound null hypothesis computed follows: now set numerical integration grid alternate hypothesis compute continuation probability. Compute initial iteration analysis 2 bounds initial estimate second analysis bounds computed way actual first analysis bounds. Compute actual boundary crossing probabilities initial approximations get actual boundary crossing probabilities second analysis, update numerical integration grids. null hypothesis, need update interval b2_0. get first order Taylor’s series approximation update bound, need derivative probability respect Z-value cutoff. given subdensity computed grid. , grid contains numerical integration weight w weight times subdensity h. Thus, get subdensity bound, estimated derivative boundary crossing probability, compute: see Taylor’s series update gotten us substantially closer targeted boundary probability. now update lower bound analogous fashion. Confirm gs_power_npe()","code":"# Information for both null and alternative info <- c(1, 4) # information fraction timing <- info / max(info)  # Type I error alpha <- 0.025 # Type II error (1 - power) beta <- 0.1 # Cumulative alpha-spending at IA, Final alphaspend <- alpha * timing^2 # Cumulative beta-spending at IA, Final betaspend <- beta * timing^2 # Average treatment effect at analyses theta <- c(1, 3)/2 # Upper bound under null hypothesis b1 <- qnorm(alphaspend[1], lower.tail = FALSE) # Lower bound under alternate hypothesis a1 <- qnorm(betaspend[1], mean = sqrt(info[1]) * theta[1]) # Compare probability of crossing vs target for bounds: cat(\"Upper bound =\", b1, \"Target spend=\", alphaspend[1],     \"Actual spend=\", pnorm(b1, lower.tail=FALSE)) #> Upper bound = 2.955167 Target spend= 0.0015625 Actual spend= 0.0015625 # Lower bound under alternate hypothesis a1 <- qnorm(betaspend[1], mean = sqrt(info[1]) * theta[1]) # Compare probability of crossing vs target for bounds: cat(\"Lower bound =\", a1, \"Target spend=\", betaspend[1],     \"Actual spend=\", pnorm(a1, mean = sqrt(info[1]) * theta[1])) #> Lower bound = -1.997705 Target spend= 0.00625 Actual spend= 0.00625 # Set up grid over continuation region # Null hypothesis grid1_0 <- h1(theta = 0, I = info[1], a = a1, b = b1) grid1_0 %>% head() #> # A tibble: 6 × 3 #>       z      w        h #>   <dbl>  <dbl>    <dbl> #> 1 -2.00 0.0135 0.000733 #> 2 -1.96 0.0540 0.00317  #> 3 -1.92 0.0274 0.00174  #> 4 -1.88 0.0556 0.00382  #> 5 -1.83 0.0278 0.00206  #> 6 -1.79 0.0556 0.00445 probH0continue <- grid1_0 %>% summarize(sum(h)) %>% as.numeric() cat(\"Probability of continuing trial under null hypothesis\\n\",     \" Using numerical integration:\", probH0continue,      \"\\n  Using normal cdf:\", pnorm(b1) - pnorm(a1), \"\\n\") #> Probability of continuing trial under null hypothesis #>   Using numerical integration: 0.9755632  #>   Using normal cdf: 0.9755632 grid1_1 <- h1(theta = theta[1], I = info[1], a = a1, b = b1) probH1continue <- grid1_1 %>% summarize(sum(h)) %>% as.numeric() h1mean <- sqrt(info[1]) * theta[1] cat(\"Probability of continuing trial under alternate hypothesis\\n\",     \" Using numerical integration:\", probH1continue,      \"\\n  Using normal cdf:\", pnorm(b1, mean = h1mean) - pnorm(a1, h1mean), \"\\n\") #> Probability of continuing trial under alternate hypothesis #>   Using numerical integration: 0.986709  #>   Using normal cdf: 0.986709 # Upper bound under null hypothesis # incremental spend spend0 <- alphaspend[2] - alphaspend[1] # H0 bound at 2nd analysis; 1st approximation b2_0 <- qnorm(spend0, lower.tail = FALSE) # Lower bound under alternate hypothesis spend1 <- betaspend[2] - betaspend[1] a2_0 <- qnorm(spend1, mean = sqrt(info[2]) * theta[2]) cat(\"Initial bound approximation for 2nd analysis\\n (\",     a2_0, \", \", b2_0,\")\\n\", sep=\"\") #> Initial bound approximation for 2nd analysis #>  (1.681989, 1.987428) # Upper rejection region grid under H0 grid2_0 <- hupdate(theta = 0, I = info[2], a = b2_0, b = Inf, Im1 = info[1], gm1 = grid1_0) pupper_0 <- grid2_0 %>% summarize(sum(h)) %>% as.numeric() cat(\"Upper spending at analysis 2\\n Target:\", spend0, \"\\n Using initial bound approximation:\",     pupper_0,\"\\n\") #> Upper spending at analysis 2 #>  Target: 0.0234375  #>  Using initial bound approximation: 0.02290683 # First point in grid is at bound # Compute derivative dpdb2 <- grid2_0$h[1] / grid2_0$w[1] # Compute difference between target and actual bound crossing probability pdiff <- spend0 - pupper_0 # Taylor's series update b2_1 <- b2_0 - pdiff / dpdb2 # Compute boundary crossing probability at updated bound cat(\"Original bound approximation:\", b2_0,      \"\\nUpdated bound approximation:\", b2_1     ) #> Original bound approximation: 1.987428  #> Updated bound approximation: 1.977726 grid2_0 <- hupdate(theta = 0, I = info[2], a = b2_1, b = Inf, Im1 = info[1], gm1 = grid1_0) pupper_1 <- grid2_0 %>% summarize(sum(h)) %>% as.numeric() cat(\"\\nOriginal boundary crossing probability:\", pupper_0,      \"\\nUpdated boundary crossing probability:\", pupper_1,     \"\\nTarget:\", spend0, \"\\n\"     ) #>  #> Original boundary crossing probability: 0.02290683  #> Updated boundary crossing probability: 0.02344269  #> Target: 0.0234375 # Lower rejection region grid under H1 grid2_1 <- hupdate(theta = theta[2], I = info[2], a = -Inf, b = a2_0,                     thetam1 = theta[1], Im1 = info[1], gm1 = grid1_1) plower_0 <- grid2_1 %>% summarize(sum(h)) %>% as.numeric() # Last point in grid is at bound # Compute derivative indx <- nrow(grid2_1) dpda2 <- grid2_1$h[indx] / grid2_1$w[indx] # Compute difference between target and actual bound crossing probability pdiff <- spend1 - plower_0 # Taylor's series update a2_1 <- a2_0 + pdiff / dpda2 # Compute boundary crossing probability at updated bound cat(\"Original bound approximation:\", a2_0,      \"\\nUpdated bound approximation:\", a2_1     ) #> Original bound approximation: 1.681989  #> Updated bound approximation: 1.702596 grid2_1 <- hupdate(theta = theta[2], I = info[2], a = -Inf, b = a2_1,                     thetam1 = theta[1], Im1 = info[1], gm1 = grid1_1) plower_1 <- grid2_1 %>% summarize(sum(h)) %>% as.numeric() cat(\"\\nOriginal boundary crossing probability:\", plower_0,      \"\\nUpdated boundary crossing probability:\", plower_1,     \"\\nTarget:\", spend1, \"\\n\"     ) #>  #> Original boundary crossing probability: 0.09035972  #> Updated boundary crossing probability: 0.09379707  #> Target: 0.09375 gs_power_npe(theta = theta, theta1 = theta, info = info, binding = TRUE,              upper = gs_spending_bound,              upar = list(sf = gsDesign::sfPower, total_spend = 0.025, param = 2),              lower = gs_spending_bound,              lpar = list(sf = gsDesign::sfPower, total_spend = 0.1, param = 2) ) #> # A tibble: 4 × 9 #>   Analysis Bound     Z Probability theta theta1  info info0 info1 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  2.96     0.00704   0.5    0.5     1     1     1 #> 2        2 Upper  1.98     0.845     1.5    1.5     4     4     4 #> 3        1 Lower -2.00     0.00625   0.5    0.5     1     1     1 #> 4        2 Lower  1.70     0.100     1.5    1.5     4     4     4"},{"path":"https://merck.github.io/gsdmvn/articles/NPEbackground.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Non-Proportional Effect Size in Group Sequential Design","text":"Jennison, Christopher, Bruce W. Turnbull. 2000. Group Sequential Methods Applications Clinical Trials. Boca Raton, FL: Chapman; Hall/CRC. Proschan, Michael ., K. K. Gordon Lan, Janet Turk Wittes. 2006. Statistical Monitoring Clinical Trials. Unified Approach. New York, NY: Springer. Tsiatis, Anastasios . 1982. “Repeated Significance Testing General Class Statistics Use Censored Survival Analysis.” Journal American Statistical Association 77: 855–61.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbounds.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Computing Bounds Under Non-Constant Treatment Effect","text":"consider one- two-sided hypothesis testing using group sequential design possibly non-constant treatment effect. can useful situations assumed non-proportional hazards model using weighted logrank tests time--event endpoint. Asymptotic distributional assumptions document laid vignette Non-Proportional Effect Size Group Sequential Design. general, assume \\(K\\ge 1\\) analyses statistical information \\(\\mathcal{}_k\\) information fraction \\(t_k=\\mathcal{}_k/\\mathcal{}_k\\) analysis \\(k\\), \\(1\\le k\\le K\\). denote null hypothesis \\(H_{0}\\): \\(\\theta(t)=0\\) alternate hypothesis \\(H_1\\): \\(\\theta(t)=\\theta_1(t)\\) \\(t> 0\\) \\(t\\) represents information fraction study. study planned stop information fraction \\(t=1\\), define \\(\\theta(t)\\) \\(t>0\\) since trial can overrun planned statistical information final analysis. , use shorthand notation \\(\\theta\\) represent \\(\\theta()\\), \\(\\theta=0\\) represent \\(\\theta(t)\\equiv 0\\) \\(t\\) \\(\\theta_1\\) represent \\(\\theta_i(t_k)\\), effect size analysis \\(k\\), \\(1\\le k\\le K\\). purposes, \\(H_0\\) represent treatment difference, represent non-inferiority hypothesis. Recall assume \\(K\\) analyses bounds \\(-\\infty \\le a_k< b_k<\\le \\infty\\) \\(1\\le k < K\\) \\(-\\infty \\le a_K\\le b_K<\\infty\\). denote probability crossing upper boundary analysis \\(k\\) without previously crossing bound \\[\\alpha_{k}(\\theta)=P_{\\theta}(\\{Z_{k}\\geq b_{k}\\}\\cap_{j=1}^{-1}\\{a_{j}\\le Z_{j}< b_{j}\\}),\\] \\(k=1,2,\\ldots,K.\\) total probability crossing upper bound prior crossing lower bound denoted \\[\\alpha(\\theta)\\equiv\\sum_{k=1}^K\\alpha_k(\\theta).\\] denote probability crossing lower bound analysis \\(k\\) without previously crossing bound \\[\\beta_{k}(\\theta)=P_{\\theta}((Z_{k}< a_{k}\\}\\cap_{j=1}^{k-1}\\{ a_{j}\\le Z_{j}< b_{j}\\}).\\] Efficacy bounds \\(b_k\\), \\(1\\le k\\le K\\), group sequential design derived control Type level \\(\\alpha=\\alpha(0)\\). Lower bounds \\(a_k\\), \\(1\\le k\\le K\\) may used control boundary crossing probabilities either null hypothesis (2-sided testing), alternate hypothesis hypothesis (futility testing). Thus, may consider 3 values \\(\\theta(t)\\): null hypothesis \\(\\theta_0(t)=0\\) computing efficacy bounds, value \\(\\theta_1(t)\\) computing lower bounds, value \\(\\theta_a(t)\\) computing sample size power. refer information 3 assumptions \\(\\mathcal{}^{(0)}(t)\\), \\(\\mathcal{}^{(1)}(t)\\), \\(\\mathcal{}^{()}(t)\\), respectively. Often assume \\(\\mathcal{}(t)=\\mathcal{}^{(0)}(t)=\\mathcal{}^{(1)}(t)=\\mathcal{}^{()}(t).\\) note information may differ different values \\(\\theta(t)\\). fixed designs,  computes sample size based different variances null alternate hypothesis.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbounds.html","id":"two-sided-testing-and-design","dir":"Articles","previous_headings":"","what":"Two-sided testing and design","title":"Computing Bounds Under Non-Constant Treatment Effect","text":"denote alternative \\(H_{1}\\): \\(\\theta(t)=\\theta_1(t)\\); always assume \\(H_1\\) power calculations sometimes use \\(H_1\\) controlling lower boundary \\(a_k\\) crossing probabilities. value \\(\\theta(t)>0\\) reflect positive benefit. restrict alternate hypothesis \\(\\theta_1(t)>0\\) \\(t\\). value \\(\\theta(t)\\) referred (standardized) treatment effect information fraction \\(t\\). assume interest stopping early good evidence reject one hypothesis favor . \\(a_k= -\\infty\\) analysis \\(k\\) \\(1\\le k\\le K\\) alternate hypothesis rejected analysis \\(k\\); .e., futility bound analysis \\(k\\). \\(k=1,2,\\ldots,K\\), trial stopped analysis \\(k\\) reject \\(H_0\\) \\(a_j<Z_j< b_j\\), \\(j=1,2,\\dots,-1\\) \\(Z_k\\geq b_k\\). trial continues stage \\(k\\) without crossing bound \\(Z_k\\leq a_k\\) \\(H_1\\) rejected favor \\(H_0\\), \\(k=1,2,\\ldots,K\\). Note \\(a_K< b_K\\) possibility completing trial without rejecting \\(H_0\\) \\(H_1\\).","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbounds.html","id":"haybittle-peto-and-spending-bounds","dir":"Articles","previous_headings":"Two-sided testing and design","what":"Haybittle-Peto and spending bounds","title":"Computing Bounds Under Non-Constant Treatment Effect","text":"recursive algorithm previous section allows computation spending bounds Haybittle-Peto bounds. Haybittle-Peto efficacy bound, one normally set \\(b_k=\\Phi^{-1}(1-\\epsilon)\\) \\(k=1,2,\\ldots,K-1\\) small \\(\\epsilon>0\\) \\(\\epsilon= 0.001\\) yields \\(b_k=3.09\\). original proposal use \\(b_K=\\Phi^{-1}(1-\\alpha)\\) final analysis, fully control one-sided Type error level \\(\\alpha\\) suggest computing final bound \\(b_K\\) using algorithm \\(\\alpha(0)=\\alpha\\). Bounds computed spending \\(\\alpha_k(0)\\) analysis \\(k\\) can computed using equation (9) \\(b_1\\). \\(k=2,\\ldots,K\\) algorithm previous section used. noted Jennison Turnbull (2000), \\(b_1,\\ldots,b_K\\) determined null hypothesis depend \\(t_k\\) \\(\\alpha_k(0)\\) dependence \\(\\mathcal{}_k\\), \\(k=1,\\ldots,K\\). computing bounds based \\(\\beta_k(\\theta)\\), \\(k=1,\\ldots,K\\), \\(\\theta(t_k)\\neq 0\\) additional dependency \\(a_k\\) depending \\(t_k\\) \\(b_k\\), \\(k=1,\\ldots,K\\), also final total information \\(\\mathcal{}_K\\). Thus, spending bound something null hypothesis needs recomputed time \\(\\mathcal{}_K\\) changes, whereas needs computed \\(\\theta(t_k)=0\\), \\(k=1,\\ldots,K\\).","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbounds.html","id":"bounds-based-on-boundary-families","dir":"Articles","previous_headings":"Two-sided testing and design","what":"Bounds based on boundary families","title":"Computing Bounds Under Non-Constant Treatment Effect","text":"Assume constants \\(b_1^*,\\ldots,b_K^*\\) total targeted one-sided Type error \\(\\alpha\\). wish find \\(C_u\\) function \\(t_1,\\ldots t_K\\) \\(b_k=C_ub_k^*\\) \\(\\alpha(0)=\\alpha.\\) Thus, problem solve \\(C_u\\). \\(a_k\\), \\(k=1,2,\\ldots,K\\) fixed simple root finding problem. Since one normally normally uses non-binding efficacy bounds, normally case \\(a_k=-\\infty\\), \\(k=1,\\ldots,K\\) problem. Now assume constants \\(a_k^*\\) wish find \\(C_l\\) \\(a_k=C_la_k^*+\\theta(t_k)\\sqrt{\\mathcal{}_k}\\) \\(k=1,\\ldots,K\\) \\(\\beta(\\theta)=\\beta\\). use constant upper bounds previous paragraph, finding \\(C_l\\) simple root-finding problem. 2-sided symmetric bounds \\(a_k=-b_k\\), \\(k=1,\\ldots,K\\), need solve \\(C_u\\) use simple root finding. point, solve type bound asymmetric upper lower bounds.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbounds.html","id":"sample-size","dir":"Articles","previous_headings":"","what":"Sample size","title":"Computing Bounds Under Non-Constant Treatment Effect","text":"sample size, assume \\(t_k\\), \\(\\theta(t_k)\\) \\(1,\\ldots,K\\) fixed. assume \\(\\beta(\\theta)\\) decreasing \\(\\mathcal{}\\) decreasing. automatically case \\(\\theta(t_k)>0\\), \\(k=1,\\ldots,K\\) many cases. Thus, information required done search \\(\\mathcal{I_K}\\) yields \\(\\alpha(\\theta)\\) yields targeted power.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/NPEbounds.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Computing Bounds Under Non-Constant Treatment Effect","text":"Jennison, Christopher, Bruce W. Turnbull. 2000. Group Sequential Methods Applications Clinical Trials. Boca Raton, FL: Chapman; Hall/CRC.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/PowerEvaluationWithSpending.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Power Evaluation with Spending bounds","text":"vignette covers compute power Type error design derived spending bound. write general non-constant treatment effect using gs_design_npe() derived design one parameter setting computing power another setting. use trial binary endpoint enable full illustration.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/PowerEvaluationWithSpending.html","id":"scenario-for-consideration","dir":"Articles","previous_headings":"","what":"Scenario for consideration","title":"Power Evaluation with Spending bounds","text":"consider scenario largely based CAPTURE study Investigators others (1997) primary endpoint composite death, acute myocardial infarction need recurrent percutaneous intervention within 30 days randomization. , consider 2-arm trial experimental arm control arm. primary endpoint trial binary indicator participant failed outcome. case, consider parameter \\(\\theta = p_1 - p _2\\) \\(p_1\\) denotes probability trial participant control group experiences failure \\(p_2\\) represents probability trial participant experimental group. assume \\(K=3\\) analyses 350, 700, 1400 patients observed equal randomization treatment groups. study designed approximately 80% power (Type II error \\(\\beta = 1 - 0.8 = 0.2\\)) 2.5% one-sided Type error (\\(\\alpha = 0.025\\)) detect reduction 15% event rate (\\(p_1 = 0.15\\)) control group 10% (\\(p_2 = 0.1\\)) experimental group. parameter interest \\(\\theta = p_1 - p_2\\). denote alternate hypothesis \\[H_1: \\theta = \\theta_1= p_1^1 - p_2^1 = 0.15 - 0.10 = 0.05\\] null hypothesis \\[H_0: \\theta = \\theta_0 = 0 = p_1^0 - p_2^0\\] \\(p^0_1 = p^0_2= (p_1^1+p_2^1)/2 = 0.125\\) laid Lachin (2009). note considered success outcome objective response oncology study, let \\(p_1\\) denote experimental group \\(p_2\\) control group response rate. Thus, always set notation \\(p_1>p_2\\) represents superiority experimental group.","code":"library(gsdmvn)  p0 <- 0.15 # assumed failure rate in control group p1 <- 0.10 # assumed failure rate in experimental group alpha <- 0.025 # Design Type I error beta <- 0.2 # Design Type II error for 80% power"},{"path":"https://merck.github.io/gsdmvn/articles/PowerEvaluationWithSpending.html","id":"notation-and-statistical-testing","dir":"Articles","previous_headings":"","what":"Notation and Statistical Testing","title":"Power Evaluation with Spending bounds","text":"let \\(X_{ij}\\sim \\hbox{Bernoulli}(p_i),\\) \\(j=1,2,\\ldots,n_{ik}\\) denote independent random variables control (\\(=1\\)) experimental (\\(=2\\)) groups analysis \\(k=1,2,\\ldots,K\\) \\(n_{i1} < n_{i2}<\\ldots<n_K\\). let \\(Y_{ik}=\\sum_{j=1}^{n_k}X_{ij}\\). analysis \\(k=1,2,\\ldots\\) \\(j=1,2\\) denote proportion failures group \\(\\) analysis \\(k\\) \\[ \\hat{p}_{ik}=Y_{ik}/n_{ik}.\\] Estimating null hypothesis \\(H_0: p_1=p_2\\equiv p_0\\) estimate \\[ \\hat{p}_{0k}=\\frac{Y_{1k}+ Y_{2k}}{n_{1k}+ n_{2k}}= \\frac{n_{1k}\\hat p_{1k} + n_{2k}\\hat p_{2k}}{n_{1k} + n_{2k}}. \\] note \\[\\hbox{Var}(\\hat p_{ik})=\\frac{p_{}(1-p_i)}{n_{ik}},\\] consistent estimatator \\[\\widehat{\\hbox{Var}}(\\hat p_{ik})=\\frac{\\hat p_{ik}(1-\\hat p_{ik})}{n_{ik}},\\] \\(=1,2\\), \\(k=1,2,\\ldots,K\\). Letting \\(\\hat\\theta_k=\\hat p_{1k}-\\hat p_{2k},\\) also \\[\\sigma^2_k\\equiv \\hbox{Var}(\\hat\\theta_i)=\\frac{p_1(1-p_1)}{n_{1k}}+\\frac{p_2(1-p_2)}{n_{2k}},\\] consistent estimator \\[\\hat\\sigma^2_k=\\frac{\\hat p_{1k}(1-\\hat p_{1k})}{n_{1k}}+\\frac{\\hat p_{2k}(1-\\hat p_{2k})}{n_{2k}},\\] corresponding null hypothesis estimator \\[\\hat\\sigma^2_{0k}=\\hat p_{0k}(1-\\hat p_{0k})\\left(\\frac{1}{n_{1k}}+ \\frac{1}{n_{2k}}\\right),\\] \\(k=1,2,\\ldots,K\\). Statistical information quantities corresponding estimators denoted \\[\\begin{align} \\mathcal{}_k = &1/\\sigma^2_k,\\\\ \\mathcal{\\hat }_k = &1/\\hat \\sigma^2_k,\\\\ \\mathcal{}_{0k} =& 1/ \\sigma^2_{0k},\\\\ \\mathcal{\\hat }_{0k} =& 1/\\hat \\sigma^2_{0k}, \\end{align}\\] \\(k=1,2,\\ldots,K\\). Testing, recommended Lachin (2009), done large sample test null hypothesis variance estimate without continuity correction: \\[Z_k = \\hat\\theta_k/\\hat\\sigma_{0k}=\\frac{\\hat p_{1k} - \\hat p_{2k}}{\\sqrt{(1/n_{1k}+ 1/n_{2k})\\hat p_{0k}(1-\\hat p_{0k})} },\\] asymptotically Normal(0,1) \\(p_1=p_2\\) Normal(0, \\(\\sigma_{0k}^2/\\sigma_k^2\\)) generally \\(p_1, p_2\\), \\(k=1,2,\\ldots,K\\). note \\(\\chi^2=Z^2_k\\) \\(\\chi^2\\) test without continuity correction recommended Gordon Watson (1996). Note finally extends straightforward way non-inferiority test Farrington Manning (1990) null hypothesis \\(\\theta = p_1 - p_2 - \\delta = 0\\) non-inferiority margin \\(\\delta > 0\\); \\(\\delta < 0\\) correspond referred super-superiority Chan (2002), requiring experimental therapy shown superior control least margin \\(-\\delta>0\\).","code":""},{"path":"https://merck.github.io/gsdmvn/articles/PowerEvaluationWithSpending.html","id":"power-calculations","dir":"Articles","previous_headings":"","what":"Power Calculations","title":"Power Evaluation with Spending bounds","text":"begin fixed design simplify. follow approach Lachin (2009) compute power accounting different variance (statistical information) computations null hypothesis noted previous section. Noting asymptotic equivalence \\[Z_k\\approx \\hat\\theta_k/\\sigma_{0k}=\\frac{\\hat p_{1k} - \\hat p_{2k}}{\\sqrt{(1/n_{1k}+ 1/n_{2k})p_{0}(1- p_0)} }.\\] denote \\(n_k=n_{1k}+n_{2k},\\) \\(k=1,2,\\ldots, K\\) assume constant proportion \\(\\xi_i\\) randomized group \\(=1,2.\\) Thus, \\[Z_k\\approx \\frac{\\sqrt{n_k}(\\hat p_{1k} - \\hat p_{2k})}{\\sqrt{(1/\\xi_1+ 1/\\xi_2)p_{0}(1- p_0)} }.\\] asymptotic distribution \\[Z_k\\sim\\hbox{Normal}\\left(\\sqrt{n_k}\\frac{p_1 - p_2}{\\sqrt{(1/\\xi_1+ 1/\\xi_2) p_0(1- p_0)} },\\sigma^2_{0k}/\\sigma^2_{1k}\\right).\\] note \\[ \\sigma^2_{0k}/\\sigma^2_{1k} = \\frac{ p_0(1-p_0)\\left(1/\\xi_1+ 1/\\xi_2\\right)}{p_1(1-p_1)/\\xi_1+p_2(1-p_2)/\\xi_2}.\\] also note definition \\(\\sigma^2_{0k}/\\sigma^2_{1k}=\\mathcal I_k/\\mathcal I_{0k}.\\) Based input \\(p_1, p_2, \\xi_1, \\xi_2 = 1-\\xi_1, n_k\\) compute \\(\\theta, \\mathcal{}_k, \\mathcal{}_{0k}\\), \\(k=1,2,\\ldots,K\\). CAPTURE trial, Now examine information smaller assumed treatment difference alternative: can plug gs_power_npe() intended spending functions. begin power alternate hypothesis","code":"library(tibble)  gs_info_binomial <- function(p1, p2, xi1, n, delta = NULL){   if (is.null(delta)) delta <- p1 - p2   # Compute (constant) effect size at each analysis theta   theta <- rep(p1 - p2, length(n))   # compute null hypothesis rate, p0   p0 <- xi1 * p1 + (1 - xi1) * p2   # compute information based on p1, p2   info <-  n / (p1 * (1 - p1) / xi1 + p2 * (1 - p2) / (1 - xi1))   # compute information based on null hypothesis rate of p0   info0 <- n / (p0 * (1 - p0)*(1 / xi1 + 1 / (1 - xi1)))   # compute information based on H1 rates of p1star, p2star   p1star <- p0 + delta * xi1   p2star <- p0 - delta * (1 - xi1)   info1 <-  n / (p1star * (1 - p1star) / xi1 + p2star * (1 - p2star) / (1 - xi1))   return(tibble(Analysis = 1:length(n),                 n = n,                 theta = theta,                 theta1 = rep(delta, length(n)),                 info = info,                 info0 = info0,                 info1 = info1)) } h1 <- gs_info_binomial(p1 = .15, p2 = .1, xi1 = .5, n = c(350, 700, 1400)) h1 #> # A tibble: 3 × 7 #>   Analysis     n theta theta1  info info0 info1 #>      <int> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1   350  0.05   0.05  805.   800  805. #> 2        2   700  0.05   0.05 1609.  1600 1609. #> 3        3  1400  0.05   0.05 3218.  3200 3218. h <- gs_info_binomial(p1 = .15, p2 = .12, xi1 = .5, delta = .05, n = c(350, 700, 1400)) h #> # A tibble: 3 × 7 #>   Analysis     n theta theta1  info info0 info1 #>      <int> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1   350  0.03   0.05  751.  749.  753. #> 2        2   700  0.03   0.05 1502. 1499. 1507. #> 3        3  1400  0.03   0.05 3003. 2997. 3013. gs_power_npe(theta = h1$theta, theta1 = h1$theta, info = h1$info,              info0 = h1$info0, info1 = h1$info1,              upper = gs_spending_bound,              upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),              lower = gs_spending_bound,              lpar = list(sf = gsDesign::sfHSD, param = -2, total_spend = 0.2) ) #> # A tibble: 6 × 9 #>   Analysis Bound      Z Probability theta theta1  info info0 info1 #>      <int> <chr>  <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  4.33      0.00178  0.05   0.05  805.   800  805. #> 2        2 Upper  2.96      0.169    0.05   0.05 1609.  1600 1609. #> 3        3 Upper  1.97      0.794    0.05   0.05 3218.  3200 3218. #> 4        1 Lower -0.629     0.0203   0.05   0.05  805.   800  805. #> 5        2 Lower  0.295     0.0538   0.05   0.05 1609.  1600 1609. #> 6        3 Lower  1.94      0.200    0.05   0.05 3218.  3200 3218. gs_power_npe(theta = h$theta, theta1 = h$theta1, info = h$info,              info0 = h$info0, info1 = h$info1,              upper = gs_spending_bound,              upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),              lower = gs_spending_bound,              lpar = list(sf = gsDesign::sfHSD, param = -2, total_spend = 0.2) ) #> # A tibble: 6 × 9 #>   Analysis Bound      Z Probability theta theta1  info info0 info1 #>      <int> <chr>  <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  4.33     0.000224  0.03   0.05  751.  749.  753. #> 2        2 Upper  2.96     0.0359    0.03   0.05 1502. 1499. 1507. #> 3        3 Upper  1.97     0.364     0.03   0.05 3003. 2997. 3013. #> 4        1 Lower -0.675    0.0672    0.03   0.05  751.  749.  753. #> 5        2 Lower  0.230    0.195     0.03   0.05 1502. 1499. 1507. #> 6        3 Lower  1.85     0.594     0.03   0.05 3003. 2997. 3013."},{"path":"https://merck.github.io/gsdmvn/articles/PowerEvaluationWithSpending.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Power Evaluation with Spending bounds","text":"Chan, Ivan SF. 2002. “Power Sample Size Determination Noninferiority Trials Using Exact Method.” Journal Biopharmaceutical Statistics 12 (4): 457–69. Farrington, Conor P, Godfrey Manning. 1990. “Test Statistics Sample Size Formulae Comparative Binomial Trials Null Hypothesis Non-Zero Risk Difference Non-Unity Relative Risk.” Statistics Medicine 9 (12): 1447–54. Gordon, Ian, Ray Watson. 1996. “Myth Continuity-Corrected Sample Size Formulae.” Biometrics, 71–76. Investigators, Capture, others. 1997. “Randomised Placebo-Controlled Trial Abciximab Coronary Intervention Refractory Unstable Angina: Capture Study.” Lancet 349: 1429–35. Lachin, John M. 2009. Biostatistical Methods: Assessment Relative Risks. Vol. 509. John Wiley & Sons.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/QuickStart.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Quick Start for NPH Sample Size and Power","text":"provide simple examples use gsdmvn package deriving fixed group sequential designs non-proportional hazards. piecewise model enrollment, failure rates, dropout rates changing hazard ratio time allow great flexibility design assumptions. Users encouraged suggest features immediate long-term interest add. Topics included : Packages required used. Specifying enrollment rates. Specifying failure dropout rates possibly changing hazard ratio time. Deriving fixed design interim analysis. Simple boundary specification group sequential design. Deriving group sequential design non-proportional hazards. Displaying design properties. Design properties alternate assumptions. Differences gsDesign. Future enhancement priorities. items discussed briefly enable quick start early adopters also suggesting ultimate possibilities software enables. Finally, final section provides current enhancement priorities, potential topic-related enhancements discussed throughout document.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/QuickStart.html","id":"packages-used","dir":"Articles","previous_headings":"","what":"Packages Used","title":"Quick Start for NPH Sample Size and Power","text":"gsdmvn package used implement group sequential distribution theory non-proportional hazards derive wide variety boundary types group sequential designs. gsDesign package used check results proportional hazards well source deriving bounds using spending functions. gsDesign2 package provides computations compute expected event accumulation average hazard ratio time; key inputs group sequential distribution parameters. simtrial package used verify design properties using simulation. gsdmvn package likely likely incorporated eventually gsDesign2 package, resulting fully featured design package. However, features implementation gsdmvn allowed change needed agile rapid development phase.","code":"library(gsdmvn) library(gsDesign) library(gsDesign2) library(simtrial) library(knitr) library(dplyr)"},{"path":"https://merck.github.io/gsdmvn/articles/QuickStart.html","id":"enrollment-rates","dir":"Articles","previous_headings":"","what":"Enrollment Rates","title":"Quick Start for NPH Sample Size and Power","text":"Piecewise constant enrollment rates input tabular format. assume enrollment ramp-25%, 50%, 75% final enrollment rate 2 months followed steady state 100% enrollment another 6 months. rates increased later power design appropriately. However, fixed enrollment rate periods remain unchanged.","code":"enrollRates <- tibble::tibble(Stratum = \"All\", duration = c(2, 2, 2, 6), rate = (1:4) / 4) enrollRates #> # A tibble: 4 × 3 #>   Stratum duration  rate #>   <chr>      <dbl> <dbl> #> 1 All            2  0.25 #> 2 All            2  0.5  #> 3 All            2  0.75 #> 4 All            6  1"},{"path":"https://merck.github.io/gsdmvn/articles/QuickStart.html","id":"failure-and-dropout-rates","dir":"Articles","previous_headings":"","what":"Failure and Dropout Rates","title":"Quick Start for NPH Sample Size and Power","text":"Constant failure dropout rates specified study period stratum; consider single stratum . hazard ratio provided treatment/control hazard rate period stratum. dropout rate period assumed treatment group; restriction eliminated future version, needed. Generally, take advantage identity exponential distribution median \\(m\\), corresponding failure rate \\(\\lambda\\) \\[\\lambda = \\log(2) / m.\\] consider control group exponential time--event 12 month median. assume hazard ratio 1 4 months, followed hazard ratio 0.6 thereafter. Finally, assume low 0.001 exponential dropout rate treatment groups.","code":"medianSurv <- 12 failRates <- tibble::tibble(   Stratum = \"All\",   duration = c(4, Inf),   failRate = log(2) / medianSurv,   hr = c(1, .6),   dropoutRate = .001 ) failRates #> # A tibble: 2 × 5 #>   Stratum duration failRate    hr dropoutRate #>   <chr>      <dbl>    <dbl> <dbl>       <dbl> #> 1 All            4   0.0578   1         0.001 #> 2 All          Inf   0.0578   0.6       0.001"},{"path":"https://merck.github.io/gsdmvn/articles/QuickStart.html","id":"fixed-design","dir":"Articles","previous_headings":"","what":"Fixed Design","title":"Quick Start for NPH Sample Size and Power","text":"enrollment, failure dropout rate assumptions now derive sample size trial targeted complete 36 months interim analysis, 90% power 2.5% Type error. parameter upar = qnorm(1 - .025) case upper bound single analysis, parameter beta = 0.1 Type II error (1 - power). information fraction = 1 final analysis just statement analysis done 100% design planned endpoints. Finally, lpar = -Inf just means futility bound design. design can derived assumptions. three components resulting design. First, enrollment rates period increased proportionately size trial desired properties; duration enrollment rate changed. output failRates input. output bounds just upper bound \\(\\Phi^{-1}(1-\\alpha)\\) lower bound \\(-\\infty\\). targeted time single analysis Time column. average hazard ratio used compute sample size based Schoenfeld (1981) approximation AHR. targeted events analysis column Events. targeted time analysis Time. power Probability column row Upper bound. parameter theta natural parameter effect size outlined Jennison Turnbull (2000) elsewhere package; case -log(AHR). variables info info0 statistical information analysis alternate null hypothesis, respectively. note targeted Events approximately proposed Schoenfeld (1981) formula: difference gs_design_ahr() accounts null alternate hypothesis variance estimate theta=log(AHR) analysis yield slightly conservative event target due slower accumulation statistical information alternate hypothesis. See Lachin (2009) approach fixed design; knowledge, previously extended group sequential design. Due simplicity, may reasons allow approaches single variance estimate future releases. Finally, note shorter trial duration 30 months, need larger sample size targeted number events due larger expected AHR time analysis:","code":"# Type I error alpha <- .025 design <-   gs_design_ahr(     enrollRates = enrollRates,     failRates = failRates,     alpha = alpha,     beta = .1, # Type II error = 1 - power     analysisTimes = 36, # Planned trial duration     IF = 1, # Single analysis at information-fraction of 1     upar = qnorm(1 - alpha), # Final analysis bound     lpar = -Inf # No futility bound   ) names(design) #> [1] \"enrollRates\" \"failRates\"   \"bounds\" design$enrollRates %>% kable() design$bounds #> # A tibble: 1 × 11 #>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0 #>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper    36  434.   315.  1.96         0.9 0.693 0.366  77.7  78.8 gsDesign::nEvents(hr = design$bounds$AHR) #> [1] 313.535 gs_design_ahr(   enrollRates = enrollRates,   failRates = failRates,   alpha = alpha,   beta = .1, # Type II error = 1 - power   analysisTimes = 30, # Planned trial duration   IF = 1, # single analysis at information-fraction of 1   upar = qnorm(1 - alpha), # Final analysis bound   lpar = -Inf # No futility bound )$bounds %>% kable(digits = c(0, 0, 1, 0, 0, 3, 4, 3, 3, 2, 2))"},{"path":"https://merck.github.io/gsdmvn/articles/QuickStart.html","id":"group-sequential-design","dir":"Articles","previous_headings":"","what":"Group Sequential Design","title":"Quick Start for NPH Sample Size and Power","text":"go detail group sequential designs . brief, however, sequence tests \\(Z_1, Z_2,\\ldots, Z_K\\) follow multivariate normal distribution peformed test new treatment better control Jennison Turnbull (2000). assume \\(Z_k>0\\) favorable experimental treatment. Generally Type error set tests controlled null hypothesis treatment difference sequence bounds \\(b_1, b_2,\\ldots,b_K\\) chosen Type error \\(\\alpha > 0\\) \\[\\alpha = 1 - P_0(\\cap_{k=1}^K Z_k < b_k)\\] \\(P_0()\\) refers probability null hypothesis. referred non-binding bound since assumed trial stopped early futility \\(Z_k\\) small.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/QuickStart.html","id":"simple-efficacy-bound-definition","dir":"Articles","previous_headings":"Group Sequential Design","what":"Simple Efficacy Bound Definition","title":"Quick Start for NPH Sample Size and Power","text":"Lan DeMets (1983) developed spending function method deriving group sequential bounds. involves use non-decreasing spending function \\(f(t)\\) \\(t\\ge 0\\) \\(f(0)=0\\) \\(f(t)=\\alpha\\) \\(t \\ge 1\\). Suppose \\(K>0\\) analyses performed proportion \\(t_1< t_2 <\\ldots t_K=1\\) planned statistical information (e.g., proportion planned events time--event endpoint trial proportion observations binomial normal endpoint). Bounds first \\(k\\) analyses \\(1\\le k\\le K\\) recursively defined spending function multivariate normal distribution satisfy \\[f(t_k) = 1 - P_0(\\cap_{j=1}^k Z_j < b_j).\\] quick start, illustrate type efficacy bound. Perhaps common spending function approach Lan DeMets (1983) approximation O’Brien-Fleming bound \\[f(t) = 2-2\\Phi\\left(\\frac{\\Phi^{-1}(1-\\alpha/2)}{t^{1/2}}\\right).\\]  Suppose \\(K=3\\) \\(t_1=0.5\\), \\(t_2 = 0.75\\), \\(t_3 = 1\\). can define bounds gsDesign group sequential design function gsDesign() Lan-DeMets O’Brien-Fleming spending function \\(\\alpha = 0.025\\). Now can define one-sided group sequential design enrollment, failure dropout assumptions used previously. Bounds 3 analyses follows; note expected sample size time data cutoff analysis also N. filter upper bound lower bounds Z = -Inf shown. boundary crossing probabilities column labeled Probability alternate hypothesis. cumulative probabilities totaling 0.9 final analysis, representing 90% power. wish see boundary probabilities null hypothesis, can change hazard ratio 1 input failRates, use output enrollRates designs follows.","code":"b <- gsDesign::gsDesign(k = 3, timing = c(0.5, 0.75, 1), test.type = 1, alpha = 0.025, sfu = gsDesign::sfLDOF)$upper$bound b #> [1] 2.962588 2.359018 2.014084 design1s <- gs_design_ahr(   enrollRates = enrollRates,   failRates = failRates,   analysisTimes = 36, # Trial duration   upper = gs_spending_bound,   upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),   lower = gs_b,   lpar = rep(-Inf, 3), # No futility bound   IF = c(.5, .75, 1) ) design1s$bounds %>%   filter(Bound == \"Upper\") %>%   kable(digits = c(0, 0, 1, 0, 0, 3, 4, 3, 3, 2, 2)) gs_power_ahr(   enrollRates = design1s$enrollRates,   failRates = design1s$failRates %>% mutate(hr = 1),   upper = gs_spending_bound,   upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),   lpar = rep(-Inf, 3), # No futility bound   events = design1s$bound$Events[1:3] ) %>%   filter(Bound == \"Upper\") %>%   kable(digits = c(0, 0, 1, 0, 0, 3, 4, 3, 3, 2, 2))"},{"path":"https://merck.github.io/gsdmvn/articles/QuickStart.html","id":"two-sided-testing","dir":"Articles","previous_headings":"Group Sequential Design","what":"Two-Sided Testing","title":"Quick Start for NPH Sample Size and Power","text":"consider symmetric asymmetric 2-sided designs.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/QuickStart.html","id":"symmetric-2-sided-bounds","dir":"Articles","previous_headings":"Group Sequential Design > Two-Sided Testing","what":"Symmetric 2-sided bounds","title":"Quick Start for NPH Sample Size and Power","text":"symmetric design, can define bound using gsDesign::gsDesign(). example, bound identical b digits calculated. Now replicate gs_design_ahr(): Design bounds confirmed : bounds can plotted easily:","code":"b2 <- gsDesign::gsDesign(test.type = 2, sfu = sfLDOF, alpha = 0.025, timing = c(.5, .75, 1))$upper$bound b2 #> [1] 2.962588 2.359018 2.014084 design2ss <- gs_design_ahr(   enrollRates = enrollRates,   failRates = failRates,   analysisTimes = 36, # Trial duration   IF = c(.5, .75, 1), # Information fraction at analyses   upper = gs_spending_bound,   upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),   lower = gs_spending_bound,   lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),   h1_spending = FALSE ) design2ss$bounds %>% kable(digits = c(0, 0, 1, 0, 0, 3, 4, 3, 3, 2, 2)) ggplot(data = design2ss$bound, aes(x = Events, y = Z, group = Bound)) +   geom_line(aes(linetype = Bound)) +   geom_point() +   ggtitle(\"2-sided symmetric bounds with O'Brien-Fleming-like spending\")"},{"path":"https://merck.github.io/gsdmvn/articles/QuickStart.html","id":"asymmetric-2-sided-bounds","dir":"Articles","previous_headings":"Group Sequential Design > Two-Sided Testing","what":"Asymmetric 2-sided bounds","title":"Quick Start for NPH Sample Size and Power","text":"Asymmetric 2-sided designs common symmetric since objectives two bounds tend different. often caution analyze early efficacy use conservative bound; principles used example designs far. Stopping lack benefit experimental treatment control overt indication unfavorable trend generally might examined early bounds less stringent. add early futility analysis nominal 1-sided p-value 0.05 wrong direction (\\(Z=\\Phi^{-1}(0.05)\\) 30% 50% events accrued. might considered disaster check. point time, may perceived need futility analysis. efficacy, add infinite bound first interim analysis. now slightly larger sample size account possibility early futility stop. Bounds now: see need (finite) stopping rule bound analysis. , futility bound first 2 analyses efficacy bound last 3 analyses. still targeted power. efficacy bound changed first design. reason changing address regulator concerns bounds always stopped . However, bound obeyed, Type error can seen slightly reduced follows. price generally acceptable regulatory acceptance operational flexibility enabled controlling Type error even futility bound obeyed.","code":"b2sa <- c(Inf, b) # Same efficacy bound as before a2sa <- c(rep(qnorm(.05), 2), rep(-Inf, 2)) # Single futility analysis bound  design2sa <- gs_design_nph(   enrollRates = enrollRates,   failRates = failRates,   analysisTimes = 36, # Trial duration   upper = gs_b, upar = b2sa,   lower = gs_b, lpar = a2sa, # Asymmetric 2-sided bound   IF = c(.3, .5, .75, 1) ) design2sa$enrollRates %>% summarise(N = ceiling(sum(rate * duration))) #> # A tibble: 1 × 1 #>       N #>   <dbl> #> 1   459 design2sa$bounds %>%   filter(abs(Z) < Inf) %>%   kable(digits = c(0, 0, 2, 4, 2, 1, 2, 1, 1, 1)) events <- (design2sa$bounds %>% filter(Bound == \"Upper\"))$Events gs_power_nph(   enrollRates = design1s$enrollRates,   failRates = design2sa$failRates %>% mutate(hr = 1),   upar = b2sa,   lpar = a2sa,   events = events,   maxEvents = max(events) ) %>%   # filter eliminates bounds that are infinite   filter(abs(Z) < Inf) %>% kable(digits = c(0, 0, 2, 4, 1, 1, 2, 2, 1, 1))"},{"path":"https://merck.github.io/gsdmvn/articles/QuickStart.html","id":"confirmation-by-simulation","dir":"Articles","previous_headings":"","what":"Confirmation by Simulation","title":"Quick Start for NPH Sample Size and Power","text":"small simulation approximate boundary crossing probabilities just shown 2-sided asymmetric design design2sa. First, generate test statistics analysis number simulated trials. Now analyze individual trials summarize results. larger simulation required accurately assess asymptotic approximation boundary crossing probabilities.","code":"fr <- simfix2simPWSurv(failRates = failRates) nsim <- 200 # Number of trial simulations simresult <- NULL N <- ceiling(design2sa$enrollRates %>% summarize(N = sum(rate / 2 * duration))) * 2 K <- max(design2sa$bounds$Analysis) events <- ceiling(sort(unique(design2sa$bounds$Events)))  for (i in 1:nsim) {   sim <- simPWSurv(     n = as.numeric(N),     enrollRates = design2sa$enrollRates,     failRates = fr$failRates,     dropoutRates = fr$dropoutRates   )   for (k in 1:K) {     Z <- sim %>%       cutDataAtCount(events[k]) %>% # Cut simulation for analysis at targeted events       tensurv(txval = \"Experimental\") %>%       tenFH(rg = tibble(rho = 0, gamma = 0))     simresult <- rbind(       simresult,       tibble(sim = i, k = k, Z = -Z$Z) # Change sign for Z     )   } } bds <- tibble::tibble(   k = sort(unique(design2sa$bounds$Analysis)),   upper = (design2sa$bounds %>% filter(Bound == \"Upper\"))$Z,   lower = (design2sa$bounds %>% filter(Bound == \"Lower\"))$Z ) trialsum <- simresult %>%   full_join(bds, by = \"k\") %>%   filter(Z < lower | Z >= upper | k == K) %>%   group_by(sim) %>%   slice(1) %>%   ungroup() trialsum %>% summarize(   nsim = n(),   \"Early futility (%)\" = 100 * mean(Z < lower),   \"Power (%)\" = 100 * mean(Z >= upper) ) #> # A tibble: 1 × 3 #>    nsim `Early futility (%)` `Power (%)` #>   <int>                <dbl>       <dbl> #> 1   200                  2.5        85.5 xx <- trialsum %>% mutate(Positive = (Z >= upper)) table(xx$k, xx$Positive) #>     #>     FALSE TRUE #>   1     5    0 #>   2     0    3 #>   3     0   91 #>   4    24   77"},{"path":"https://merck.github.io/gsdmvn/articles/QuickStart.html","id":"differences-from-gsdesign","dir":"Articles","previous_headings":"","what":"Differences From gsDesign","title":"Quick Start for NPH Sample Size and Power","text":"sample size computation gsdmvn package slightly different gsDesign package proportional hazards model. demonstrate bounds 1-sided test proportional hazards model underlying hazard ratio 0.7. results total sample size: Now derive design targeted properties using gsDesign package. Enrollment rates :","code":"design1sPH <- gs_design_nph(   enrollRates = enrollRates,   failRates = failRates %>% mutate(hr = .7),   analysisTimes = 36, # Trial duration   upar = b,   lpar = rep(-Inf, 3), # No futility bound   IF = c(.5, .75, 1) ) design1sPH$enrollRates %>% kable() design1sPH$enrollRates %>%   tail(1) %>%   select(N) #> # A tibble: 1 × 1 #>       N #>   <dbl> #> 1  460. x <- gsSurv(   k = 3,   test.type = 1,   alpha = .025,   beta = .1,   timing = c(.5, .75, 1),   sfu = sfLDOF,   lambda = log(2) / medianSurv,   hr = .7,   eta = .001, # Dropout rate   gamma = c(.25, .5, .75, 1),   R = c(2, 2, 2, 6),   minfup = 24,   T = 36 ) gsBoundSummary(x) %>% kable(row.names = FALSE) x$gamma %>% kable()"},{"path":"https://merck.github.io/gsdmvn/articles/QuickStart.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Quick Start for NPH Sample Size and Power","text":"Jennison, Christopher, Bruce W. Turnbull. 2000. Group Sequential Methods Applications Clinical Trials. Boca Raton, FL: Chapman; Hall/CRC. Lachin, John M. 2009. Biostatistical Methods: Assessment Relative Risks. Vol. 509. John Wiley & Sons. Lan, K. K. G., David L. DeMets. 1983. “Discrete Sequential Boundaries Clinical Trials.” Biometrika 70: 659–63. Schoenfeld, David. 1981. “Asymptotic Properties Nonparametric Tests Comparing Survival Distributions.” Biometrika 68 (1): 316–19.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Spending Time Examples","text":"multiple scenarios event-based spending group sequential designs limitations terms ensuring adequate follow-ensuring adequate spending preserved final analysis. Example contexts often arises trials may delayed treatment effect, control failure rates different expected, multiple hypotheses tested. general, situations found ensuring adequate follow-duration adequate number events important fully evaluate potential effectiveness new treatment. testing multiple hypotheses, carefully thinking possible spending issues can critical. addition, group sequential trials, preserving adequate \\(\\alpha\\)-spending final evaluation hypothesis important difficult using traditional event-based spending. document, outline three examples demonstrate issues: importance adequate events adequate follow-duration ensure power fixed design, importance guaranteeing reasonable amount \\(\\alpha\\)-spending final analysis group sequential design. trial examining outcome biomarker positive overall populations, show importance considering design reacts incorrect design assumptions biomarker prevalence. group sequential design options, demonstrate concept spending time effective way adapt. Traditionally Lan DeMets (1983), spending done according targeting specific number events outcome end trial. However, delayed treatment effect scenarios substantial literature (e.g., Lin et al. (2020), Roychoudhury et al. (2021)) documenting importance adequate follow-duration addition requiring adequate number events traditional proportional hazards assumption. approaches taken, found spending time approach generalizes well addressing variety scenarios. fact spending need correspond information fraction perhaps first raised Lan DeMets (1989) calendar-time spending discussed. However, note Proschan, Lan, Wittes (2006) raised scenarios spending alternatives considered. Two specific spending approaches suggested : Spending according minimum planned observed event counts. suggested delayed effect examples. Spending common spending time across multiple hypotheses; e.g., multiple population example, spending overall population rate biomarker positive subgroup regardless event counts time overall population. consistent Follmann, Proschan, Geller (1994) applied multiple experimental treatments compared common control. Spending time case corresponds approach Fleming, Harrington, O’Brien (1984) fixed incremental spending set potentially variable number interim analyses. document fairly long demonstrates number scenarios relevant spending time concept. layout intended make easy possibly focus individual examples interested full review. Code available unhide interested implementation. Rather bog conceptual discussion implementation details, tried provide sufficient comments code guide implementation interested .","code":""},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"delayed-effect-scenario","dir":"Articles","previous_headings":"","what":"Delayed effect scenario","title":"Spending Time Examples","text":"consider example single stratum possibility delayed treatment effect. next two sections consider 1) fixed design interim analysis, 2) design interim analysis. Following common assumptions: control group time--event exponentially distributed median 12 months. 2.5% one-sided Type error. 90% power. constant enrollment rate expected enrollment duration 12 months. targeted trial duration 30 months. delayed effect experimental group compared control, hazard ratio 1 first 4 months hazard ratio 0.6 thereafter. restrictions constant control failure rate, two hazard ratio time intervals constant enrollment required, simplify example. approach taken uses average-hazard ratio approach approximating treatment effect Mukhopadhyay et al. (2020) asymptotic group sequential theory Tsiatis (1982).","code":"m <- 12 # Control median enrollRates <- tibble(Stratum=\"All\", duration = 12, rate = 1) failRates <- tibble(Stratum=\"All\", duration = c(4, 100), hr = c(1, .6),                      failRate = log(2) / m, dropoutRate = .001)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"fixed-design-delayed-effect","dir":"Articles","previous_headings":"","what":"Fixed design, delayed effect","title":"Spending Time Examples","text":"sample size events design shown . see average hazard ratio (AHR) assumptions 0.703, part way early HR 1 later HR 0.6 assumed experimental versus control therapy.","code":"# Output table function for fixed design  table_fixed_design <- function(x, enrollRates){   N <- sum(enrollRates$rate * enrollRates$duration)   x %>%     filter(Bound == \"Upper\") %>%     transmute(Time = Time, N = ceiling(N), Events = ceiling(Events), \"Nominal p\" = pnorm(-Z), AHR = AHR, Power = Probability) %>%   gt() %>%    fmt_number(columns = c(\"N\", \"Events\"), decimals=0) %>%   fmt_number(columns = \"Time\", decimals = 1) %>%   fmt_number(columns = c(\"Nominal p\", \"Power\", \"AHR\"), decimals=3) } # Bounds for fixed design are just a fixed bound for nominal p = 0.025, 1-sided Z_025 <- qnorm(.975)  # Fixed design, single stratum # Find sample size for 30 month trial under given  # enrollment and sample size assumptions xx <- gs_design_ahr(enrollRates,                      failRates,                      analysisTimes= 30,                     upar = Z_025,                      lpar = Z_025) xx$bounds %>% table_fixed_design(enrollRates)"},{"path":[]},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"scenario-with-less-experimental-benefit","dir":"Articles","previous_headings":"Fixed design, delayed effect > Power when assumptions design are wrong","what":"Scenario with less experimental benefit","title":"Spending Time Examples","text":"assume instead effect delay 6 months instead 4 control median 10 months instead 12, substantial impact power. , assumed targeted events required final analysis resulting expected final analysis time 25 months instead planned 30 average hazard ratio 0.78 expected time analysis rather targeted average hazard ratio 0.70 original assumptions. Now also require 30 months trial duration addition targeted events. improves power 63% 76% increase 25 30 months duration 340 377 expected events, important gain. driven average hazard ratio 0.78 compared 0.76 increased expected number events. also ensures adequate follow-better describe longer-term differences survival; may particularly important early follow-suggests delayed effect crossing survival curves. Thus, adaptation event-based design based also require adequate follow-can help ensure power large clinical trial investment clinically relevant underlying survival benefit.","code":"am <- 10 # Alternate control median failRates$duration[1] <- 6 failRates$failRate <- log(2) / am yy <-    gs_power_ahr(       enrollRates = xx$enrollRates,       failRates = failRates,       events = xx$bounds$Events,       upar = Z_025,       lpar = Z_025 ) yy %>% table_fixed_design(xx$enrollRates) yy <-    gs_power_ahr(       enrollRates = xx$enrollRates,       failRates = failRates,       events = xx$bounds$Events,       analysisTimes = 30,       upar = Z_025,       lpar = Z_025 ) yy %>% table_fixed_design(xx$enrollRates)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"scenario-with-low-control-event-rates","dir":"Articles","previous_headings":"Fixed design, delayed effect","what":"Scenario with low control event rates","title":"Spending Time Examples","text":"Now assume longer planned control median, 16 months demonstrate value retaining event count requirement. analyze 30 months, power trial 87% 288 events expected. also require adequate events, restore power 94.5, originally targeted level 90%. cost expected trial duration becomes 38.5 months rather 30; however, since control median now larger, additional follow-useful characterize tail behavior. Note scenario likely particularly interested retaining power treatment effect actually stronger original alternate hypothesis. Thus, example, time cutoff alone ensured sufficient follow-power trial.","code":"am <- 16 # Alternate control median failRates$failRate <- log(2) / am failRates$duration[1] <- 4 yy <-    gs_power_ahr(       enrollRates = xx$enrollRates,       failRates = failRates,       events = 1,       analysisTimes = 30,       upar = Z_025,       lpar = Z_025 ) yy %>% table_fixed_design(xx$enrollRates) yy <-    gs_power_ahr(       enrollRates = xx$enrollRates,       failRates = failRates,       events = xx$bounds$Events,       analysisTimes = 30,       upar = Z_025,       lpar = Z_025 ) yy %>% table_fixed_design(xx$enrollRates)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"conclusions-for-fixed-design","dir":"Articles","previous_headings":"Fixed design, delayed effect","what":"Conclusions for fixed design","title":"Spending Time Examples","text":"summary, demonstrated value requiring adequate events adequate follow-duration approach analysis done one requirements. Requiring retain power important treatment benefit characterization time potential delayed onset positive beneficial treatment effect.","code":""},{"path":[]},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"alternative-spending-strategies","dir":"Articles","previous_headings":"Group sequential design","what":"Alternative spending strategies","title":"Spending Time Examples","text":"extend design detect delayed effect group sequential design single interim analysis 80% final planned events accrued. assume final analysis require targeted trial duration events based fixed design based evaluations . assume efficacy bound uses Lan DeMets (1983) spending function approximating O’Brien-Fleming bound. futility bound planned, exception demonstration one scenario. interim analysis far enough trial substantial probability stopping early design assumptions. Coding different strategies must done carefully. time design, specify spending function specifying use information fraction design. wished use 22 30 months calendar analysis times use calendar fraction spending, need specify spending time design. Next show set information-based spending power calculation timing analysis based information fraction; e.g., propose requiring achieving planned event counts, also planned study duration analysis performed. critical set maximum planned information update information fraction calculation case. final case replace information fraction design specific spending time plugged spending function compute incremental \\(\\alpha\\)-spending analysis. case, use planned information fraction design, 0.8 interim analysis 1 final analysis. used regardless scenario using compute power, recall information fraction still used computing correlations asymptotic distribution approximation design tests. Finally, set function print table describe characteristics design updates updated different scenarios.","code":"# Spending for design with planned information fraction upar_design_IF <- list(                        # total_spend represents one-sided Type I error                        total_spend = 0.025,                        # Spending function and associated                         # parameter (NULL, in this case)                        sf = sfLDOF, param = NULL,                        # Do NOT specify spending time here as it will be set                        # by information fraction specified in call to                        # gs_design_ahr()                        timing = NULL,                        # Do NOT specify maximum information here as it will be                        # set as the design maximum information                        max_info = NULL ) upar_design_CF <- upar_design_IF # Now switch spending time to calendar fraction upar_design_CF$timing <- c(22, 30)/30 # We now need to change max_info from spending as specified for design upar_actual_IF <- upar_design_IF # Note that we still have timing = NULL, unchanged from information-based design upar_actual_IF <- NULL # Replace NULL maximum information with planned maximum null hypothesis # information from design # This max will be updated for each planned design later upar_actual_IF$max_info <- 100 # Copy original upper planned spending upar_planned_IF <- upar_design_IF # Interim and final spending time will always be the same, regardless of  # expected events or calendar timing of analysis upar_planned_IF$timing <- c(0.8, 1) # We will reset planned maximum information later # Function to print table for group sequential design # Desire is to print both information fraction relative to planned with planned # input in maxinfo0.  # Input spending_time here needs to be consistent with what is given in design; # if NULL, information fraction will be used to compute spending time based in input max_info0. # Enrollment rates input are used to compute sample size. # This works for cases here, but N calculation will NOT work for # IA's before planned enrollment completion; for that gs_power_ahr needs to be fixed table_gs_design <- function(x, enrollRates, spending_time = NULL, max_info0 = NULL){   N <- sum(enrollRates$rate * enrollRates$duration)   if(is.null(max_info0)) stop(\"Must enter maximum H0 planned information in max_info0\")   x$N <- N   if (is.null(spending_time)) {x$spending_time <- pmin(x$info0 / max_info0, 1)   }else x$spending_time <- spending_time   x %>%       transmute(Time = Time, N = ceiling(N), Events = ceiling(Events), \"Information fraction\" = info0 / max_info0,                \"Spending time\" = spending_time,                \"Nominal p\" = pnorm(-Z), AHR = AHR, \"~HR at bound\" = exp(-Z / sqrt(info)), Power = Probability) %>%      gt() %>%      fmt_number(columns = c(\"N\",\"Events\"), decimals = 0) %>%      fmt_number(columns = \"Time\", decimals=1) %>%      fmt_number(columns = c(\"AHR\", \"Power\",\"Information fraction\", \"Spending time\", \"~HR at bound\"), decimals=3) %>%      fmt_number(columns = \"Nominal p\", decimals = 4) }"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"planned-design","dir":"Articles","previous_headings":"Group sequential design","what":"Planned design","title":"Spending Time Examples","text":"extend design studied group sequential design single interim analysis 80% final planned events accrued. assume final analysis require targeted trial duration events based fixed design evaluations made . assume efficacy bound uses Lan-DeMets spending function approximating O’Brien-Fleming bound. futility bound planned. interim analysis far enough trial substantial probability stopping early design assumptions.","code":"m <- 12 # Control median failRates$failRate[1] <- log(2) / m failRates$duration[1] <- 4 # Planned information fraction at interim(s) and final planned_IF <- c(.8,1) # No futility bound lpar <- rep(-Inf,2) # lower Z bound of -Inf at all analyses enrollRates <- tibble(Stratum=\"All\", duration = 12, rate = 1) failRates <- tibble(Stratum=\"All\", duration = c(4, 100), hr = c(1, .6),                      failRate = log(2) / m, dropoutRate = .001) # Note that timing here matches what went into planned_IF above # Final analysis time set to targeted study duration; analysis times before are 'small' # to ensure use of information fraction for timing xx <- gs_design_ahr(enrollRates, failRates, analysisTimes = c(1,30), IF = planned_IF,       upper = gs_spending_bound, upar = upar_design_IF,        lower = gs_b, lpar = lpar) # Get upper bounds planned_bounds <- xx$bounds %>% filter(Bound == \"Upper\") # Planned number of analyses K <- nrow(planned_bounds) # Planned events max_events <- max(planned_bounds$Events) # save max information planned under H0 # This will be used for future information fraction calculations # when event accumulation is not same as planned max_info0 <- max(xx$bounds$info0) # Planned analysis timing planned_time <- planned_bounds$Time planned_bounds %>% table_gs_design(xx$enrollRates, max_info0 = max(xx$bounds$info0))"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"two-alternate-approaches","dir":"Articles","previous_headings":"Group sequential design","what":"Two alternate approaches","title":"Spending Time Examples","text":"consider two alternate approaches demonstrate spending time concept may helpful practice. However, skipping following two subsections can done interest. first demonstrates calendar spending Lan DeMets (1989). second basically method Fleming, Harrington, O’Brien (1984) fixed incremental spend used potentially variable number interim analyses, final bound computed based unspent one-sided Type error assigned hypothesis.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"calendar-spending","dir":"Articles","previous_headings":"Group sequential design > Two alternate approaches","what":"Calendar spending","title":"Spending Time Examples","text":"use sample size , change efficacy bound spending calendar-based. reason spending different information-based spending mainly due fact expected information linear time. case, calendar fraction interim less information fraction, exactly opposite true earlier trial. just note calendar-based spending chosen, may worth comparing design bounds bounds using spending function, information-based spending see important differences trial team possibly scientific regulatory community. note also risk enough events achieve targeted power final analysis calendar-based spending strategy. examine calendar-based spending document.","code":"yy <-    gs_power_ahr(enrollRates = xx$enrollRates,                 failRates = xx$failRates,                 events = 1:2, # Planned time will drive timing since information accrues faster                analysisTimes = c(22, 30), # Interim time rounded                upper = gs_spending_bound, upar = upar_design_CF, lpar = lpar) %>% filter(Bound == \"Upper\") actual_IF <- NULL # yy$info0 / max(yy$info0) yy %>% table_gs_design(xx$enrollRates, spending_time = upar_design_CF$timing, max_info0 = max(yy$info0))"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"fixed-incremental-spend-with-a-variable-number-of-analyses","dir":"Articles","previous_headings":"Group sequential design > Two alternate approaches","what":"Fixed incremental spend with a variable number of analyses","title":"Spending Time Examples","text":"noted, method proposed Fleming, Harrington, O’Brien (1984). general strategy demonstrated interim analses every 6 months final targeted follow-time cumulative number events achieved. efficacy analyses start, fixed incremental spend 0.001 used interim. criteria final analysis met, remaining \\(\\alpha\\) spent. Cumulative spending months 18 24 0.001 0.002, respectively, full cumulative \\(\\alpha\\)-spending 0.025 final analysis. done setting spending time 18 24 months 1/25, 2/25 1; .e., 1/25 incremental \\(\\alpha\\)-spending incorporated interim analysis remaining \\(\\alpha\\) spent final analysis. enables strategy analyzing every 6 months minimum targeted follow-minimum number events observed, time final analysis performed. skip efficacy analyses first two interim analyses months 6 12. futility, simply use nominal 1-sided p-value 0.05 favoring control interim. note raises flag futility bound crossed Data Monitoring Committee (DMC) can choose continue trial even futility bound crossed. However, bound may effective providing DMC guidance stop futility prematurely. comparison designs, leave enrollment rates, failure rates, dropout rates final analysis time . see following table summarizing efficacy bounds power little impact total power futility analyses specified. cumulative \\(\\alpha\\)-spending 0.001 0.002 efficacy interim analyses, see nominal p-value bound second interim 0.0015, 0.001 incremental \\(\\alpha\\)-spend. also note nominal p-values testing, approximate hazard ratio required cross bounds presumably help justify consideration completing trial based definitive interim efficacy finding. Also, small interim spend, final nominal p-value reduced much overall \\(\\alpha=0.025\\) Type error set group sequential design. also examine futility bound. nominal p-value 0.05 analysis one-sided p-value favor control experimental treatment. can see probability stopping early alternate hypothesis (\\(\\beta\\)-spending) substantial even given early delayed effect. Also, substantial approximate observed hazard ratios cross futility bound seem reasonable given timing number events observed; exception small number events first interim, larger number observed time early excess risk. may useful plan additional analyses futility bound crossed support stopping . example, looking subgroups evaluating smoothed hazard rates time treatment group may useful. clinical trial study team complete discussion futility bound considerations time design.","code":"# Cumulative spending at IA3 and IA4 will be 0.001 and 0.002, respectively. # Power spending function sfPower with param = 1 is linear in timing # which makes setting the above cumulative spending targets simple by # setting timing variable the the cumulative proportion of spending at each analysis. # There will be no efficacy testing at IA1 or IA2. # Thus, incremental spend, which will be unused, is set very small for these analyses. upar_FHO <- list(total_spend = 0.025,                  sf = sfPower,                  param = 1,                  timing = c((1:2)/250, (1:2)/25, 1) ) FHO <-   gs_power_ahr(enrollRates = xx$enrollRates,                failRates = xx$failRates,                events = NULL,                analysisTimes = seq(6, 30, 6),                # No efficacy testing at IA1 or IA2                # Thus, the small alpha the spending function would have                # allocated will not be used                test_upper = c(FALSE, FALSE, TRUE, TRUE, TRUE),                upper = gs_spending_bound,                upar = upar_FHO,                lpar = c(rep(qnorm(.05), 4), -Inf), ) FHO %>% filter(Bound == \"Upper\", Z < Inf) %>% table_gs_design(xx$enrollRates, spending_time = upar_FHO$timing[3:5], max_info0 = max(FHO$info0)) FHO %>%    filter(Bound == \"Lower\", abs(Z) < Inf) %>%   transmute(Time = Time,              Events = ceiling(Events),             \"Nominal p\" = pnorm(Z),              \"Information fraction\" = info0 / max_info0,             \"~HR at bound\" = exp(-Z / sqrt(info)),             \"Cumulative beta-spend\" = Probability             ) %>%   gt() %>%   fmt_number(columns = c(\"Events\"), decimals = 0) %>%   fmt_number(columns = \"Time\", decimals=1) %>%   fmt_number(columns = c(\"~HR at bound\", \"Cumulative beta-spend\",\"Information fraction\"), decimals=3) %>%   fmt_number(columns = \"Nominal p\", decimals = 4)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"less-treatment-effect-scenario","dir":"Articles","previous_headings":"Group sequential design","what":"Less treatment effect scenario","title":"Spending Time Examples","text":", compute power assumption changing median control group time--event 10 months rather assumed 12 delay effect onset 6 months rather 4. otherwise change enrollment, dropout hazard ratio assumptions. following examples, require targeted number events targeted trial duration group sequential design interim final analyses. first example, uses interim spending based event count observed originally planned final event count information fraction 323 / 355 = 0.91. gives event-based spending 0.0191, substantially targeted information fraction 284 / 355 = 0.8 targeted interim spending 0.0122. reduces power overall 76% 73% lowers nominal p-value bound final analysis 0.0218 0.0165; see following two tables. Noting average hazard ratio 0.8 interim 0.76 final analysis emphasizes value preserving \\(\\alpha\\)-spending final analysis. Thus, example valuable limit spending interim analysis minimum planned spending opposed using event-based spending. Just important, general design principle making interim analysis criteria stringent final ensured alternate scenario. multiple trials delayed effects observed difference final nominal p-value bound made difference ensure statistically significant finding.","code":"am <- 10 # Alternate control median failRates$failRate <- log(2) / am failRates$duration[1] <- 6 # Set planned maximum information from planned design max_info0 <- max(xx$bounds$info0) upar_actual_IF <- upar_design_IF upar_actual_IF$max_info <- max_info0 # compute power if actual information fraction relative to original # planned total is used yy <-    gs_power_ahr(enrollRates = xx$enrollRates,                 failRates = failRates,                 events = 1:2, # Planned time will drive timing since information accrues faster                analysisTimes = planned_time,                upper = gs_spending_bound, upar = upar_actual_IF, lpar = lpar) %>% filter(Bound == \"Upper\") actual_IF <- yy$info0 / max_info0 yy %>% table_gs_design(xx$enrollRates, spending_time = actual_IF, max_info0 = max_info0) yz <-    gs_power_ahr(enrollRates = xx$enrollRates,                 failRates = failRates,                 events = xx$bounds$Events[1:2],                analysisTimes = planned_time,                upper = gs_spending_bound,                 upar = upar_planned_IF,                 lpar = lpar) %>% filter(Bound == \"Upper\") # Note that max_info0 is denominator to compute \"Information fraction\" in table # However, spending time is less since we use planned IF to compute spending yz %>% table_gs_design(xx$enrollRates, spending_time = planned_IF, max_info0 = max_info0)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"scenario-with-longer-control-median","dir":"Articles","previous_headings":"Group sequential design","what":"Scenario with longer control median","title":"Spending Time Examples","text":"Now return example control median longer expected confirm spending according planned level alone without considering actual number events also result power reduction. power gain great (94.2% vs 95.0%) interim final p-value bounds aligned intent emphasizing final analysis smaller average hazard ratio expected (0.680 vs 0.723 interim). First, show result using planned spending. Since number events less expected, used actual number events interim bound stringent obtain slightly greater power.","code":"am <- 16 # Alternate control median failRates$failRate <- log(2) / am # Return to 4 month delay with HR=1 before HR = 0.6 failRates$duration[1] <- 4 # Start with spending based on planned information # which is greater than actual information yy <-    gs_power_ahr(enrollRates = xx$enrollRates,                 failRates = failRates,                 events = c(1,max_events),                analysisTimes = planned_time,                upper = gs_spending_bound, upar = upar_planned_IF, lpar = lpar) %>% filter(Bound == \"Upper\") yy %>% table_gs_design(xx$enrollRates, spending_time = planned_IF, max_info0 = max_info0) yz <-    gs_power_ahr(enrollRates = xx$enrollRates,                 failRates = failRates,                 events = c(1,xx$bounds$Events[2]),                analysisTimes = planned_time,                upper = gs_spending_bound, upar = upar_actual_IF, lpar = lpar) %>% filter(Bound == \"Upper\") yz %>% table_gs_design(xx$enrollRates, spending_time = NULL, max_info0 = max_info0)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"summary-for-spending-time-motivation-assuming-delayed-benefit","dir":"Articles","previous_headings":"Group sequential design","what":"Summary for spending time motivation assuming delayed benefit","title":"Spending Time Examples","text":"summary, using minimum planned actual spending adapt design based event-based spending adapts interim bound stringent final bound different scenarios ensures better power event-based interim analysis spending.","code":""},{"path":[]},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"assumptions","dir":"Articles","previous_headings":"Testing multiple hypotheses","what":"Assumptions","title":"Spending Time Examples","text":"consider simple case use method Maurer Bretz (2013) test overall population biomarker subgroup endpoint. assume exponential failure rate median 12 control group regardless population. hazard ratio biomarker positive subgroup assumed 0.6, negative population 0.8. assume biomarker positive group represents half population, meaning enrollment rates assumed negative positive patients. difference failure rates two strata hazard ratio. case, assume proportional hazards within negative (HR = 0.8) positive (HR = 0.6) patients. illustrative purposes, choosing strategy based possible feeling much less certainty study start whether underlying benefit biomarker negative population. wish ensure power biomarker positive group, allow good chance positive overall population finding lesser benefit biomarker negative population. alternative trial strategy planned, alternate approach following considered. case, design first biomarker positive population one-sided Type error controlled \\(\\alpha = 0.0125\\):","code":"enrollRates <- tibble(Stratum=c(\"Positive\",\"Negative\"), duration = 12, rate=20) m <- 12 failRates <- tibble(Stratum = c(\"Positive\", \"Negative\"), hr = c(0.6, 0.8),                     duration = 100,                     failRate = log(2) / m, dropoutRate = 0.001)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"planned-design-for-biomarker-positive-population","dir":"Articles","previous_headings":"Testing multiple hypotheses","what":"Planned design for biomarker positive population","title":"Spending Time Examples","text":"","code":"# Spending based on information fraction # At time of design, timing = NULL is used to select # maximum planned information for information fraction. # Since execution will be event-based for biomarker population, # there will be no need to change spending plan for different scenarios. # Total alpha spend is now 0.0125  upar_design_spend <- list(sf = gsDesign::sfLDOF, total_spend = 0.0125, param = NULL, timing = NULL) # No futility bound lpar <- rep(-Inf,2) # Z = -infinity for lower bound # We will base the combined hypothesis design to ensure power in the biomarker subgroup positive <-    gs_design_ahr(enrollRates = enrollRates %>% filter(Stratum == \"Positive\"),                 failRates = failRates %>% filter(Stratum == \"Positive\"),                 # Following drives information fraction for interim                 IF = c(.8, 1),                  # Total study duration driven by final analysisTimes value                 # Enter small increasing values before that so information                 # fraction in planned_IF drives timing of interims                 analysisTimes = c(1,30),                 upper =gs_spending_bound, upar = upar_design_spend,                 # Fixed lower bound with Z = -infinity                 lower = gs_b, lpar = lpar                ) # This planned design will drive adaptation for deviations from plan positive_planned_bounds <- positive$bounds %>% filter(Bound == \"Upper\") # Planned timing of analysis positive_planned_time <- positive_planned_bounds$Time # Planned sample size positive_planned_N <- max(positive_planned_bounds$N) # Planned events positive_max_events <- max(positive_planned_bounds$Events) # save max information planned under H0 positive_max_info0 <- max(positive_planned_bounds$info0) positive_planned_bounds %>%    table_gs_design(enrollRates = positive$enrollRates,                    max_info0 = positive_max_info0)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"planned-design-for-overall-population","dir":"Articles","previous_headings":"Testing multiple hypotheses","what":"Planned design for overall population","title":"Spending Time Examples","text":"adjust overall study enrollment rate match design requirement biomarker positive population. Now can examine power overall population based hazard ratio assumptions biomarker negative biomarker positive subgroups just calculated enrollment assumption. use analysis times biomarker positive population design. see interim information fraction overall population slightly greater biomarker positive population . compensate enable flexibility biomarker positive prevalence changes, use spending time biomarker positive subgroup regardless true fraction final planned events analysis. Thus, interim nominal p-value bound biomarker positive overall populations. make much difference , see natural way adapt design observed biomarker positive prevalence different assumed design.","code":"# Get enrollment rate inflation factor compared to originally input rate inflation_factor <- positive$enrollRates$rate[1] / enrollRates$rate[1] # Using this inflation factor, set planned enrollment rates planned_enrollRates <- enrollRates %>% mutate(rate = rate * inflation_factor) planned_enrollRates %>% gt() # Store overall enrollment rates for future use overall_enrollRates <-    planned_enrollRates %>%    summarize(Stratum = \"All\", duration=first(duration), rate = sum(rate)) # Set total spend for overall population, O'Brien-Fleming spending function, and  # same spending time as biomarker subgroup upar_overall_planned_IF <- list(sf = gsDesign::sfLDOF,  param = NULL,                                    total_spend = 0.0125, timing = c(.8, 1),                   # We will use actual final information as planned initially                                    max_info = NULL) overall_planned_bounds <-    gs_power_ahr(enrollRates = planned_enrollRates,                failRates = failRates,                analysisTimes = positive_planned_time,                # Events will be determined by expected events at planned analysis times                events = NULL,                 upper = gs_spending_bound,                # Recall planned spending times are specified the same as before                 upar = upar_overall_planned_IF,                lower = gs_b,                lpar = lpar # fixed lower Z = -infinity               ) %>% filter(Bound == \"Upper\") # Planned events overall_max_events <- max(overall_planned_bounds$Events) # save max information planned under H0 overall_max_info0 <- max(overall_planned_bounds$info0) overall_planned_bounds %>%    table_gs_design(planned_enrollRates,                   spending_time = planned_IF,                   max_info0 = overall_max_info0)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"alternate-scenarios-overview","dir":"Articles","previous_headings":"Testing multiple hypotheses","what":"Alternate scenarios overview","title":"Spending Time Examples","text":"divide evaluations three subsections, one higher prevalence biomarker positive patients expected, one lower biomarker prevalence, followed section differing event rate hazard ratio assumptions. case, assume total enrollment rate 50.8 per month planned . also assume enroll targeted biomarker positive subgroup enrollment 305 achieved, regardless overall enrollment. specify interim analysis timing require 80% planned final analysis events biomarker positive population least 10 months minimum follow-; thus, biomarker population never vary events spending . spending time used overall population, compare event-based spending. choices arbitrary. think reasonable, design planner think carefully variations suit clinical trial team needs.","code":"## Setting spending alternatives  # Using information (event)-based spending time relative to overall population plan # Set total spend for overall population, O'Brien-Fleming spending function.  # For design information-spending, we set timing =  NULL and max_info to plan from above upar_overall_planned_IF <- list(sf = gsDesign::sfLDOF,  param = NULL,                                    total_spend = 0.0125, timing = planned_IF,                   # We will use planned final information for overall population from design                   # to compute information fraction relative to plan                                    max_info = overall_max_info0) # Using planned information fraction will demonstrate problems below. # Set total spend for overall population, O'Brien-Fleming spending function, and  # same spending time as biomarker subgroup upar_overall_actual_IF <- list(sf = gsDesign::sfLDOF,  param = NULL,                                   total_spend = 0.0125, timing = NULL,                   # We will use planned final information for overall population from design                                   max_info = overall_max_info0)"},{"path":[]},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"biomarker-subgroup-power","dir":"Articles","previous_headings":"Testing multiple hypotheses > Biomarker subgroup prevalence higher than planned","what":"Biomarker subgroup power","title":"Spending Time Examples","text":"suppose biomarker prevalence 60%, higher 50% prevalence design anticipated. enrollment rates positive versus negative patients expected enrollment duration now: Now can compute power biomarker positive group targeted events. Since simple proportional hazards model, thing changing original design takes slightly less time.","code":"positive_60_enrollRates <- rbind(   overall_enrollRates %>% mutate(Stratum = \"Positive\", rate = 0.6 * rate),   overall_enrollRates %>% mutate(Stratum = \"Negative\", rate = 0.4 * rate) ) positive_60_duration <- positive_planned_N / overall_enrollRates$rate / 0.6 positive_60_enrollRates$duration <- positive_60_duration positive_60_enrollRates %>% gt() %>% fmt_number(columns=\"rate\",decimals=1) positive_60_power <-  gs_power_ahr(enrollRates = positive_60_enrollRates %>% filter(Stratum == \"Positive\"),              failRates = failRates %>% filter(Stratum == \"Positive\"),              events = positive$bounds$Events[1:K],              analysisTimes = NULL,              upper = gs_spending_bound,              upar = upar_design_spend,              lower = gs_b,              lpar = lpar ) %>% filter(Bound == \"Upper\") positive_60_power %>%    table_gs_design(enrollRates = positive_60_enrollRates %>% filter(Stratum == \"Positive\"),                    max_info0 = positive_max_info0)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"overall-population-power","dir":"Articles","previous_headings":"Testing multiple hypotheses > Biomarker subgroup prevalence higher than planned","what":"Overall population power","title":"Spending Time Examples","text":"Now use spending overall population, resulting full \\(\\alpha\\)-spending end trial even though originally targeted events expected achieved. note information fraction computed based originally planned events overall population. Given larger proportion patients biomarker positive, average hazard ratio stronger originally planned power overall population still 90%. used information-based (.e., event-based) spending, reached full spending final analysis thus lower power.","code":"gs_power_ahr(enrollRates = positive_60_enrollRates,              failRates = failRates,              events = 1:2,              analysisTimes = positive_60_power$Time[1:2],              upper = gs_spending_bound,              # use planned spending in spite of lower overall information              upar = upar_overall_planned_IF,              # Still use Z = -infinity lower bound               lower = gs_b, lpar = rep(-Inf, 2)) %>%    filter(Bound == \"Upper\") %>%    table_gs_design(enrollRates = positive_60_enrollRates, spending_time = c(.8,1), max_info0 = overall_max_info0) xz <- gs_power_ahr(enrollRates = positive_60_enrollRates,              failRates = failRates,              events = 1:2,              analysisTimes = positive_60_power$Time[1:2],              upper = gs_spending_bound,              # use actual spending which uses less than complete alpha              upar = upar_overall_actual_IF,              # Still use Z = -infinity lower bound               lower = gs_b, lpar = lpar) %>%    filter(Bound == \"Upper\" )  xz %>% table_gs_design(enrollRates = positive_60_enrollRates, spending_time = NULL, max_info0 = overall_max_info0)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"biomarker-subgroup-prevalence-lower-than-planned","dir":"Articles","previous_headings":"Testing multiple hypotheses","what":"Biomarker subgroup prevalence lower than planned","title":"Spending Time Examples","text":"suppose biomarker prevalence 40%, lower 50% prevalence design anticipated. enrollment rates positive versus negative patients expected enrollment duration now :","code":"positive_40_enrollRates <- rbind(   overall_enrollRates %>% mutate(Stratum = \"Positive\", rate = 0.4 * rate),   overall_enrollRates %>% mutate(Stratum = \"Negative\", rate = 0.6 * rate) ) positive_40_enrollRates$duration <- positive_planned_N / positive_40_enrollRates$rate[1] positive_40_enrollRates %>% gt() %>% fmt_number(columns = \"rate\", decimals=1)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"biomarker-positive-subgroup-power","dir":"Articles","previous_headings":"Testing multiple hypotheses > Biomarker subgroup prevalence lower than planned","what":"Biomarker positive subgroup power","title":"Spending Time Examples","text":"Now can compute power biomarker positive group targeted events.","code":"upar_actual_IF$total_spend <- 0.0125 upar_actual_IF$max_info <- positive_max_info0 positive_40_power <-  gs_power_ahr(enrollRates = positive_40_enrollRates %>% filter(Stratum == \"Positive\"),              failRates = failRates %>% filter(Stratum == \"Positive\"),              events = (positive$bounds %>% filter(Bound == \"Upper\"))$Events,              analysisTimes = NULL,              upper = gs_spending_bound,              upar = upar_actual_IF,               lpar = rep(-Inf, 2) ) positive_40_power %>% filter(Bound == \"Upper\") %>%    table_gs_design(enrollRates = positive_40_enrollRates %>% filter(Stratum == \"Positive\"),                    max_info0 = positive_max_info0)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"overall-population-power-1","dir":"Articles","previous_headings":"Testing multiple hypotheses > Biomarker subgroup prevalence lower than planned","what":"Overall population power","title":"Spending Time Examples","text":"see adapting overall sample size spending according biomarker subgroup, retain 90% power. spite lower overall effect size, larger adapted sample size ensures power retention.","code":"gs_power_ahr(enrollRates = positive_40_enrollRates,              failRates = failRates,              events = 1:2,              analysisTimes = positive_40_power$Time[1:2],              upper = gs_spending_bound,              upar = upar_overall_planned_IF,              lpar = rep(-Inf,2) ) %>% filter(Bound == \"Upper\")  %>%    table_gs_design(enrollRates = positive_60_enrollRates,                    spending_time = c(.8, 1),                   max_info0 = overall_max_info0)"},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"summary-of-findings","dir":"Articles","previous_headings":"","what":"Summary of findings","title":"Spending Time Examples","text":"suggested two overall findings planning executing trial potentially delayed treatment effect: Require targeted event count minimum follow-completing analysis trial helps ensure powering trial appropriately better description tail behavior may essential long-term results key establishing potentially positive risk-benefit. Use fixed, small incremental \\(\\alpha\\)-spend interim proposed Fleming, Harrington, O’Brien (1984) variable number interim analyses ensure adequate follow-. Use minimum planned actual spending interim analyses. implementing Fleming, Harrington, O’Brien (1984) approach, also suggested simple approach futility may quite useful practically scenario potentially delayed onset treatment effect. basically looks evidence favorable control group effect relative experimental setting nominal p-value cutoff 1-sided 0.05 level early interim futility analyses. crossing survival curves inferior survival curves may exist, may useful way ensure continuing trial ethical; approach perhaps useful experimental treatment replacing components control treatment case add-treatment may toxic potentially detrimental effects. addition delayed effect example, considered example testing biomarker positive subgroup overall population. Using common spending time hypotheses common interim analysis strategy advocated Follmann, Proschan, Geller (1994) can helpful implement spending hypotheses adequate \\(\\alpha\\) spend final analysis also ensure full utilization \\(\\alpha\\)-spending. suggested using minimum planned actual spending interim analysis. Spending can based key hypothesis (e.g., biomarker positive population) minimum spending time among hypotheses tested. Taking advantage know correlations ensure full \\(\\alpha\\) utilitization multiple hypothesis testing also simply implemented strategy Anderson et al. (2021). summary, illustrated motivation illustration spending time approach examples commonly encountered. Approaches suggested included implementation Fleming, Harrington, O’Brien (1984) fixed incremental \\(\\alpha\\)-spend interim analysis well use minimum planned actual spending interim analyses.","code":""},{"path":"https://merck.github.io/gsdmvn/articles/SpendingTimeExamples.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Spending Time Examples","text":"Anderson, Keaven M, Zifang Guo, Jing Zhao, Linda Z Sun. 2021. “Unified Framework Weighted Parametric Group Sequential Design (Wpgsd).” arXiv Preprint arXiv:2103.10537. Fleming, Thomas R, David P Harrington, Peter C O’Brien. 1984. “Designs Group Sequential Tests.” Controlled Clinical Trials 5 (4): 348–61. Follmann, Dean , Michael Proschan, Nancy L Geller. 1994. “Monitoring Pairwise Comparisons Multi-Armed Clinical Trials.” Biometrics, 325–36. Lan, K. K. G., David L. DeMets. 1983. “Discrete Sequential Boundaries Clinical Trials.” Biometrika 70: 659–63. ———. 1989. “Group Sequential Procedures: Calendar Versus Information Time.” Statistics Medicine 8: 1191–8. https://doi.org/10.1002/sim.4780081003. Lin, Ray S, Ji Lin, Satrajit Roychoudhury, Keaven M Anderson, Tianle Hu, Bo Huang, Larry F Leon, et al. 2020. “Alternative Analysis Methods Time Event Endpoints Nonproportional Hazards: Comparative Analysis.” Statistics Biopharmaceutical Research 12 (2): 187–98. Maurer, Willi, Frank Bretz. 2013. “Multiple Testing Group Sequential Trials Using Graphical Approaches.” Statistics Biopharmaceutical Research 5: 311–20. https://doi.org/10.1080/19466315.2013.807748. Mukhopadhyay, Pralay, Wenmei Huang, Paul Metcalfe, Fredrik Öhrn, Mary Jenner, Andrew Stone. 2020. “Statistical Practical Considerations Designing Immuno-Oncology Trials.” Journal Biopharmaceutical Statistics, 1–17. Proschan, Michael ., K. K. Gordon Lan, Janet Turk Wittes. 2006. Statistical Monitoring Clinical Trials. Unified Approach. New York, NY: Springer. Roychoudhury, Satrajit, Keaven M Anderson, Jiabu Ye, Pralay Mukhopadhyay. 2021. “Robust Design Analysis Clinical Trials Non-Proportional Hazards: Straw Man Guidance Cross-Pharma Working Group.” Statistics Biopharmaceutical Research, 1–37. Tsiatis, Anastasios . 1982. “Repeated Significance Testing General Class Statistics Use Censored Survival Analysis.” Journal American Statistical Association 77: 855–61.","code":""},{"path":"https://merck.github.io/gsdmvn/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Keaven Anderson. Author, maintainer. Merck Sharp & Dohme Corp. Copyright holder.","code":""},{"path":"https://merck.github.io/gsdmvn/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Anderson K (2022). gsdmvn: Group sequential design non-constant effect. https://merck.github.io/gsdmvn/, https://github.com/Merck/gsdmvn.","code":"@Manual{,   title = {gsdmvn: Group sequential design with non-constant effect},   author = {Keaven Anderson},   year = {2022},   note = {https://merck.github.io/gsdmvn/, https://github.com/Merck/gsdmvn}, }"},{"path":"https://merck.github.io/gsdmvn/index.html","id":"gsdmvn","dir":"","previous_headings":"","what":"Group sequential design with non-constant effect","title":"Group sequential design with non-constant effect","text":"goal gsdmvn enable fixed group sequential design non-proportional hazards. Piecewise constant enrollment, failure rates dropout rates stratified population available enable highly flexible enrollment, time--event time--dropout assumptions. Substantial flexibility top gsDesign package intended selecting boundaries. work progress, substantial capabilities enabled. Comments usability features encouraged development version package. goal gsdmvn enable group sequential trial design time--event endpoints non-proportional hazards assumptions. package still maturing; package functions become stable, likely included gsDesign2 package.","code":""},{"path":"https://merck.github.io/gsdmvn/index.html","id":"branch-specification","dir":"","previous_headings":"","what":"Branch specification","title":"Group sequential design with non-constant effect","text":"development branch includes work development. table_bound branch branched development branch, targets get outputs gd_design_ahr(), gs_power_ahr(),gs_desgin_wlr(), etc., well-organized form. update_futility_bound branch branched development branch, targets develop code one can update futility bound.","code":""},{"path":"https://merck.github.io/gsdmvn/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Group sequential design with non-constant effect","text":"can install gsdmvn :","code":"remotes::install_github(\"Merck/gsdmvn\")"},{"path":"https://merck.github.io/gsdmvn/index.html","id":"specifying-enrollment-and-failure-rates","dir":"","previous_headings":"","what":"Specifying enrollment and failure rates","title":"Group sequential design with non-constant effect","text":"basic example shows solve common problem. assume 4 month delay treatment effect. Specifically, assume hazard ratio 1 4 months 0.6 thereafter. example assume exponential failure rate low exponential dropout rate. enrollRates specification indicates expected enrollment duration 12 months exponential inter-arrival times. resulting failure rate specification following table. many rows strata needed can specified approximate whatever patterns wish. Computing fixed sample size design 2.5% one-sided Type error 90% power. specify trial duration 36 months analysisTimes. Since single analysis, specify upper p-value bound 0.025 upar = qnorm(0.975). lower bound specified lpar = -Inf. input enrollment rates scaled achieve power: failure dropout rates remain unchanged input: Finally, expected analysis time Time, sample size N, events required Events bound Z design$bounds. Note AHR average hazard ratio used calculate targeted event counts. natural parameter (log(AHR)) theta corresponding statistical information alternate hypothesis info null hypothesis info0.","code":"library(gsdmvn) library(gsDesign) library(gsDesign2) library(dplyr) library(knitr)  ## basic example code  ## Constant enrollment over 12 months ## rate will be adjusted later by gsDesignNPH to get sample size enrollRates <- tibble::tibble(Stratum = \"All\", duration = 12, rate = 1)  ## 12 month median exponential failure rate in control ## 4 month delay in effect with HR=0.6 after ## Low exponential dropout rate medianSurv <- 12 failRates <- tibble::tibble(   Stratum = \"All\",   duration = c(4, Inf),   failRate = log(2) / medianSurv,   hr = c(1, .6),   dropoutRate = .001 ) failRates %>% kable() design <-   gs_design_ahr(enrollRates, failRates, upar = qnorm(.975), lpar = -Inf, IF = 1, analysisTimes = 36) design$enrollRates %>% kable() design$failRates %>% kable() design$bounds %>% kable()"},{"path":"https://merck.github.io/gsdmvn/reference/ahr_blinded.html","id":null,"dir":"Reference","previous_headings":"","what":"Blinded estimation of average hazard ratio — ahr_blinded","title":"Blinded estimation of average hazard ratio — ahr_blinded","text":"Based blinded data assumed hazard ratios different intervals, compute blinded estimate average hazard ratio (AHR) corresponding estimate statistical information. function intended use computing futility bounds based spending assuming input hazard ratio (hr) values intervals specified .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/ahr_blinded.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Blinded estimation of average hazard ratio — ahr_blinded","text":"","code":"ahr_blinded(   Srv = survival::Surv(time = simtrial::Ex1delayedEffect$month, event =     simtrial::Ex1delayedEffect$evntd),   intervals = array(3, 3),   hr = c(1, 0.6),   ratio = 1 )"},{"path":"https://merck.github.io/gsdmvn/reference/ahr_blinded.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Blinded estimation of average hazard ratio — ahr_blinded","text":"Srv input survival object (see Surv); note 0=censored, 1=event Surv intervals Vector containing positive values indicating interval lengths exponential rates assumed. Note final infinite interval added events occur final interval specified. hr vector hazard ratios assumed interval ratio ratio experimental control randomization.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/ahr_blinded.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Blinded estimation of average hazard ratio — ahr_blinded","text":"tibble one row containing AHR blinded average hazard ratio based assumed period-specific hazard ratios input failRatesand observed events corresponding intervals Events total observed number events, info statistical information based Schoenfeld approximation, info0 (information related null hypothesis) value totalDuration input; simple=FALSE, Stratum t (beginning constant HR period) also returned HR returned instead AHR","code":""},{"path":"https://merck.github.io/gsdmvn/reference/ahr_blinded.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Blinded estimation of average hazard ratio — ahr_blinded","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/ahr_blinded.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Blinded estimation of average hazard ratio — ahr_blinded","text":"","code":"if (FALSE) { library(simtrial) library(survival) ahr_blinded(Srv = Surv(time = simtrial::Ex2delayedEffect$month,                        event = simtrial::Ex2delayedEffect$evntd),             intervals = c(4,100),             hr = c(1, .55),             ratio = 1) }"},{"path":"https://merck.github.io/gsdmvn/reference/gridpts.html","id":null,"dir":"Reference","previous_headings":"","what":"Grid points for group sequential design numerical integration — gridpts","title":"Grid points for group sequential design numerical integration — gridpts","text":"Points weights Simpson's rule numerical integration p 349 - 350 Jennison Turnbull book. used arbitrary integration, canonical form Jennison Turnbull. mu computed elsewhere drift parameter times sqrt information. Since lower-level routine, checking input done; calling routines ensure input correct. Lower limit integration can -Inf upper limit integration can Inf","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gridpts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Grid points for group sequential design numerical integration — gridpts","text":"","code":"gridpts(r = 18, mu = 0, a = -Inf, b = Inf)"},{"path":"https://merck.github.io/gsdmvn/reference/gridpts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Grid points for group sequential design numerical integration — gridpts","text":"r Integer, least 2; default 18 recommended Jennison Turnbull mu Mean normal distribution (scalar) consideration lower limit integration (scalar) b upper limit integration (scalar > )","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gridpts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Grid points for group sequential design numerical integration — gridpts","text":"tibble grid points z numerical integration weights w","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gridpts.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Grid points for group sequential design numerical integration — gridpts","text":"Jennison Turnbull (p 350) claim accuracy 10E-6 r=16. numerical integration grid spreads tail enable accurate tail probability calcuations.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gridpts.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Grid points for group sequential design numerical integration — gridpts","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gridpts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Grid points for group sequential design numerical integration — gridpts","text":"","code":"library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union  # approximate variance of standard normal (i.e., 1) gridpts() %>% summarise(var = sum(z^2 * w * dnorm(z))) #> # A tibble: 1 × 1 #>     var #>   <dbl> #> 1  1.00  # approximate probability above .95 quantile (i.e., .05) gridpts(a = qnorm(.95), b = Inf) %>% summarise(p05 = sum(w * dnorm(z))) #> # A tibble: 1 × 1 #>      p05 #>    <dbl> #> 1 0.0500"},{"path":"https://merck.github.io/gsdmvn/reference/gs_b.html","id":null,"dir":"Reference","previous_headings":"","what":"gs_b: Default boundary generation — gs_b","title":"gs_b: Default boundary generation — gs_b","text":"gs_b() simplest version function used upper lower arguments gs_prob(), gs_power_nph gs_design_nph(); simply returns vector input input vector Z , k specified par[k]j returned. Note bounds need change changing information analyses, gs_b() used. instance, spending function bounds use","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_b.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gs_b: Default boundary generation — gs_b","text":"","code":"gs_b(par = NULL, info = NULL, k = NULL, ...)"},{"path":"https://merck.github.io/gsdmvn/reference/gs_b.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gs_b: Default boundary generation — gs_b","text":"par gs_b(), just Z-values boundaries; can include infinite values info Information analyses; used function; present standard parameter boundary computation routines k NULL (default), return par, else return par[k] ... arguments passed methods","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_b.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gs_b: Default boundary generation — gs_b","text":"returns vector input par k NULL, otherwise, par[k]","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_b.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"gs_b: Default boundary generation — gs_b","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_b.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gs_b: Default boundary generation — gs_b","text":"","code":"# Simple: enter a vector of length 3 for bound gs_b(par = 4:2) #> [1] 4 3 2  # 2nd element of par gs_b(4:2, k = 2) #> [1] 3  # Generate an efficacy bound using a spending function # Use Lan-DeMets spending approximation of O'Brien-Fleming bound # as 50%, 75% and 100% of final spending # Information fraction IF <- c(.5, .75, 1) gs_b(par = gsDesign::gsDesign(alpha = .025, k= length(IF),       test.type = 1, sfu = gsDesign::sfLDOF,       timing = IF)$upper$bound) #> [1] 2.962588 2.359018 2.014084"},{"path":"https://merck.github.io/gsdmvn/reference/gs_bound.html","id":null,"dir":"Reference","previous_headings":"","what":"Lower and Upper Bound of Group Sequential Design — gs_bound","title":"Lower and Upper Bound of Group Sequential Design — gs_bound","text":"Lower Upper Bound Group Sequential Design","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_bound.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Lower and Upper Bound of Group Sequential Design — gs_bound","text":"","code":"gs_bound(   alpha,   beta,   theta,   corr,   analysis = 1:length(alpha),   theta0 = rep(0, length(analysis)),   binding_lower_bound = FALSE,   algorithm = GenzBretz(maxpts = 1e+05, abseps = 1e-05),   alpha_bound = FALSE,   beta_bound = FALSE,   ... )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_bound.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Lower and Upper Bound of Group Sequential Design — gs_bound","text":"alpha numeric vector cumulative allocated alpha interim analysis beta numeric vector cumulative allocated beta interim analysis theta numeric vector effect size alternative. corr matrix correlation matrix analysis numeric vector interim analysis indicator. Default 1:length(alpha). theta0 numeric vector effect size null hypothesis. Default 0. binding_lower_bound logical value indicate binding lower bound. algorithm object class GenzBretz,                     Miwa TVPACK                     specifying algorithm used well                     associated hyper parameters. alpha_bound logical value indicate alpha Type error upper bound. Default FALSE. beta_bound logical value indicate beta Type II error lower bound. Default FALSE. ... additional parameters transfer mvtnorm::pmvnorm","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_bound.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Lower and Upper Bound of Group Sequential Design — gs_bound","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_bound.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Lower and Upper Bound of Group Sequential Design — gs_bound","text":"","code":"library(gsDesign) #> Loading required package: ggplot2  x <- gsDesign::gsSurv( k = 3 , test.type = 4 , alpha = 0.025 ,                        beta = 0.2 , astar = 0 , timing = c( 1 ) ,                        sfu = sfLDOF , sfupar = c( 0 ) , sfl = sfLDOF ,                        sflpar = c( 0 ) , lambdaC = c( 0.1 ) ,                        hr = 0.6 , hr0 = 1 , eta = 0.01 ,                        gamma = c( 10 ) ,                        R = c( 12 ) , S = NULL ,                        T = 36 , minfup = 24 , ratio = 1 )  cbind(x$lower$bound, x$upper$bound) #>            [,1]     [,2] #> [1,] -0.2361874 3.710303 #> [2,]  1.1703638 2.511407 #> [3,]  1.9929702 1.992970  gsdmvn:::gs_bound(alpha = sfLDOF(0.025, 1:3/3)$spend,          beta = sfLDOF(0.2, 1:3/3)$spend,          analysis = 1:3,          theta = x$theta[2] * sqrt(x$n.I),          corr = outer(1:3, 1:3, function(x,y) pmin(x,y) / pmax(x,y))) #>      upper      lower #> 1 3.710295 -0.2361879 #> 2 2.513755  1.1335603 #> 3 2.023256  2.0232556"},{"path":"https://merck.github.io/gsdmvn/reference/gs_create_arm.html","id":null,"dir":"Reference","previous_headings":"","what":"Create ","title":"Create ","text":"Create \"npsurvSS\" arm object","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_create_arm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create ","text":"","code":"gs_create_arm(enrollRates, failRates, ratio, total_time = 1e+06)"},{"path":"https://merck.github.io/gsdmvn/reference/gs_create_arm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create ","text":"enrollRates enrollment rates failRates failure dropout rates ratio Experimental:Control randomization ratio total_time total analysis time","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_create_arm.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Create ","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_ahr.html","id":null,"dir":"Reference","previous_headings":"","what":"Group sequential design using average hazard ratio under non-proportional hazards — gs_design_ahr","title":"Group sequential design using average hazard ratio under non-proportional hazards — gs_design_ahr","text":"Group sequential design using average hazard ratio non-proportional hazards","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_ahr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group sequential design using average hazard ratio under non-proportional hazards — gs_design_ahr","text":"","code":"gs_design_ahr(   enrollRates = tibble::tibble(Stratum = \"All\", duration = c(2, 2, 10), rate = c(3, 6,     9)),   failRates = tibble::tibble(Stratum = \"All\", duration = c(3, 100), failRate =     log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),   ratio = 1,   alpha = 0.025,   beta = 0.1,   IF = NULL,   analysisTimes = 36,   binding = FALSE,   upper = gs_b,   upar = gsDesign(k = 3, test.type = 1, n.I = c(0.25, 0.75, 1), sfu = sfLDOF, sfupar =     NULL)$upper$bound,   lower = gs_b,   lpar = c(qnorm(0.1), -Inf, -Inf),   h1_spending = TRUE,   test_upper = TRUE,   test_lower = TRUE,   r = 18,   tol = 1e-06 )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_ahr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group sequential design using average hazard ratio under non-proportional hazards — gs_design_ahr","text":"enrollRates enrollment rates failRates failure dropout rates ratio Experimental:Control randomization ratio (yet implemented) alpha One-sided Type error beta Type II error Targeted information fraction analysis analysisTimes Minimum time analysis binding indicator whether futility bound binding; default FALSE recommended upper Function compute upper bound upar Parameter passed upper() lower Function compute lower bound lpar Parameter passed lower() h1_spending Indicator lower bound set spending alternate hypothesis (input failRates) spending used lower bound test_upper indicator analyses include upper (efficacy) bound; single value TRUE (default) indicates analyses; otherwise, logical vector length info indicate analyses efficacy bound test_lower indicator analyses include lower bound; single value TRUE (default) indicates analyses; single value FALSE indicated lower bound; otherwise, logical vector length info indicate analyses lower bound r Integer, least 2; default 18 recommended Jennison Turnbull tol Tolerance parameter boundary convergence (Z-scale)","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_ahr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Group sequential design using average hazard ratio under non-proportional hazards — gs_design_ahr","text":"tibble columns Analysis, Bound, Z, Probability, theta, Time, AHR, Events","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_ahr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Group sequential design using average hazard ratio under non-proportional hazards — gs_design_ahr","text":"Need added","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_ahr.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Group sequential design using average hazard ratio under non-proportional hazards — gs_design_ahr","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_ahr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group sequential design using average hazard ratio under non-proportional hazards — gs_design_ahr","text":"","code":"library(gsDesign) library(gsDesign2) library(dplyr) # call with defaults gs_design_ahr() #> $enrollRates #> # A tibble: 3 × 3 #>   Stratum duration  rate #>   <chr>      <dbl> <dbl> #> 1 All            2  13.2 #> 2 All            2  26.4 #> 3 All           10  39.7 #>  #> $failRates #> # A tibble: 2 × 5 #>   Stratum duration failRate    hr dropoutRate #>   <chr>      <dbl>    <dbl> <dbl>       <dbl> #> 1 All            3   0.0770   0.9       0.001 #> 2 All          100   0.0385   0.6       0.001 #>  #> $bounds #> # A tibble: 1 × 11 #>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0 #>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper    36  476.   292.  1.96         0.9 0.683 0.381  71.7  73.0 #>   # Single analysis gs_design_ahr(analysisTimes = 40) #> $enrollRates #> # A tibble: 3 × 3 #>   Stratum duration  rate #>   <chr>      <dbl> <dbl> #> 1 All            2  11.9 #> 2 All            2  23.8 #> 3 All           10  35.6 #>  #> $failRates #> # A tibble: 2 × 5 #>   Stratum duration failRate    hr dropoutRate #>   <chr>      <dbl>    <dbl> <dbl>       <dbl> #> 1 All            3   0.0770   0.9       0.001 #> 2 All          100   0.0385   0.6       0.001 #>  #> $bounds #> # A tibble: 1 × 11 #>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0 #>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper    40  428.   280.  1.96         0.9 0.678 0.389  68.8  69.9 #>   # Multiple analysisTimes gs_design_ahr(analysisTimes = c(12,24,36)) #> $enrollRates #> # A tibble: 3 × 3 #>   Stratum duration  rate #>   <chr>      <dbl> <dbl> #> 1 All            2  14.0 #> 2 All            2  27.9 #> 3 All           10  41.9 #>  #> $failRates #> # A tibble: 2 × 5 #>   Stratum duration failRate    hr dropoutRate #>   <chr>      <dbl>    <dbl> <dbl>       <dbl> #> 1 All            3   0.0770   0.9       0.001 #> 2 All          100   0.0385   0.6       0.001 #>  #> $bounds #> # A tibble: 6 × 11 #>   Analysis Bound  Time     N Events       Z Probability   AHR theta  info info0 #>      <int> <chr> <dbl> <dbl>  <dbl>   <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper    12  419.   95.0    4.33    0.000454 0.811 0.210  23.4  23.8 #> 2        2 Upper    24  503.  229.     2.34    0.566    0.715 0.335  55.9  57.1 #> 3        3 Upper    36  503.  309.     2.01    0.900    0.683 0.381  75.8  77.1 #> 4        1 Lower    12  419.   95.0   -1.28    0.0108   0.811 0.210  23.4  23.8 #> 5        2 Lower    24  503.  229.  -Inf       0.0108   0.715 0.335  55.9  57.1 #> 6        3 Lower    36  503.  309.  -Inf       0.0108   0.683 0.381  75.8  77.1 #>   # Specified information fraction gs_design_ahr(IF = c(.25,.75,1), analysisTimes = 36) #> $enrollRates #> # A tibble: 3 × 3 #>   Stratum duration  rate #>   <chr>      <dbl> <dbl> #> 1 All            2  14.2 #> 2 All            2  28.4 #> 3 All           10  42.5 #>  #> $failRates #> # A tibble: 2 × 5 #>   Stratum duration failRate    hr dropoutRate #>   <chr>      <dbl>    <dbl> <dbl>       <dbl> #> 1 All            3   0.0770   0.9       0.001 #> 2 All          100   0.0385   0.6       0.001 #>  #> $bounds #> # A tibble: 6 × 11 #>   Analysis Bound  Time     N Events       Z Probability   AHR theta  info info0 #>      <int> <chr> <dbl> <dbl>  <dbl>   <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  10.7  371.   78.3    4.33    0.000254 0.823 0.195  19.3  19.6 #> 2        2 Upper  24.4  511.  235.     2.34    0.585    0.714 0.337  57.4  58.7 #> 3        3 Upper  36    511.  313.     2.01    0.900    0.683 0.381  76.9  78.3 #> 4        1 Lower  10.7  371.   78.3   -1.28    0.0163   0.823 0.195  19.3  19.6 #> 5        2 Lower  24.4  511.  235.  -Inf       0.0163   0.714 0.337  57.4  58.7 #> 6        3 Lower  36    511.  313.  -Inf       0.0163   0.683 0.381  76.9  78.3 #>   # multiple analysis times & IF # driven by times gs_design_ahr(IF = c(.25,.75,1), analysisTimes = c(12,25,36)) #> $enrollRates #> # A tibble: 3 × 3 #>   Stratum duration  rate #>   <chr>      <dbl> <dbl> #> 1 All            2  14.0 #> 2 All            2  27.9 #> 3 All           10  41.9 #>  #> $failRates #> # A tibble: 2 × 5 #>   Stratum duration failRate    hr dropoutRate #>   <chr>      <dbl>    <dbl> <dbl>       <dbl> #> 1 All            3   0.0770   0.9       0.001 #> 2 All          100   0.0385   0.6       0.001 #>  #> $bounds #> # A tibble: 6 × 11 #>   Analysis Bound  Time     N Events       Z Probability   AHR theta  info info0 #>      <int> <chr> <dbl> <dbl>  <dbl>   <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper    12  419.   95.0    4.33    0.000454 0.811 0.210  23.4  23.8 #> 2        2 Upper    25  503.  236.     2.34    0.600    0.711 0.341  57.8  59.1 #> 3        3 Upper    36  503.  308.     2.01    0.900    0.683 0.381  75.8  77.1 #> 4        1 Lower    12  419.   95.0   -1.28    0.0108   0.811 0.210  23.4  23.8 #> 5        2 Lower    25  503.  236.  -Inf       0.0108   0.711 0.341  57.8  59.1 #> 6        3 Lower    36  503.  308.  -Inf       0.0108   0.683 0.381  75.8  77.1 #>  # driven by IF gs_design_ahr(IF = c(1/3, .8, 1), analysisTimes = c(12,25,36)) #> $enrollRates #> # A tibble: 3 × 3 #>   Stratum duration  rate #>   <chr>      <dbl> <dbl> #> 1 All            2  13.9 #> 2 All            2  27.8 #> 3 All           10  41.7 #>  #> $failRates #> # A tibble: 2 × 5 #>   Stratum duration failRate    hr dropoutRate #>   <chr>      <dbl>    <dbl> <dbl>       <dbl> #> 1 All            3   0.0770   0.9       0.001 #> 2 All          100   0.0385   0.6       0.001 #>  #> $bounds #> # A tibble: 6 × 11 #>   Analysis Bound  Time     N Events       Z Probability   AHR theta  info info0 #>      <int> <chr> <dbl> <dbl>  <dbl>   <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  12.5  439.   102.    4.33    0.000576 0.806 0.216  25.2  25.6 #> 2        2 Upper  26.4  501.   246.    2.34    0.640    0.706 0.348  60.1  61.4 #> 3        3 Upper  36    501.   307.    2.01    0.900    0.683 0.381  75.4  76.8 #> 4        1 Lower  12.5  439.   102.   -1.28    0.00904  0.806 0.216  25.2  25.6 #> 5        2 Lower  26.4  501.   246. -Inf       0.00904  0.706 0.348  60.1  61.4 #> 6        3 Lower  36    501.   307. -Inf       0.00904  0.683 0.381  75.4  76.8 #>   # 2-sided symmetric design with O'Brien-Fleming spending gs_design_ahr(analysisTimes = c(12, 24, 36),               binding = TRUE,               upper = gs_spending_bound,               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025,                            param = NULL, timing = NULL),               lower = gs_spending_bound,               lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025,                            param = NULL, timing = NULL),               h1_spending = FALSE) #> $enrollRates #> # A tibble: 3 × 3 #>   Stratum duration  rate #>   <chr>      <dbl> <dbl> #> 1 All            2  13.7 #> 2 All            2  27.5 #> 3 All           10  41.2 #>  #> $failRates #> # A tibble: 2 × 5 #>   Stratum duration failRate    hr dropoutRate #>   <chr>      <dbl>    <dbl> <dbl>       <dbl> #> 1 All            3   0.0770   0.9       0.001 #> 2 All          100   0.0385   0.6       0.001 #>  #> $bounds #> # A tibble: 6 × 11 #>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0 #>      <int> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper    12  412.   93.4  3.87 0.00208     0.811 0.210  23.0  23.4 #> 2        2 Upper    24  494.  225.   2.36 0.551       0.715 0.335  54.9  56.2 #> 3        3 Upper    36  494.  303.   2.01 0.900       0.683 0.381  74.5  75.8 #> 4        1 Lower    12  412.   93.4 -3.87 0.000000532 0.811 0.210  23.0  23.4 #> 5        2 Lower    24  494.  225.  -2.36 0.00000116  0.715 0.335  54.9  56.2 #> 6        3 Lower    36  494.  303.  -2.01 0.00000120  0.683 0.381  74.5  75.8 #>   # 2-sided asymmetric design with O'Brien-Fleming upper spending # Pocock lower spending under H1 (NPH) gs_design_ahr(analysisTimes = c(12, 24, 36),               binding = TRUE,               upper = gs_spending_bound,               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025,                            param = NULL, timing = NULL),               lower = gs_spending_bound,               lpar = list(sf = gsDesign::sfLDPocock, total_spend = 0.1,                            param = NULL, timing = NULL),               h1_spending = TRUE) #> $enrollRates #> # A tibble: 3 × 3 #>   Stratum duration  rate #>   <chr>      <dbl> <dbl> #> 1 All            2  16.5 #> 2 All            2  33.0 #> 3 All           10  49.5 #>  #> $failRates #> # A tibble: 2 × 5 #>   Stratum duration failRate    hr dropoutRate #>   <chr>      <dbl>    <dbl> <dbl>       <dbl> #> 1 All            3   0.0770   0.9       0.001 #> 2 All          100   0.0385   0.6       0.001 #>  #> $bounds #> # A tibble: 6 × 11 #>   Analysis Bound  Time     N Events      Z Probability   AHR theta  info info0 #>      <int> <chr> <dbl> <dbl>  <dbl>  <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper    12  495.   112.  3.87      0.00281 0.811 0.210  27.7  28.1 #> 2        2 Upper    24  594.   270.  2.36      0.639   0.715 0.335  66.0  67.5 #> 3        3 Upper    36  594.   364.  1.98      0.900   0.683 0.381  89.5  91.1 #> 4        1 Lower    12  495.   112. -0.618     0.0426  0.811 0.210  27.7  28.1 #> 5        2 Lower    24  594.   270.  1.13      0.0819  0.715 0.335  66.0  67.5 #> 6        3 Lower    36  594.   364.  1.98      0.100   0.683 0.381  89.5  91.1 #>"},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_combo.html","id":null,"dir":"Reference","previous_headings":"","what":"Group sequential design using MaxCombo test under non-proportional hazards — gs_design_combo","title":"Group sequential design using MaxCombo test under non-proportional hazards — gs_design_combo","text":"Group sequential design using MaxCombo test non-proportional hazards","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_combo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group sequential design using MaxCombo test under non-proportional hazards — gs_design_combo","text":"","code":"gs_design_combo(   enrollRates,   failRates,   fh_test,   ratio = 1,   alpha = 0.025,   beta = 0.2,   binding = FALSE,   upper = gs_b,   upar = c(3, 2, 1),   lower = gs_b,   lpar = c(-1, 0, 1),   algorithm = GenzBretz(maxpts = 1e+05, abseps = 1e-05),   n_upper_bound = 1000,   ... )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_combo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group sequential design using MaxCombo test under non-proportional hazards — gs_design_combo","text":"enrollRates enrollment rates failRates failure dropout rates fh_test data frame summarize test analysis. Refer examples data structure. ratio Experimental:Control randomization ratio (yet implemented) alpha One-sided Type error beta Type II error binding indicator whether futility bound binding; default FALSE recommended upper Function compute upper bound upar Parameter passed upper() lower Function compute lower bound lpar Parameter passed lower() algorithm object class GenzBretz,                     Miwa TVPACK                     specifying algorithm used well                     associated hyper parameters. n_upper_bound numeric value upper limit sample size ... additional parameters transfer mvtnorm::pmvnorm","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_combo.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group sequential design using MaxCombo test under non-proportional hazards — gs_design_combo","text":"","code":"if (FALSE) {  # The example is slow to run  library(dplyr) library(mvtnorm) library(gsDesign)  enrollRates <- tibble::tibble(Stratum = \"All\", duration = 12, rate = 500/12)  failRates <- tibble::tibble(Stratum = \"All\",                             duration = c(4, 100),                             failRate = log(2) / 15,  # median survival 15 month                             hr = c(1, .6),                             dropoutRate = 0.001)  fh_test <- rbind( data.frame(rho = 0, gamma = 0, tau = -1,                              test = 1,                              Analysis = 1:3,                              analysisTimes = c(12, 24, 36)),                   data.frame(rho = c(0, 0.5), gamma = 0.5, tau = -1,                              test = 2:3,                              Analysis = 3, analysisTimes = 36) )  x <- gsDesign::gsSurv( k = 3 , test.type = 4 , alpha = 0.025 ,                        beta = 0.2 , astar = 0 , timing = c( 1 ) ,                        sfu = sfLDOF , sfupar = c( 0 ) , sfl = sfLDOF ,                        sflpar = c( 0 ) , lambdaC = c( 0.1 ) ,                        hr = 0.6 , hr0 = 1 , eta = 0.01 ,                        gamma = c( 10 ) ,                        R = c( 12 ) , S = NULL ,                        T = 36 , minfup = 24 , ratio = 1 )  # User defined boundary gs_design_combo(enrollRates,                 failRates,                 fh_test,                 alpha = 0.025,                 beta = 0.2,                 ratio = 1,                 binding = FALSE,                 # test.type = 4 non-binding futility bound                 upar = x$upper$bound,                 lpar = x$lower$bound)  # Boundary derived by spending function gs_design_combo(enrollRates,                 failRates,                 fh_test,                 alpha = 0.025,                 beta = 0.2,                 ratio = 1,                 binding = FALSE,                 # test.type = 4 non-binding futility bound                 upper = gs_spending_combo,                 upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),   # alpha spending                 lower = gs_spending_combo,                 lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.2),     # beta spending ) }"},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_npe.html","id":null,"dir":"Reference","previous_headings":"","what":"Group sequential design computation with non-constant effect and information — gs_design_npe","title":"Group sequential design computation with non-constant effect and information — gs_design_npe","text":"gs_design_npe() derives group sequential design size, bounds boundary crossing probabilities based proportionate information effect size analyses. allows non-constant treatment effect time, also can applied usual homogeneous effect size designs. requires treatment effect proportionate statistical information analysis well method deriving bounds, spending. routine enables two things available gsDesign package: 1) non-constant effect, 2) flexibility boundary selection. many applications, non-proportional-hazards design function gs_design_nph() used; calls function. Initial bound types supported 1) spending bounds, 2) fixed bounds, 3) Haybittle-Peto-like bounds. requirement boundary update method can bound without knowledge future bounds. example, bounds based conditional power require knowledge future bounds supported routine; limited conditional power method demonstrated. Boundary family designs Wang-Tsiatis designs including original (non-spending-function-based) O'Brien-Fleming Pocock designs supported gs_power_npe().","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_npe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group sequential design computation with non-constant effect and information — gs_design_npe","text":"","code":"gs_design_npe(   theta = 0.1,   theta1 = NULL,   info = 1,   info0 = NULL,   info1 = NULL,   alpha = 0.025,   beta = 0.1,   binding = FALSE,   upper = gs_b,   lower = gs_b,   upar = qnorm(0.975),   lpar = -Inf,   test_upper = TRUE,   test_lower = TRUE,   r = 18,   tol = 1e-06 )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_npe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group sequential design computation with non-constant effect and information — gs_design_npe","text":"theta natural parameter group sequential design representing expected incremental drift analyses; used power calculation theta1 natural parameter used lower bound spending; NULL, set theta yields usual beta-spending. set 0, spending 2-sided null hypothesis. info proportionate statistical information analyses input theta info0 proportionate statistical information null hypothesis, different alternative; impacts null hypothesis bound calculation info1 proportionate statistical information alternate hypothesis; impacts null hypothesis bound calculation alpha One-sided Type error beta Type II error binding indicator whether futility bound binding; default FALSE recommended upper function compute upper bound lower function compare lower bound upar parameter pass function provided upper lpar Parameter passed function provided lower test_upper indicator analyses include upper (efficacy) bound; single value TRUE (default) indicates analyses; otherwise, logical vector length info indicate analyses efficacy bound test_lower indicator analyses include lower bound; single value TRUE (default) indicates analyses; single value FALSE indicated lower bound; otherwise, logical vector length info indicate analyses lower bound r Integer, least 2; default 18 recommended Jennison Turnbull tol Tolerance parameter boundary convergence (Z-scale)","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_npe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Group sequential design computation with non-constant effect and information — gs_design_npe","text":"tibble columns Analysis, Bound, Z, Probability,  theta, info, info0","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_npe.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Group sequential design computation with non-constant effect and information — gs_design_npe","text":"inputs info info0 vectors length increasing positive numbers. design returned change constant scale factor ensure design power 1 - beta. bound specifications upper, lower, upar, lpar used ensure Type error boundary properties specified.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_npe.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Group sequential design computation with non-constant effect and information — gs_design_npe","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_npe.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Group sequential design computation with non-constant effect and information — gs_design_npe","text":"Keaven Anderson keaven_anderson@merck.com","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_npe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group sequential design computation with non-constant effect and information — gs_design_npe","text":"","code":"library(gsDesign) library(dplyr) # Single analysis # Lachin book p 71 difference of proportions example pc <- .28 # Control response rate pe <- .40 # Experimental response rate p0 <- (pc + pe) / 2 # Ave response rate under H0 # Information per increment of 1 in sample size info0 <- 1 / (p0 * (1 - p0) * 4) info1 <- 1 / (pc * (1 - pc) * 2 + pe * (1 - pe) * 2) # Result should round up to next even number = 652 # Divide information needed under H1 by information per patient added gs_design_npe(theta = pe - pc, info = info1, info0 = info0)$info[1] / info1 #> [1] 650.7984  # Fixed bound design <- gs_design_npe(theta = c(.1, .2, .3), info = (1:3) * 80,                info0 = (1:3) * 80, info1 = (1:3) * 80,               upper = gs_b, upar = gsDesign::gsDesign(k=3,sfu=gsDesign::sfLDOF)$upper$bound,               lower = gs_b, lpar = c(-1, 0, 0)) design #> # A tibble: 6 × 9 #>   Analysis Bound     Z Probability theta theta1  info info0 info1 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  3.71     0.00120   0.1    0.1  45.6  45.6  45.6 #> 2        2 Upper  2.51     0.273     0.2    0.2  91.1  91.1  91.1 #> 3        3 Upper  1.99     0.900     0.3    0.3 137.  137.  137.  #> 4        1 Lower -1        0.0470    0.1    0.1  45.6  45.6  45.6 #> 5        2 Lower  0        0.0619    0.2    0.2  91.1  91.1  91.1 #> 6        3 Lower  0        0.0619    0.3    0.3 137.  137.  137.   # Same fixed bounds, null hypothesis gs_power_npe(theta = rep(0,3), info = design$info0[1:3],              upar = design$Z[1:3], lpar = design$Z[4:6]) #> # A tibble: 6 × 9 #>   Analysis Bound     Z Probability theta theta1  info info0 info1 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  3.71    0.000104     0      0  45.6  45.6  45.6 #> 2        2 Upper  2.51    0.00605      0      0  91.1  91.1  91.1 #> 3        3 Upper  1.99    0.0249       0      0 137.  137.  137.  #> 4        1 Lower -1       0.159        0      0  45.6  45.6  45.6 #> 5        2 Lower  0       0.513        0      0  91.1  91.1  91.1 #> 6        3 Lower  0       0.606        0      0 137.  137.  137.   # Same upper bound; this represents non-binding Type I error and will total 0.025 gs_power_npe(theta = rep(0,3), info = design$info0[1:3],               upar = design$Z[1:3], lpar = rep(-Inf,3)) %>%    filter(Bound==\"Upper\") #> # A tibble: 3 × 9 #>   Analysis Bound     Z Probability theta theta1  info info0 info1 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  3.71    0.000104     0      0  45.6  45.6  45.6 #> 2        2 Upper  2.51    0.00605      0      0  91.1  91.1  91.1 #> 3        3 Upper  1.99    0.0250       0      0 137.  137.  137.   # Spending bound examples  # Design with futility only at analysis 1; efficacy only at analyses 2, 3 # Spending bound for efficacy; fixed bound for futility # NOTE: test_upper and test_lower DO NOT WORK with gs_b; must explicitly make bounds infinite # test_upper and test_lower DO WORK with gs_spending_bound design <- gs_design_npe(theta = c(.1, .2, .3), info = (1:3) * 40, info0 = (1:3) * 40,               upper = gs_spending_bound,               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025,                            param = NULL, timing = NULL),               lower = gs_b, lpar = c(-1, -Inf, -Inf),               test_upper = c(FALSE, TRUE, TRUE)) design #> # A tibble: 6 × 9 #>   Analysis Bound       Z Probability theta theta1  info info0 info1 #>      <int> <chr>   <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  Inf         0        0.1    0.1  44.6  44.6  44.6 #> 2        2 Upper    2.51      0.267    0.2    0.2  89.1  89.1  89.1 #> 3        3 Upper    1.99      0.900    0.3    0.3 134.  134.  134.  #> 4        1 Lower   -1         0.0477   0.1    0.1  44.6  44.6  44.6 #> 5        2 Lower -Inf         0.0477   0.2    0.2  89.1  89.1  89.1 #> 6        3 Lower -Inf         0.0477   0.3    0.3 134.  134.  134.   # Spending function bounds # 2-sided asymmetric bounds # Lower spending based on non-zero effect gs_design_npe(theta = c(.1, .2, .3), info = (1:3) * 40,              upper = gs_spending_bound,              upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025,                           param = NULL, timing = NULL),              lower = gs_spending_bound,              lpar = list(sf = gsDesign::sfHSD, total_spend = 0.1,                           param = -1, timing = NULL)) #> # A tibble: 6 × 9 #>   Analysis Bound      Z Probability theta theta1  info info0 info1 #>      <int> <chr>  <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  3.71      0.00114   0.1    0.1  43.3  43.3  43.3 #> 2        2 Upper  2.51      0.258     0.2    0.2  86.7  86.7  86.7 #> 3        3 Upper  1.99      0.900     0.3    0.3 130.  130.  130.  #> 4        1 Lower -1.34      0.0230    0.1    0.1  43.3  43.3  43.3 #> 5        2 Lower  0.146     0.0552    0.2    0.2  86.7  86.7  86.7 #> 6        3 Lower  1.99      0.100     0.3    0.3 130.  130.  130.   # Two-sided symmetric spend, O'Brien-Fleming spending # Typically, 2-sided bounds are binding xx <- gs_design_npe(theta = c(.1, .2, .3), theta1 = rep(0, 3), info = (1:3) * 40,                     binding = TRUE,                     upper = gs_spending_bound,                     upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025,                                  param = NULL, timing = NULL),                     lower = gs_spending_bound,                     lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025,                                  param = NULL, timing = NULL)) xx #> # A tibble: 6 × 9 #>   Analysis Bound     Z Probability theta theta1  info info0 info1 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  3.71  0.00103      0.1      0  39.7  39.7  39.7 #> 2        2 Upper  2.51  0.233        0.2      0  79.4  79.4  79.4 #> 3        3 Upper  1.99  0.900        0.3      0 119.  119.  119.  #> 4        1 Lower -3.71  0.00000711   0.1      0  39.7  39.7  39.7 #> 5        2 Lower -2.51  0.0000154    0.2      0  79.4  79.4  79.4 #> 6        3 Lower -1.99  0.0000154    0.3      0 119.  119.  119.   # Re-use these bounds under alternate hypothesis # Always use binding = TRUE for power calculations upar <- (xx %>% filter(Bound==\"Upper\"))$Z gs_power_npe(theta = c(.1, .2, .3), info = (1:3) * 40,              binding = TRUE,              upar = upar,              lpar = -upar) #> # A tibble: 6 × 9 #>   Analysis Bound     Z Probability theta theta1  info info0 info1 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  3.71  0.00104      0.1    0.1    40    40    40 #> 2        2 Upper  2.51  0.235        0.2    0.2    80    80    80 #> 3        3 Upper  1.99  0.902        0.3    0.3   120   120   120 #> 4        1 Lower -3.71  0.00000704   0.1    0.1    40    40    40 #> 5        2 Lower -2.51  0.0000151    0.2    0.2    80    80    80 #> 6        3 Lower -1.99  0.0000151    0.3    0.3   120   120   120"},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_nph.html","id":null,"dir":"Reference","previous_headings":"","what":"Fixed and group sequential design under non-proportional hazards — gs_design_nph","title":"Fixed and group sequential design under non-proportional hazards — gs_design_nph","text":"gs_design_nph() flexible routine provide sample size power fixed group sequential design various non-proportional hazards assumptions either single multiple strata studies. piecewise exponential distribution allows simple method specify distribtuion enrollment pattern enrollment, failure dropout rates changes time.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_nph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fixed and group sequential design under non-proportional hazards — gs_design_nph","text":"","code":"gs_design_nph(   enrollRates = tibble::tibble(Stratum = \"All\", duration = c(2, 2, 10), rate = c(3, 6,     9)),   failRates = tibble::tibble(Stratum = \"All\", duration = c(3, 100), failRate =     log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),   ratio = 1,   alpha = 0.025,   beta = 0.1,   analysisTimes = 30,   IF = c(0.25, 0.75, 1),   upper = gs_b,   upar = gsDesign::gsDesign(k = 3, test.type = 1, n.I = c(0.25, 0.75, 1), maxn.IPlan =     1, sfu = sfLDOF, sfupar = NULL)$upper$bound,   lower = gs_b,   lpar = c(qnorm(0.1), rep(-Inf, length(IF) - 1)),   r = 18 )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_nph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fixed and group sequential design under non-proportional hazards — gs_design_nph","text":"enrollRates Piecewise constant enrollment rates stratum time period. failRates Piecewise constant control group failure rates, duration piecewise constant period, hazard ratio experimental vs control, dropout rates stratum time period. ratio Experimental:Control randomization ratio alpha One-sided Type error beta Type II error analysisTimes Final calendar time beta NULL information fraction planned analyses upper Function produce upper bound upar Parameters pass upper lower Function produce lower bound lpar Parameters pass lower r Control grid size; normally leave default r=18","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_nph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fixed and group sequential design under non-proportional hazards — gs_design_nph","text":"list 3 tibbles: 1) enrollRates enrollRates$rate adjusted sample size calculation adding N cumulative enrollment end enrollment rate period, 2) failRates input, 3) codebounds row bound analysis; rows contain variables Analysis analysis number, Z Z-value bound, Probability cumulative probability crossing bound analysis alternate hypothesis input, theta standardized effect size analysis, info cumulative statistical information theta analysis, Time expected timing analysis, avehr expected average hazard ratio time analysis, Events expected events time analysis alternate hypothesis, info0 information null hypothesis expected total events alternate hypothesis, N expected enrollment time analysis.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_nph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fixed and group sequential design under non-proportional hazards — gs_design_nph","text":"","code":"library(dplyr)  # Design library(dplyr)  x <- gs_design_nph() # Notice cumulative power is 0.9 (90%) x #> $enrollRates #> # A tibble: 3 × 4 #>   Stratum duration  rate     N #>   <chr>      <dbl> <dbl> <dbl> #> 1 All            2  17.7  35.3 #> 2 All            2  35.3 106.  #> 3 All           10  53.0 636.  #>  #> $failRates #> # A tibble: 2 × 5 #>   Stratum duration failRate    hr dropoutRate #>   <chr>      <dbl>    <dbl> <dbl>       <dbl> #> 1 All            3   0.0770   0.9       0.001 #> 2 All          100   0.0385   0.6       0.001 #>  #> $bounds #> # A tibble: 6 × 11 #>   Analysis Bound       Z Probability theta  info  Time   AHR Events info0     N #>      <int> <chr>   <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> #> 1        1 Upper    4.33    0.000257 0.186  21.3  10.1 0.830   86.2  21.5  427. #> 2        2 Upper    2.34    0.563    0.314  63.2  21.1 0.730  258.   64.6  636. #> 3        3 Upper    2.01    0.900    0.364  84.4  30.0 0.695  345.   86.2  636. #> 4        1 Lower   -1.28    0.0161   0.186  21.3  10.1 0.830   86.2  21.5  427. #> 5        2 Lower -Inf       0.0161   0.314  63.2  21.1 0.730  258.   64.6  636. #> 6        3 Lower -Inf       0.0161   0.364  84.4  30.0 0.695  345.   86.2  636. #>  # Check Type I error, non-binding; should be .025 (2.5%) gs_power_nph(enrollRates = x$enrollRates,            failRates = x$failRates %>% mutate(hr = 1),            events = (x$bounds %>% filter(Bound == \"Upper\"))$Events,            upar = (x$bounds %>% filter(Bound == \"Upper\"))$Z,            lpar = rep(-Inf,3),            upper = gs_b,            lower = gs_b            ) %>% filter(abs(Z) < Inf) #> # A tibble: 3 × 10 #>   Analysis Bound     Z Probability  Time Events   AHR theta  info info0 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  4.33  0.00000737  9.66   86.2     1     0  21.5  21.5 #> 2        2 Upper  2.34  0.00965    18.8   258.      1     0  64.6  64.6 #> 3        3 Upper  2.01  0.0250     25.6   345.      1     0  86.2  86.2 # Power under proportional hazards, HR = 0.75 gs_power_nph(enrollRates = x$enrollRates,            failRates = x$failRates %>% mutate(hr = .75),            events = (x$bounds %>% filter(Bound == \"Upper\"))$Events,            upar = (x$bounds %>% filter(Bound == \"Upper\"))$Z,            lpar = rep(-Inf,3),            upper = gs_b,            lower = gs_b            ) %>% filter(abs(Z) < Inf) #> # A tibble: 3 × 10 #>   Analysis Bound     Z Probability  Time Events   AHR theta  info info0 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  4.33     0.00132  10.2   86.2  0.75 0.288  21.2  21.5 #> 2        2 Upper  2.34     0.484    20.8  258.   0.75 0.288  63.8  64.6 #> 3        3 Upper  2.01     0.750    28.7  345.   0.75 0.288  85.3  86.2"},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_wlr.html","id":null,"dir":"Reference","previous_headings":"","what":"Group sequential design using weighted log-rank test under non-proportional hazards — gs_design_wlr","title":"Group sequential design using weighted log-rank test under non-proportional hazards — gs_design_wlr","text":"Group sequential design using weighted log-rank test non-proportional hazards","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_wlr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group sequential design using weighted log-rank test under non-proportional hazards — gs_design_wlr","text":"","code":"gs_design_wlr(   enrollRates = tibble::tibble(Stratum = \"All\", duration = c(2, 2, 10), rate = c(3, 6,     9)),   failRates = tibble::tibble(Stratum = \"All\", duration = c(3, 100), failRate =     log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),   ratio = 1,   weight = wlr_weight_fh,   approx = \"asymptotic\",   alpha = 0.025,   beta = 0.1,   IF = NULL,   analysisTimes = 36,   binding = FALSE,   upper = gs_b,   upar = gsDesign(k = 3, test.type = 1, n.I = c(0.25, 0.75, 1), sfu = sfLDOF, sfupar =     NULL)$upper$bound,   lower = gs_b,   lpar = c(qnorm(0.1), -Inf, -Inf),   h1_spending = TRUE,   test_upper = TRUE,   test_lower = TRUE,   r = 18,   tol = 1e-06 )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_wlr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group sequential design using weighted log-rank test under non-proportional hazards — gs_design_wlr","text":"enrollRates enrollment rates failRates failure dropout rates ratio Experimental:Control randomization ratio (yet implemented) weight weight weighted log rank test \"1\"=unweighted, \"n\"=Gehan-Breslow, \"sqrtN\"=Tarone-Ware, \"FH_p[]_q[b]\"= Fleming-Harrington p=q=b approx approximate estimation method Z statistics \"event driven\" = work proportional hazard model log rank test \"asymptotic\" alpha One-sided Type error beta Type II error Targeted information fraction analysis analysisTimes Minimum time analysis binding indicator whether futility bound binding; default FALSE recommended upper Function compute upper bound upar Parameter passed upper() lower Function compute lower bound lpar Parameter passed lower() h1_spending Indicator lower bound set spending alternate hypothesis (input failRates) spending used lower bound test_upper indicator analyses include upper (efficacy) bound; single value TRUE (default) indicates analyses; otherwise, logical vector length info indicate analyses efficacy bound test_lower indicator analyses include lower bound; single value TRUE (default) indicates analyses; single value FALSE indicated lower bound; otherwise, logical vector length info indicate analyses lower bound r Integer, least 2; default 18 recommended Jennison Turnbull tol Tolerance parameter boundary convergence (Z-scale)","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_wlr.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Group sequential design using weighted log-rank test under non-proportional hazards — gs_design_wlr","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_design_wlr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group sequential design using weighted log-rank test under non-proportional hazards — gs_design_wlr","text":"","code":"library(dplyr) library(mvtnorm) library(gsDesign)  enrollRates <- tibble::tibble(Stratum = \"All\", duration = 12, rate = 500/12)  failRates <- tibble::tibble(Stratum = \"All\",                             duration = c(4, 100),                             failRate = log(2) / 15,  # median survival 15 month                             hr = c(1, .6),                             dropoutRate = 0.001)   x <- gsDesign::gsSurv( k = 3 , test.type = 4 , alpha = 0.025 ,                        beta = 0.2 , astar = 0 , timing = c( 1 ) ,                        sfu = sfLDOF , sfupar = c( 0 ) , sfl = sfLDOF ,                        sflpar = c( 0 ) , lambdaC = c( 0.1 ) ,                        hr = 0.6 , hr0 = 1 , eta = 0.01 ,                        gamma = c( 10 ) ,                        R = c( 12 ) , S = NULL ,                        T = 36 , minfup = 24 , ratio = 1 )  # User defined boundary gs_design_wlr(enrollRates = enrollRates, failRates = failRates,              ratio = 1, alpha = 0.025, beta = 0.2,              weight = function(x, arm0, arm1){                  gsdmvn:::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)              },              upar = x$upper$bound,              lpar = x$lower$bound,              analysisTimes = c(12, 24, 36)) #> $enrollRates #> # A tibble: 1 × 3 #>   Stratum duration  rate #>   <chr>      <dbl> <dbl> #> 1 All           12  30.6 #>  #> $failRates #> # A tibble: 2 × 5 #>   Stratum duration failRate    hr dropoutRate #>   <chr>      <dbl>    <dbl> <dbl>       <dbl> #> 1 All            4   0.0462   1         0.001 #> 2 All          100   0.0462   0.6       0.001 #>  #> $bounds #> # A tibble: 6 × 11 #>   Analysis Bound  Time     N Events      Z Probability   AHR theta  info info0 #>      <int> <chr> <dbl> <dbl>  <dbl>  <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper    12  368.   79.0  3.71      0.00356 0.781 0.626  2.65  2.66 #> 2        2 Upper    24  368.  181.   2.51      0.504   0.666 0.765 11.3  11.6  #> 3        3 Upper    36  368.  244.   1.99      0.800   0.639 0.732 20.0  20.9  #> 4        1 Lower    12  368.   79.0 -0.236     0.105   0.781 0.626  2.65  2.66 #> 5        2 Lower    24  368.  181.   1.17      0.157   0.666 0.765 11.3  11.6  #> 6        3 Lower    36  368.  244.   1.99      0.200   0.639 0.732 20.0  20.9  #>   # Boundary derived by spending function gs_design_wlr(enrollRates = enrollRates, failRates = failRates,              ratio = 1, alpha = 0.025, beta = 0.2,              weight = function(x, arm0, arm1){                   gsdmvn:::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)              },              upper = gs_spending_bound,              upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),              lower = gs_spending_bound,              lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.2),              analysisTimes = c(12, 24, 36)) #> $enrollRates #> # A tibble: 1 × 3 #>   Stratum duration  rate #>   <chr>      <dbl> <dbl> #> 1 All           12  24.0 #>  #> $failRates #> # A tibble: 2 × 5 #>   Stratum duration failRate    hr dropoutRate #>   <chr>      <dbl>    <dbl> <dbl>       <dbl> #> 1 All            4   0.0462   1         0.001 #> 2 All          100   0.0462   0.6       0.001 #>  #> $bounds #> # A tibble: 6 × 11 #>   Analysis Bound  Time     N Events      Z  Probability   AHR theta  info info0 #>      <int> <chr> <dbl> <dbl>  <dbl>  <dbl>        <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper    12  288.   61.9  6.18  0.0000000671 0.781 0.626  2.08  2.09 #> 2        2 Upper    24  288.  142.   2.80  0.301        0.666 0.765  8.86  9.07 #> 3        3 Upper    36  288.  191.   1.97  0.800        0.639 0.732 15.7  16.4  #> 4        1 Lower    12  288.   61.9 -2.43  0.000431     0.781 0.626  2.08  2.09 #> 5        2 Lower    24  288.  142.   0.925 0.0882       0.666 0.765  8.86  9.07 #> 6        3 Lower    36  288.  191.   1.97  0.2          0.639 0.732 15.7  16.4  #>"},{"path":"https://merck.github.io/gsdmvn/reference/gs_info_ahr.html","id":null,"dir":"Reference","previous_headings":"","what":"Information and effect size based on AHR approximation — gs_info_ahr","title":"Information and effect size based on AHR approximation — gs_info_ahr","text":"Based piecewise enrollment rate, failure rate, dropout rates computes approximate information effect size using average hazard ratio model.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_info_ahr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Information and effect size based on AHR approximation — gs_info_ahr","text":"","code":"gs_info_ahr(   enrollRates = tibble::tibble(Stratum = \"All\", duration = c(2, 2, 10), rate = c(3, 6,     9)),   failRates = tibble::tibble(Stratum = \"All\", duration = c(3, 100), failRate =     log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),   ratio = 1,   events = NULL,   analysisTimes = NULL )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_info_ahr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Information and effect size based on AHR approximation — gs_info_ahr","text":"enrollRates enrollment rates failRates failure dropout rates ratio Experimental:Control randomization ratio events Targeted minimum events analysis analysisTimes Targeted minimum study duration analysis","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_info_ahr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Information and effect size based on AHR approximation — gs_info_ahr","text":"tibble columns Analysis, Time, AHR, Events, theta, info, info0.info, info0 contains statistical information H1, H0, respectively. analysis k, Time[k] maximum analysisTimes[k] expected time required accrue targeted events[k]. AHR expected average hazard ratio analysis.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_info_ahr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Information and effect size based on AHR approximation — gs_info_ahr","text":"AHR() function computes statistical information targeted event times. tEvents() function used get events average HR targeted analysisTimes.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_info_ahr.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Information and effect size based on AHR approximation — gs_info_ahr","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_info_ahr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Information and effect size based on AHR approximation — gs_info_ahr","text":"","code":"library(gsDesign) library(gsDesign2)  # Only put in targeted events gs_info_ahr(events = c(30, 40, 50)) #> # A tibble: 3 × 7 #>   Analysis  Time Events   AHR theta  info info0 #>      <int> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1  14.9   30.0 0.787 0.240  7.37  7.50 #> 2        2  19.2   40.0 0.744 0.295  9.79 10.0  #> 3        3  24.5   50.0 0.713 0.339 12.2  12.5  # Only put in targeted analysis times gs_info_ahr(analysisTimes = c(18, 27, 36)) #> # A tibble: 3 × 7 #>   Analysis  Time Events   AHR theta  info info0 #>      <int> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1    18   37.6 0.755 0.282  9.21  9.40 #> 2        2    27   54.0 0.704 0.351 13.2  13.5  #> 3        3    36   66.2 0.683 0.381 16.3  16.6  # Some analysis times after time at which targeted events accrue # Check that both Time >= input analysisTime and Events >= input events gs_info_ahr(events = c(30, 40, 50), analysisTimes = c(16, 19, 26)) #> # A tibble: 3 × 7 #>   Analysis  Time Events   AHR theta  info info0 #>      <int> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1  16     33.1 0.776 0.254  8.12  8.27 #> 2        2  19.2   40.0 0.744 0.295  9.79 10.0  #> 3        3  26     52.4 0.707 0.346 12.8  13.1  gs_info_ahr(events = c(30, 40, 50), analysisTimes = c(14, 20, 24)) #> # A tibble: 3 × 7 #>   Analysis  Time Events   AHR theta  info info0 #>      <int> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1  14.9   30.0 0.787 0.240  7.37  7.50 #> 2        2  20     41.7 0.738 0.304 10.2  10.4  #> 3        3  24.5   50.0 0.713 0.339 12.2  12.5"},{"path":"https://merck.github.io/gsdmvn/reference/gs_info_wlr.html","id":null,"dir":"Reference","previous_headings":"","what":"Information and effect size for Weighted Log-rank test — gs_info_wlr","title":"Information and effect size for Weighted Log-rank test — gs_info_wlr","text":"Based piecewise enrollment rate, failure rate, dropout rates computes approximate information effect size using average hazard ratio model.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_info_wlr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Information and effect size for Weighted Log-rank test — gs_info_wlr","text":"","code":"gs_info_wlr(   enrollRates = tibble::tibble(Stratum = \"All\", duration = c(2, 2, 10), rate = c(3, 6,     9)),   failRates = tibble::tibble(Stratum = \"All\", duration = c(3, 100), failRate =     log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),   ratio = 1,   events = NULL,   analysisTimes = NULL,   weight = wlr_weight_fh,   approx = \"asymptotic\" )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_info_wlr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Information and effect size for Weighted Log-rank test — gs_info_wlr","text":"enrollRates enrollment rates failRates failure dropout rates ratio Experimental:Control randomization ratio events Targeted minimum events analysis analysisTimes Targeted minimum study duration analysis weight weight weighted log rank test \"1\"=unweighted, \"n\"=Gehan-Breslow, \"sqrtN\"=Tarone-Ware, \"FH_p[]_q[b]\"= Fleming-Harrington p=q=b approx approximate estimation method Z statistics \"event driven\" = work proportional hazard model log rank test \"asymptotic\"","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_info_wlr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Information and effect size for Weighted Log-rank test — gs_info_wlr","text":"tibble columns Analysis, Time, N, Events, AHR, delta, sigma2, theta, info, info0.info, info0 contains statistical information H1, H0, respectively. analysis k, Time[k] maximum analysisTimes[k] expected time required accrue targeted events[k]. AHR expected average hazard ratio analysis.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_info_wlr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Information and effect size for Weighted Log-rank test — gs_info_wlr","text":"AHR() function computes statistical information targeted event times. tEvents() function used get events average HR targeted analysisTimes.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_ahr.html","id":null,"dir":"Reference","previous_headings":"","what":"Group sequential design power using average hazard ratio under non-proportional hazards — gs_power_ahr","title":"Group sequential design power using average hazard ratio under non-proportional hazards — gs_power_ahr","text":"Group sequential design power using average hazard ratio non-proportional hazards","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_ahr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group sequential design power using average hazard ratio under non-proportional hazards — gs_power_ahr","text":"","code":"gs_power_ahr(   enrollRates = tibble::tibble(Stratum = \"All\", duration = c(2, 2, 10), rate = c(3, 6,     9)),   failRates = tibble::tibble(Stratum = \"All\", duration = c(3, 100), failRate =     log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),   ratio = 1,   events = c(30, 40, 50),   analysisTimes = NULL,   binding = FALSE,   upper = gs_b,   upar = gsDesign(k = length(events), test.type = 1, n.I = events, maxn.IPlan =     max(events), sfu = sfLDOF, sfupar = NULL)$upper$bound,   lower = gs_b,   lpar = c(qnorm(0.1), rep(-Inf, length(events) - 1)),   test_upper = TRUE,   test_lower = TRUE,   r = 18,   tol = 1e-06 )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_ahr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group sequential design power using average hazard ratio under non-proportional hazards — gs_power_ahr","text":"enrollRates enrollment rates failRates failure dropout rates ratio Experimental:Control randomization ratio (yet implemented) events Targeted events analysis analysisTimes Minimum time analysis binding indicator whether futility bound binding; default FALSE recommended upper Function compute upper bound upar Parameter passed upper() lower Function compute lower bound lpar Parameter passed lower() test_upper indicator analyses include upper (efficacy) bound; single value TRUE (default) indicates analyses; otherwise, logical vector length info indicate analyses efficacy bound test_lower indicator analyses include lower bound; single value TRUE (default) indicates analyses; single value FALSE indicated lower bound; otherwise, logical vector length info indicate analyses lower bound r Integer, least 2; default 18 recommended Jennison Turnbull tol Tolerance parameter boundary convergence (Z-scale)","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_ahr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Group sequential design power using average hazard ratio under non-proportional hazards — gs_power_ahr","text":"tibble columns Analysis, Bound, Z, Probability, theta, Time, AHR, Events. Contains row analysis bound.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_ahr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Group sequential design power using average hazard ratio under non-proportional hazards — gs_power_ahr","text":"Bound satisfy input upper bound specification upper, upar lower bound specification lower, lpar. AHR() function computes statistical information targeted event times. tEvents() function used get events average HR targeted analysisTimes.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_ahr.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Group sequential design power using average hazard ratio under non-proportional hazards — gs_power_ahr","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_ahr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group sequential design power using average hazard ratio under non-proportional hazards — gs_power_ahr","text":"","code":"library(gsDesign2) library(dplyr)  gs_power_ahr() %>% filter(abs(Z) < Inf) #> # A tibble: 4 × 10 #>   Analysis Bound  Time Events     Z Probability   AHR theta  info info0 #>      <int> <chr> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  14.9   30.0  2.67      0.0219 0.787 0.240  7.37  7.50 #> 2        2 Upper  19.2   40.0  2.29      0.0885 0.744 0.295  9.79 10.0  #> 3        3 Upper  24.5   50.0  2.03      0.206  0.713 0.339 12.2  12.5  #> 4        1 Lower  14.9   30.0 -1.28      0.0266 0.787 0.240  7.37  7.50  # 2-sided symmetric O'Brien-Fleming spending bound # NOT CURRENTLY WORKING gs_power_ahr(analysisTimes = c(12, 24, 36),               binding = TRUE,               upper = gs_spending_bound,               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),               lower = gs_spending_bound,               lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL)) #> # A tibble: 6 × 10 #>   Analysis Bound  Time Events      Z Probability   AHR theta  info info0 #>      <int> <chr> <dbl>  <dbl>  <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  14.9   30.0  3.13     0.00656  0.787 0.240  7.37  7.50 #> 2        2 Upper  24     49.1  2.37     0.114    0.715 0.335 12.0  12.3  #> 3        3 Upper  36     66.2  2.01     0.323    0.683 0.381 16.3  16.6  #> 4        1 Lower  14.9   30.0 -2.48     0.000871 0.787 0.240  7.37  7.50 #> 5        2 Lower  24     49.1 -1.21     0.00906  0.715 0.335 12.0  12.3  #> 6        3 Lower  36     66.2 -0.474    0.0250   0.683 0.381 16.3  16.6"},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_combo.html","id":null,"dir":"Reference","previous_headings":"","what":"Group sequential design power using MaxCombo test under non-proportional hazards — gs_power_combo","title":"Group sequential design power using MaxCombo test under non-proportional hazards — gs_power_combo","text":"Group sequential design power using MaxCombo test non-proportional hazards","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_combo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group sequential design power using MaxCombo test under non-proportional hazards — gs_power_combo","text":"","code":"gs_power_combo(   enrollRates,   failRates,   fh_test,   ratio = 1,   binding = FALSE,   upper = gs_b,   upar = c(3, 2, 1),   lower = gs_b,   lpar = c(-1, 0, 1),   algorithm = GenzBretz(maxpts = 1e+05, abseps = 1e-05),   ... )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_combo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group sequential design power using MaxCombo test under non-proportional hazards — gs_power_combo","text":"enrollRates enrollment rates failRates failure dropout rates fh_test data frame summarize test analysis. Refer examples data structure. ratio Experimental:Control randomization ratio (yet implemented) binding indicator whether futility bound binding; default FALSE recommended upper Function compute upper bound upar Parameter passed upper() lower Function compute lower bound lpar Parameter passed lower() algorithm object class GenzBretz,                     Miwa TVPACK                     specifying algorithm used well                     associated hyper parameters. ... additional parameters transfer mvtnorm::pmvnorm","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_combo.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Group sequential design power using MaxCombo test under non-proportional hazards — gs_power_combo","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_combo.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group sequential design power using MaxCombo test under non-proportional hazards — gs_power_combo","text":"","code":"library(dplyr) library(mvtnorm) library(gsDesign)  enrollRates <- tibble::tibble(Stratum = \"All\", duration = 12, rate = 500/12)  failRates <- tibble::tibble(Stratum = \"All\",                             duration = c(4, 100),                             failRate = log(2) / 15,  # median survival 15 month                             hr = c(1, .6),                             dropoutRate = 0.001)  fh_test <- rbind( data.frame(rho = 0, gamma = 0, tau = -1,                              test = 1,                              Analysis = 1:3,                              analysisTimes = c(12, 24, 36)),                   data.frame(rho = c(0, 0.5), gamma = 0.5, tau = -1,                              test = 2:3,                              Analysis = 3, analysisTimes = 36) )  # User defined bound gs_power_combo(enrollRates, failRates, fh_test, upar = c(3,2,1), lpar = c(-1, 0, 1)) #>   Analysis Bound Time        N   Events  Z Probability Probability_Null #> 1        1 Upper   12 500.0001 107.3943  3  0.01750147      0.001349898 #> 3        2 Upper   24 500.0001 246.2834  2  0.72610227      0.023307604 #> 5        3 Upper   36 500.0001 331.2910  1  0.96742461      0.195644865 #> 2        1 Lower   12 500.0001 107.3943 -1  0.02926711               NA #> 4        2 Lower   24 500.0001 246.2834  0  0.03142375               NA #> 6        3 Lower   36 500.0001 331.2910  1  0.03257647               NA  # Minimal Information Fraction derived bound gs_power_combo(enrollRates, failRates, fh_test,                upper = gs_spending_combo,                upar  = list(sf = gsDesign::sfLDOF, total_spend = 0.025),                lower = gs_spending_combo,                lpar  = list(sf = gsDesign::sfLDOF, total_spend = 0.2)) #>   Analysis Bound Time        N   Events         Z  Probability Probability_Null #> 1        1 Upper   12 500.0001 107.3943  6.175397 6.329275e-08     3.299865e-10 #> 3        2 Upper   24 500.0001 246.2834  2.798651 4.260145e-01     2.565830e-03 #> 5        3 Upper   36 500.0001 331.2910  2.096983 9.016211e-01     2.505002e-02 #> 2        1 Lower   12 500.0001 107.3943 -2.516527 3.269613e-04               NA #> 4        2 Lower   24 500.0001 246.2834  1.237721 8.468664e-02               NA #> 6        3 Lower   36 500.0001 331.2910  2.096983 9.838158e-02               NA"},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_npe.html","id":null,"dir":"Reference","previous_headings":"","what":"Group sequential bound computation with non-constant effect — gs_power_npe","title":"Group sequential bound computation with non-constant effect — gs_power_npe","text":"gs_power_npe() derives group sequential bounds boundary crossing probabilities design. allows non-constant treatment effect time, also can applied usual homogeneous effect size designs. requires treatment effect statistical information analysis well method deriving bounds, spending. routine enables two things available gsDesign package: 1) non-constant effect, 2) flexibility boundary selection. many applications, non-proportional-hazards design function gs_design_nph() used; calls function. Initial bound types supported 1) spending bounds, 2) fixed bounds, 3) Haybittle-Peto-like bounds. requirement boundary update method can bound without knowledge future bounds. example, bounds based conditional power require knowledge future bounds supported routine; limited conditional power method demonstrated. Boundary family designs Wang-Tsiatis designs including original (non-spending-function-based) O'Brien-Fleming Pocock designs supported gs_power_npe().","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_npe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group sequential bound computation with non-constant effect — gs_power_npe","text":"","code":"gs_power_npe(   theta = 0.1,   theta1 = NULL,   info = 1,   info1 = NULL,   info0 = NULL,   binding = FALSE,   upper = gs_b,   lower = gs_b,   upar = qnorm(0.975),   lpar = -Inf,   test_upper = TRUE,   test_lower = TRUE,   r = 18,   tol = 1e-06 )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_npe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group sequential bound computation with non-constant effect — gs_power_npe","text":"theta natural parameter group sequential design representing expected incremental drift analyses; used power calculation theta1 natural parameter alternate hypothesis, needed lower bound computation info statistical information analyses input theta info1 statistical information hypothesis used futility bound calculation different info; impacts futility hypothesis bound calculation info0 statistical information null hypothesis, different info; impacts null hypothesis bound calculation binding indicator whether futility bound binding; default FALSE recommended upper function compute upper bound lower function compare lower bound upar parameter pass upper lpar parameter pass lower test_upper indicator analyses include upper (efficacy) bound; single value TRUE (default)  indicates analyses; otherwise, logical vector length info indicate analyses efficacy bound test_lower indicator analyses include lower bound; single value TRUE (default) indicates analyses; single value FALSE indicated lower bound; otherwise, logical vector length info indicate analyses lower bound r Integer, least 2; default 18 recommended Jennison Turnbull tol Tolerance parameter boundary convergence (Z-scale)","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_npe.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Group sequential bound computation with non-constant effect — gs_power_npe","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_npe.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Group sequential bound computation with non-constant effect — gs_power_npe","text":"Keaven Anderson keaven_anderson@merck.com","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_npe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group sequential bound computation with non-constant effect — gs_power_npe","text":"","code":"library(gsDesign) library(dplyr)  # Default (single analysis; Type I error controlled) gs_power_npe(theta=0) %>% filter(Bound==\"Upper\") #> # A tibble: 1 × 9 #>   Analysis Bound     Z Probability theta theta1  info info0 info1 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  1.96      0.0250     0      0     1     1     1  # Fixed bound gs_power_npe(theta = c(.1, .2, .3), info = (1:3) * 40, info0 = (1:3) * 40,              upper = gs_b,               upar = gsDesign::gsDesign(k = 3,                                        sfu = gsDesign::sfLDOF)$upper$bound,              lower = gs_b,               lpar = c(-1, 0, 0)) #> # A tibble: 6 × 9 #>   Analysis Bound     Z Probability theta theta1  info info0 info1 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  3.71     0.00104   0.1    0.1    40    40    40 #> 2        2 Upper  2.51     0.235     0.2    0.2    80    80    80 #> 3        3 Upper  1.99     0.869     0.3    0.3   120   120   120 #> 4        1 Lower -1        0.0513    0.1    0.1    40    40    40 #> 5        2 Lower  0        0.0715    0.2    0.2    80    80    80 #> 6        3 Lower  0        0.0715    0.3    0.3   120   120   120  # Same fixed efficacy bounds,  # no futility bound (i.e., non-binding bound), null hypothesis gs_power_npe(theta = rep(0,3),               info = (1:3) * 40,              upar = gsDesign::gsDesign(k = 3,                                        sfu = gsDesign::sfLDOF)$upper$bound,              lpar = rep(-Inf, 3)) %>% filter(Bound==\"Upper\") #> # A tibble: 3 × 9 #>   Analysis Bound     Z Probability theta theta1  info info0 info1 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  3.71    0.000104     0      0    40    40    40 #> 2        2 Upper  2.51    0.00605      0      0    80    80    80 #> 3        3 Upper  1.99    0.0250       0      0   120   120   120  # Fixed bound with futility only at analysis 1;  # efficacy only at analyses 2, 3 gs_power_npe(theta = c(.1, .2, .3),               info = (1:3) * 40,               upar = c(Inf, 3, 2),               lpar = c(qnorm(.1), -Inf, -Inf)) #> # A tibble: 6 × 9 #>   Analysis Bound       Z Probability theta theta1  info info0 info1 #>      <int> <chr>   <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  Inf         0        0.1    0.1    40    40    40 #> 2        2 Upper    3         0.113    0.2    0.2    80    80    80 #> 3        3 Upper    2         0.887    0.3    0.3   120   120   120 #> 4        1 Lower   -1.28      0.0278   0.1    0.1    40    40    40 #> 5        2 Lower -Inf         0.0278   0.2    0.2    80    80    80 #> 6        3 Lower -Inf         0.0278   0.3    0.3   120   120   120  # Spending function bounds # Lower spending based on non-zero effect gs_power_npe(theta = c(.1, .2, .3),               info = (1:3) * 40,              upper = gs_spending_bound,              upar = list(sf = gsDesign::sfLDOF,                           total_spend = 0.025,                           param = NULL,                           timing = NULL),              lower = gs_spending_bound,              lpar = list(sf = gsDesign::sfHSD,                           total_spend = 0.1,                           param = -1,                           timing = NULL)) #> # A tibble: 6 × 9 #>   Analysis Bound       Z Probability theta theta1  info info0 info1 #>      <int> <chr>   <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  3.71       0.00104   0.1    0.1    40    40    40 #> 2        2 Upper  2.51       0.235     0.2    0.2    80    80    80 #> 3        3 Upper  1.99       0.883     0.3    0.3   120   120   120 #> 4        1 Lower -1.36       0.0230    0.1    0.1    40    40    40 #> 5        2 Lower  0.0726     0.0552    0.2    0.2    80    80    80 #> 6        3 Lower  1.86       0.100     0.3    0.3   120   120   120  # Same bounds, but power under different theta gs_power_npe(theta = c(.15, .25, .35),               theta1 = c(.1, .2, .3),               info = (1:3) * 40,              upper = gs_spending_bound,              upar = list(sf = gsDesign::sfLDOF,                           total_spend = 0.025,                           param = NULL,                           timing = NULL),              lower = gs_spending_bound,              lpar = list(sf = gsDesign::sfHSD,                           total_spend = 0.1,                           param = -1,                           timing = NULL)) #> # A tibble: 6 × 9 #>   Analysis Bound       Z Probability theta theta1  info info0 info1 #>      <int> <chr>   <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  3.71       0.00288  0.15    0.1    40    40    40 #> 2        2 Upper  2.51       0.392    0.25    0.2    80    80    80 #> 3        3 Upper  1.99       0.956    0.35    0.3   120   120   120 #> 4        1 Lower -1.36       0.0104   0.15    0.1    40    40    40 #> 5        2 Lower  0.0726     0.0221   0.25    0.2    80    80    80 #> 6        3 Lower  1.86       0.0370   0.35    0.3   120   120   120  # Two-sided symmetric spend, O'Brien-Fleming spending # Typically, 2-sided bounds are binding xx <- gs_power_npe(theta = rep(0, 3),                     theta1 = rep(0, 3),                     info = (1:3) * 40,                    upper = gs_spending_bound,                    binding = TRUE,                    upar = list(sf = gsDesign::sfLDOF,                                 total_spend = 0.025,                                 param = NULL,                                 timing = NULL),                    lower = gs_spending_bound,                    lpar = list(sf = gsDesign::sfLDOF,                                 total_spend = 0.025,                                 param = NULL,                                 timing = NULL)) xx #> # A tibble: 6 × 9 #>   Analysis Bound     Z Probability theta theta1  info info0 info1 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  3.71    0.000104     0      0    40    40    40 #> 2        2 Upper  2.51    0.00605      0      0    80    80    80 #> 3        3 Upper  1.99    0.0250       0      0   120   120   120 #> 4        1 Lower -3.71    0.000104     0      0    40    40    40 #> 5        2 Lower -2.51    0.00605      0      0    80    80    80 #> 6        3 Lower -1.99    0.0250       0      0   120   120   120  # Re-use these bounds under alternate hypothesis # Always use binding = TRUE for power calculations upar <- (xx %>% filter(Bound == \"Upper\"))$Z gs_power_npe(theta = c(.1, .2, .3),               info = (1:3) * 40,              binding = TRUE,              upar = upar,              lpar = -upar) #> # A tibble: 6 × 9 #>   Analysis Bound     Z Probability theta theta1  info info0 info1 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  3.71  0.00104      0.1    0.1    40    40    40 #> 2        2 Upper  2.51  0.235        0.2    0.2    80    80    80 #> 3        3 Upper  1.99  0.902        0.3    0.3   120   120   120 #> 4        1 Lower -3.71  0.00000704   0.1    0.1    40    40    40 #> 5        2 Lower -2.51  0.0000151    0.2    0.2    80    80    80 #> 6        3 Lower -1.99  0.0000151    0.3    0.3   120   120   120"},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_nph.html","id":null,"dir":"Reference","previous_headings":"","what":"Group sequential design power under non-proportional hazards — gs_power_nph","title":"Group sequential design power under non-proportional hazards — gs_power_nph","text":"Group sequential design power non-proportional hazards","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_nph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group sequential design power under non-proportional hazards — gs_power_nph","text":"","code":"gs_power_nph(   enrollRates = tibble::tibble(Stratum = \"All\", duration = c(2, 2, 10), rate = c(3, 6,     9)),   failRates = tibble::tibble(Stratum = \"All\", duration = c(3, 100), failRate =     log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),   ratio = 1,   events = c(30, 40, 50),   analysisTimes = NULL,   maxEvents = 45,   upper = gs_b,   upar = gsDesign(k = length(events), test.type = 1, n.I = events, maxn.IPlan =     maxEvents, sfu = sfLDOF, sfupar = NULL)$upper$bound,   lower = gs_b,   lpar = c(qnorm(0.1), rep(-Inf, 2)),   r = 18 )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_nph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group sequential design power under non-proportional hazards — gs_power_nph","text":"enrollRates enrollment rates failRates failure dropout rates ratio Experimental:Control randomization ratio (yet implemented) events Targeted events analysis analysisTimes yet implemented maxEvents Final planned events upper Function compute upper bound upar Parameter passed upper() lower Function compute lower bound lpar Parameter passed lower() r Control grid size; normally leave default r=18","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_nph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Group sequential design power under non-proportional hazards — gs_power_nph","text":"tibble columns Analysis, Bound, Z, Probability, theta, Time, avehr, Events","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_nph.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Group sequential design power under non-proportional hazards — gs_power_nph","text":"Need added","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_nph.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Group sequential design power under non-proportional hazards — gs_power_nph","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_nph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group sequential design power under non-proportional hazards — gs_power_nph","text":"","code":"library(gsDesign) library(gsDesign2) library(dplyr)  gs_power_nph() %>% filter(abs(Z) < Inf) #> # A tibble: 4 × 10 #>   Analysis Bound     Z Probability  Time Events   AHR theta  info info0 #>      <int> <chr> <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> #> 1        1 Upper  2.51      0.0316  14.9   30.0 0.787 0.240  7.37  7.50 #> 2        2 Upper  2.15      0.113   19.2   40.0 0.744 0.295  9.79 10.0  #> 3        3 Upper  2.12      0.192   24.5   50.0 0.713 0.339 12.2  12.5  #> 4        1 Lower -1.28      0.0266  14.9   30.0 0.787 0.240  7.37  7.50"},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_wlr.html","id":null,"dir":"Reference","previous_headings":"","what":"Group sequential design power using weighted log rank test under non-proportional hazards — gs_power_wlr","title":"Group sequential design power using weighted log rank test under non-proportional hazards — gs_power_wlr","text":"Group sequential design power using weighted log rank test non-proportional hazards","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_wlr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group sequential design power using weighted log rank test under non-proportional hazards — gs_power_wlr","text":"","code":"gs_power_wlr(   enrollRates = tibble::tibble(Stratum = \"All\", duration = c(2, 2, 10), rate = c(3, 6,     9)),   failRates = tibble::tibble(Stratum = \"All\", duration = c(3, 100), failRate =     log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),   ratio = 1,   weight = wlr_weight_fh,   approx = \"asymptotic\",   events = c(30, 40, 50),   analysisTimes = NULL,   binding = FALSE,   upper = gs_b,   upar = gsDesign(k = length(events), test.type = 1, n.I = events, maxn.IPlan =     max(events), sfu = sfLDOF, sfupar = NULL)$upper$bound,   lower = gs_b,   lpar = c(qnorm(0.1), rep(-Inf, length(events) - 1)),   test_upper = TRUE,   test_lower = TRUE,   r = 18,   tol = 1e-06 )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_wlr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group sequential design power using weighted log rank test under non-proportional hazards — gs_power_wlr","text":"enrollRates enrollment rates failRates failure dropout rates ratio Experimental:Control randomization ratio (yet implemented) weight weight weighted log rank test \"1\"=unweighted, \"n\"=Gehan-Breslow, \"sqrtN\"=Tarone-Ware, \"FH_p[]_q[b]\"= Fleming-Harrington p=q=b approx approximate estimation method Z statistics \"event driven\" = work proportional hazard model log rank test \"asymptotic\" events Targeted events analysis analysisTimes Minimum time analysis binding indicator whether futility bound binding; default FALSE recommended upper Function compute upper bound upar Parameter passed upper() lower Function compute lower bound lpar Parameter passed lower() test_upper indicator analyses include upper (efficacy) bound; single value TRUE (default) indicates analyses; otherwise, logical vector length info indicate analyses efficacy bound test_lower indicator analyses include lower bound; single value TRUE (default) indicates analyses; single value FALSE indicated lower bound; otherwise, logical vector length info indicate analyses lower bound r Integer, least 2; default 18 recommended Jennison Turnbull tol Tolerance parameter boundary convergence (Z-scale)","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_power_wlr.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Group sequential design power using weighted log rank test under non-proportional hazards — gs_power_wlr","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_prob.html","id":null,"dir":"Reference","previous_headings":"","what":"Group sequential boundary crossing probabilities — gs_prob","title":"Group sequential boundary crossing probabilities — gs_prob","text":"Group sequential boundary crossing probabilities","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_prob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Group sequential boundary crossing probabilities — gs_prob","text":"","code":"gs_prob(theta, upper = gs_b, lower = gs_b, upar, lpar, info, r = 18)"},{"path":"https://merck.github.io/gsdmvn/reference/gs_prob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Group sequential boundary crossing probabilities — gs_prob","text":"theta natural parameter group sequentia design representing expected drift time analysis upper function compute upper bound lower function compare lower bound upar parameter pass upper lpar parameter pass lower info statistical information analysis r Integer, least 2; default 18 recommended Jennison Turnbull","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_prob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Group sequential boundary crossing probabilities — gs_prob","text":"tibble row finite bound analysis containing following variables: Analysis analysis number Bound Upper (efficacy) Lower (futility) Z Z-value bound Probability probability first bound crossed given input theta approximate natural parameter value required cross bound","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_prob.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Group sequential boundary crossing probabilities — gs_prob","text":"Approximation theta based Wald test assumes observed information equal expected.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_prob.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Group sequential boundary crossing probabilities — gs_prob","text":"","code":"library(dplyr) # Asymmetric 2-sided design gs_prob(theta = 0, upar = rep(2.2, 3), lpar = rep(0, 3),          upper=gs_b, lower=gs_b,  info = 1:3) #> # A tibble: 6 × 6 #>   Analysis Bound     Z Probability theta  info #>      <int> <chr> <dbl>       <dbl> <dbl> <int> #> 1        1 Upper   2.2      0.0139     0     1 #> 2        2 Upper   2.2      0.0236     0     2 #> 3        3 Upper   2.2      0.0305     0     3 #> 4        1 Lower   0        0.5        0     1 #> 5        2 Lower   0        0.625      0     2 #> 6        3 Lower   0        0.687      0     3 # One-sided design x <- gs_prob(theta = 0, upar = rep(2.2, 3), lpar = rep(-Inf, 3),               upper=gs_b, lower=gs_b,  info = 1:3) # Without filtering, this shows unneeded lower bound x #> # A tibble: 6 × 6 #>   Analysis Bound      Z Probability theta  info #>      <int> <chr>  <dbl>       <dbl> <dbl> <int> #> 1        1 Upper    2.2      0.0139     0     1 #> 2        2 Upper    2.2      0.0237     0     2 #> 3        3 Upper    2.2      0.0311     0     3 #> 4        1 Lower -Inf        0          0     1 #> 5        2 Lower -Inf        0          0     2 #> 6        3 Lower -Inf        0          0     3 # Filter to just show bounds intended for use x %>% filter(abs(Z) < Inf) #> # A tibble: 3 × 6 #>   Analysis Bound     Z Probability theta  info #>      <int> <chr> <dbl>       <dbl> <dbl> <int> #> 1        1 Upper   2.2      0.0139     0     1 #> 2        2 Upper   2.2      0.0237     0     2 #> 3        3 Upper   2.2      0.0311     0     3"},{"path":"https://merck.github.io/gsdmvn/reference/gs_prob_combo.html","id":null,"dir":"Reference","previous_headings":"","what":"MaxCombo Group sequential boundary crossing probabilities — gs_prob_combo","title":"MaxCombo Group sequential boundary crossing probabilities — gs_prob_combo","text":"MaxCombo Group sequential boundary crossing probabilities","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_prob_combo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MaxCombo Group sequential boundary crossing probabilities — gs_prob_combo","text":"","code":"gs_prob_combo(   upper_bound,   lower_bound,   analysis,   theta,   corr,   algorithm = GenzBretz(maxpts = 1e+05, abseps = 1e-05),   ... )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_prob_combo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MaxCombo Group sequential boundary crossing probabilities — gs_prob_combo","text":"upper_bound numeric vector upper bound lower_bound numeric vector lower bound analysis integer vector interim analysis index theta numeric vector effect size alternative hypothesis corr matrix correlation matrix algorithm object class GenzBretz,                     Miwa TVPACK                     specifying algorithm used well                     associated hyper parameters. ... additional parameters transfer mvtnorm::pmvnorm","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_spending_bound.html","id":null,"dir":"Reference","previous_headings":"","what":"Derive spending bound for group sequential boundary — gs_spending_bound","title":"Derive spending bound for group sequential boundary — gs_spending_bound","text":"Computes one bound time based spending given distributional assumptions. user specifies gs_spending_bound() use functions, intended use . important user specifications made list provided functions using gs_spending_bound(). Function uses numerical integration Newton-Raphson iteration derive individual bound group sequential design satisfies targeted boundary crossing probability. Algorithm simple extension Chapter 19 Jennison Turnbull (2000).","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_spending_bound.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Derive spending bound for group sequential boundary — gs_spending_bound","text":"","code":"gs_spending_bound(   k = 1,   par = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL,     max_info = NULL),   hgm1 = NULL,   theta = 0.1,   info = 1:3,   efficacy = TRUE,   test_bound = TRUE,   r = 18,   tol = 1e-06 )"},{"path":"https://merck.github.io/gsdmvn/reference/gs_spending_bound.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Derive spending bound for group sequential boundary — gs_spending_bound","text":"k analysis bound computed par list following items: sf (class spending function), total_spend (total spend), param (parameters needed spending function sf()), timing (vector containing values spending function evaluated NULL information-based spending used), max_info (timing NULL, can input positive number used info information fraction analysis) hgm1 subdensity grid h1 (k=2) hupdate (k>2) analysis k-1; k=1, used may NULL theta natural parameter used lower bound spending; represents average drift time analysis least analysis k; upper bound spending always set null hypothesis (theta = 0) info statistical information analyses, least analysis k efficacy TRUE (default) efficacy bound, FALSE otherwise test_bound logical vector length info indicate analyses bound r Integer, least 2; default 18 recommended Jennison Turnbull tol Tolerance parameter convergence (Z-scale)","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_spending_bound.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Derive spending bound for group sequential boundary — gs_spending_bound","text":"returns numeric bound (possibly infinite) , upon failure, generates error message.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_spending_bound.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Derive spending bound for group sequential boundary — gs_spending_bound","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_spending_bound.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Derive spending bound for group sequential boundary — gs_spending_bound","text":"Jennison C Turnbull BW (2000), Group Sequential Methods Applications Clinical Trials. Boca Raton: Chapman Hall.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_spending_bound.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Derive spending bound for group sequential boundary — gs_spending_bound","text":"Keaven Anderson keaven_anderson@merck.com","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_spending_combo.html","id":null,"dir":"Reference","previous_headings":"","what":"Derive spending bound for MaxCombo group sequential boundary — gs_spending_combo","title":"Derive spending bound for MaxCombo group sequential boundary — gs_spending_combo","text":"Derive spending bound MaxCombo group sequential boundary","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_spending_combo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Derive spending bound for MaxCombo group sequential boundary — gs_spending_combo","text":"","code":"gs_spending_combo(par = NULL, info = NULL, ...)"},{"path":"https://merck.github.io/gsdmvn/reference/gs_spending_combo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Derive spending bound for MaxCombo group sequential boundary — gs_spending_combo","text":"par list following items: sf (class spending function), total_spend (total spend), param (parameters needed spending function sf()), timing (vector containing values spending function evaluated NULL information-based spending used), max_info (timing NULL, can input positive number used info information fraction analysis) info statistical information analyses, least analysis k ... additional parameters transfered par$sf.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gs_spending_combo.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Derive spending bound for MaxCombo group sequential boundary — gs_spending_combo","text":"","code":"# alpha-spending par <- list(sf = gsDesign::sfLDOF, total_spend = 0.025) gs_spending_combo(par, info = 1:3/3) #> [1] 0.0001035057 0.0060483891 0.0250000000  # beta-spending par <- list(sf = gsDesign::sfLDOF, total_spend = 0.2) gs_spending_combo(par, info = 1:3/3) #> [1] 0.02643829 0.11651432 0.20000000"},{"path":"https://merck.github.io/gsdmvn/reference/gsdmvn.html","id":null,"dir":"Reference","previous_headings":"","what":"gsdmvn: A package for group sequential design under non-proportional hazards (NPH) — gsdmvn","title":"gsdmvn: A package for group sequential design under non-proportional hazards (NPH) — gsdmvn","text":"gsdmvn package eventually incorporated gsDesign2 package. version Regulatory/ASA Biopharmaceutical Subsection training course September, 2020 package computes asymptotic normal distribution group sequential designs, generalizing theory presented Jennison Turnbull (2000) cases non-homogeneous treatment effect time. primary application group sequential design assumption non-proportional hazards. gsdmvn package 4 types functions support asymptotic normal distribution computation, support group sequential bound derivation, support design power calculations. applications designs survival analysis non-proportional hazards assumptions.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gsdmvn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gsdmvn: A package for group sequential design under non-proportional hazards (NPH) — gsdmvn","text":"addition function categeories, vignettes show implement design binomial endpoint example extend package endpoint types, extensive capabilities around group sequential boundary calculations, including enabling capabilities gsDesign package.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gsdmvn.html","id":"gsdmvn-functions","dir":"Reference","previous_headings":"","what":"gsdmvn functions","title":"gsdmvn: A package for group sequential design under non-proportional hazards (NPH) — gsdmvn","text":"primary functions supporting non-proportional hazards short course : gs_design_ahr - derive group sequential design non-proportional hazards (NPH) logrank test gs_power_ahr - compute power group sequential design non-proportional hazards logrank test Key supportive functions specify bound derivation designs: gs_b - directly provide bounds designs gs_spending_bound - provide bounds based spending function (e.g., Lan-DeMets O'Brien-Fleming) Underlying functions support numerical integration directly needed typical users gridpts - set grid points weights numerical integration normal distribution h1 - initialize numerical integration grid points weights NPH first analysis hupdate - update numerical integration grid points weights NPH one interim next gs_power_npe - general non-constant-effect size boundary crossing probability calculation group sequential design","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gsdmvn.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"gsdmvn: A package for group sequential design under non-proportional hazards (NPH) — gsdmvn","text":"Jennison C Turnbull BW (2000), Group Sequential Methods Applications Clinical Trials. Boca Raton: Chapman Hall.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/gsdmvn.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"gsdmvn: A package for group sequential design under non-proportional hazards (NPH) — gsdmvn","text":"Keaven Anderson keaven_anderson@merck.com","code":""},{"path":"https://merck.github.io/gsdmvn/reference/h1.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize numerical integration for group sequential design — h1","title":"Initialize numerical integration for group sequential design — h1","text":"Compute grid points first interim analysis group sequential design","code":""},{"path":"https://merck.github.io/gsdmvn/reference/h1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize numerical integration for group sequential design — h1","text":"","code":"h1(r = 18, theta = 0, I = 1, a = -Inf, b = Inf)"},{"path":"https://merck.github.io/gsdmvn/reference/h1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize numerical integration for group sequential design — h1","text":"r Integer, least 2; default 18 recommended Jennison Turnbull theta Drift parameter first analysis Information first analysis lower limit integration (scalar) b upper limit integration (scalar > )","code":""},{"path":"https://merck.github.io/gsdmvn/reference/h1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initialize numerical integration for group sequential design — h1","text":"tibble grid points z, numerical integration weights w, normal density mean mu = theta * sqrt{} variance 1 times weight w.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/h1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Initialize numerical integration for group sequential design — h1","text":"Mean standard normal distribution consideration mu = theta * sqrt()","code":""},{"path":"https://merck.github.io/gsdmvn/reference/h1.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Initialize numerical integration for group sequential design — h1","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/h1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initialize numerical integration for group sequential design — h1","text":"","code":"library(dplyr) # Replicate variance of 1, mean of 35 h1(theta = 5, I = 49) %>% summarise(mu = sum(z * h), var = sum((z - mu)^2 * h)) #> # A tibble: 1 × 2 #>      mu   var #>   <dbl> <dbl> #> 1  35.0  1.00  # Replicate p-value of .0001 by numerical integration of tail h1(a = qnorm(.9999)) %>% summarise(p = sum(h)) #> # A tibble: 1 × 1 #>          p #>      <dbl> #> 1 0.000100"},{"path":"https://merck.github.io/gsdmvn/reference/hupdate.html","id":null,"dir":"Reference","previous_headings":"","what":"Update numerical integration for group sequential design — hupdate","title":"Update numerical integration for group sequential design — hupdate","text":"Update grid points numerical integration one analysis next","code":""},{"path":"https://merck.github.io/gsdmvn/reference/hupdate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update numerical integration for group sequential design — hupdate","text":"","code":"hupdate(   r = 18,   theta = 0,   I = 2,   a = -Inf,   b = Inf,   thetam1 = 0,   Im1 = 1,   gm1 = h1() )"},{"path":"https://merck.github.io/gsdmvn/reference/hupdate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update numerical integration for group sequential design — hupdate","text":"r Integer, least 2; default 18 recommended Jennison Turnbull theta Drift parameter current analysis Information current analysis lower limit integration (scalar) b upper limit integration (scalar > ) thetam1 Drift parameter previous analysis Im1 Information previous analysis gm1 numerical integration grid h1() previous run hupdate()","code":""},{"path":"https://merck.github.io/gsdmvn/reference/hupdate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update numerical integration for group sequential design — hupdate","text":"tibble grid points z, numerical integration weights w, normal density mean mu = theta * sqrt{} variance 1 times weight w.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/hupdate.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Update numerical integration for group sequential design — hupdate","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/reference/hupdate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update numerical integration for group sequential design — hupdate","text":"","code":"library(dplyr) # 2nd analysis with no interim bound and drift 0 should have mean 0, variance 1 hupdate() %>% summarise(mu = sum(z * h), var = sum((z - mu)^2 * h)) #> # A tibble: 1 × 2 #>         mu   var #>      <dbl> <dbl> #> 1 4.83e-18  1.00"},{"path":"https://merck.github.io/gsdmvn/reference/pmvnorm_combo.html","id":null,"dir":"Reference","previous_headings":"","what":"Multivariate Normal Distribution for Multivariate Maximum Statistics — pmvnorm_combo","title":"Multivariate Normal Distribution for Multivariate Maximum Statistics — pmvnorm_combo","text":"Computes distribution function multivariate normal distribution maximum statistics arbitrary limits correlation matrices","code":""},{"path":"https://merck.github.io/gsdmvn/reference/pmvnorm_combo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multivariate Normal Distribution for Multivariate Maximum Statistics — pmvnorm_combo","text":"","code":"pmvnorm_combo(   lower,   upper,   group,   mean,   corr,   algorithm = GenzBretz(maxpts = 1e+05, abseps = 1e-05),   ... )"},{"path":"https://merck.github.io/gsdmvn/reference/pmvnorm_combo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multivariate Normal Distribution for Multivariate Maximum Statistics — pmvnorm_combo","text":"lower vector lower limits length n. upper vector upper limits length n. group vector test statistics group. mean mean vector length n. corr correlation matrix dimension n. algorithm object class GenzBretz,                     Miwa TVPACK                     specifying algorithm used well                     associated hyper parameters. ... additional parameters transfer mvtnorm::pmvnorm","code":""},{"path":"https://merck.github.io/gsdmvn/reference/pmvnorm_combo.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multivariate Normal Distribution for Multivariate Maximum Statistics — pmvnorm_combo","text":"Let $Z = Z_ij$ multivariate normal distribution. group indicator j within group statistics indicator. Let G_i = max(Z_ij) test within one group. program calculating probability $$Pr( lower < max(G) < upper )$$","code":""},{"path":"https://merck.github.io/gsdmvn/reference/wlr_weight.html","id":null,"dir":"Reference","previous_headings":"","what":"Weight Function of Weighted Log-rank Test — wlr_weight","title":"Weight Function of Weighted Log-rank Test — wlr_weight","text":"wlr_weight_fh Fleming-Harriongton, FH(rho, gamma) weight function. wlr_weight_1  constant log rank test wlr_weight_power Gehan-Breslow Tarone-Ware weight function.","code":""},{"path":"https://merck.github.io/gsdmvn/reference/wlr_weight.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weight Function of Weighted Log-rank Test — wlr_weight","text":"","code":"wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0, tau = NULL)  wlr_weight_1(x, arm0, arm1)  wlr_weight_n(x, arm0, arm1, power = 1)"},{"path":"https://merck.github.io/gsdmvn/reference/wlr_weight.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weight Function of Weighted Log-rank Test — wlr_weight","text":"x analysis time arm0 \"arm\" object defined npsurvSS package arm1 \"arm\" object defined npsurvSS package rho scalar parameter controls type test gamma scalar parameter controls type test tau scalar parameter cut-time modest weighted log rank test power scalar parameter controls power weight function","code":""},{"path":"https://merck.github.io/gsdmvn/reference/wlr_weight.html","id":"specification","dir":"Reference","previous_headings":"","what":"Specification","title":"Weight Function of Weighted Log-rank Test — wlr_weight","text":"contents section shown PDF user manual .","code":""},{"path":"https://merck.github.io/gsdmvn/news/index.html","id":"gsdmvn-010","dir":"Changelog","previous_headings":"","what":"gsdmvn 0.1.0","title":"gsdmvn 0.1.0","text":"Initial version. Added NEWS.md file track changes package.","code":""}]
