---
title: "Statistical information applications"
subtitle: "Binomial Trial Example"
output:
  html_vignette
bibliography: gsDesign.bib
vignette: >
  %\VignetteIndexEntry{Statistical information in binomial endpoint trials}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Overview

This vignette demonstrates different applications of statistical
information in clinical trials studying a binary outcome. Potential
applications include but are not limited to

1)  power calculations,
2)  $\beta$-spending for futility,
3)  $\alpha$-spending for efficacy,
4)  two-sided designs, and
5)  blinded sample size adaptation.

While we focus mainly on design using a risk-difference formulation, we
also consider risk-ratios. The main underlying rationale for this to
guide appropriate use of statistical information under different
scenarios. The key citation for this is @FarringtonManning which builds
on much previous literature. This citation deals with parameter
estimation variance issues for fixed designs; here we apply their
concepts to group sequential design which we believe is a new
application.

We will demonstrate this with the general design function
`gs_design_npe()` and power function `gs_power_npe()` to show different
spending considerations under the null, alternate and other hypotheses
as well as how to update bounds and/or sample size at the time of
interim analysis.

## Example scenario

We consider a scenario largely based on the @CAPTURE where the primary
endpoint was a composite of death, acute myocardial infarction or the
need for recurrent percutaneous intervention within 30 days of
randomization. That is, we consider a 2-arm trial with an experimental
arm and a control arm. The primary endpoint for the trial is a binary
indicator for each participant if they have a failed outcome. For this
case, we consider the parameter $\theta_{rd} = p_1 - p _2$ where We let
$p_1, p_2$ denote the probability that a trial participant in the
control and experimental groups, respectively, experiences a failure. We
wish to test the null hypothesis $H_0: p_1=p_2$ against the alternative
$H_1: p_1 > p_2$. More specifically, we wish to compute power for the
scenario:

$$H_1: p_1^1 = 0.15, p_2^1 = 0.10.$$

## Testing

We consider two formulations for testing as in @FarringtonManning.

-   Risk difference: $\theta_{rd} = p_1 - p_2= 0.15 - 0.1 = 0.05.$
-   Risk ratio: $\theta_{rr}=p_2/p1=0.1/0.15 = 2/3.$

We assume an overall sample size of $N$ and a sample size ratio for
experimental versus control of $r=N_2/N_1$. We begin by defining test
statistic and estimated variance formulas for a given hypothesized
$\theta_{rd}$ or $\theta_{rr}.$

+----+-------+----------------+---------------------------------------+
| P  | Hy    | Test           | $\widehat{\hbox{Var}}(X)$             |
| ar | pothe |                |                                       |
| am | sized |                |                                       |
| et | $\t   |                |                                       |
| er | heta$ |                |                                       |
+====+:=====:+:==============:+:=====================================:+
| Ri | $\th  | $X_{rd}=\hat   | $\ha t V_{rd}=\frac{1+r}{N}\le f      |
| sk | eta_{ |  p_1 -\hat p_2 | t(\tilde p_{1,rd}\tilde q_{1,rd}+\til |
| di | r d}  |  -\theta_{rd}$ | de p_{2,rd} \tilde q_{2,rd}/r\right)$ |
| ff | = p_1 |                |                                       |
| er | -p_2$ |                |                                       |
| en |       |                |                                       |
| ce |       |                |                                       |
+----+-------+----------------+---------------------------------------+
| Ri | $\th  | $X_{r r        | $\hat V_{                             |
| sk | eta_{ | } =\theta_{rr  | rr}=\frac{ 1+r}{N}\left(\theta_{r r}  |
| r  | r r}  | } \times\hat p | ^2\tilde p_{1,rr}\tilde q_{1,rr}+\til |
| at | = p_2 |  _1 -\hat p_2$ | de p_{2,rr} \tilde q_{2,rr}/r\right)$ |
| io | /p_1$ |                |                                       |
+----+-------+----------------+---------------------------------------+

We will provide the estimates
$\tilde p_{1,rd}, \tilde p_{2,rd}, \tilde p_{1,rr}, \tilde p_{2,rr}$
below, noting that $\tilde q_{1,rd}= 1-\tilde p_{1,rd},$
$\tilde q_{2,rd}=1-\tilde p_{2,rd},$
$\tilde q_{1,rr}=1 -\tilde p_{1,rr},$
$\tilde q_{2,rr}=1-\tilde p_{2,rr}.$

Some applications of the above will be:

-   Testing for superiority: $\theta_{rd}=0$ or $\theta_{rr}=0$
-   Testing for non-inferiority: $\theta_{rd}<0$ or $\theta_{rr} < 0$
-   Testing for super-superiority (e.g., for a vaccine trial):
    $\theta_{rd}>0$ or $\theta_{rr}> 0$

We will also extend the above variance forms to compute the probability
of crossing a fixed or group sequential bound under any assumed
treatment effect. This formulation will also be used compute statistical
information and spending bounds.

## Estimating $\tilde p_{1,rd}, \tilde p_{2,rd}, \tilde p_{1,rr}, \tilde p_{2,rr}$

| Nan Xiao | $\alpha$ | Col3 |
|---------:|:---------|-----:|
|    $\mu$ |          |      |
|          |          |      |
|          |          |      |

: xyzzy

@FarringtonManning recommended the maximum likelihood approach for
estimating
$\tilde p_{1,rd}, \tilde p_{2,rd}, \tilde p_{1,rr}, \tilde p_{2,rr}$ as
previously suggested by @MiettinenNurminen and @Koopman. Rather than
repeating the estimation formula, we build functions to compute the
statistical information. We begin with risk difference.

```{r}
info_rd <- function(N1 = 100, N2 = 100, p1 = .2, p2 = .1, theta = 0){
    d0 <- (theta == 0)
    ratio <- N2 / N1
    a <- 1 + ratio
    b <- -(a + p1 + ratio * p2 + theta * (ratio + 2))
    c <- theta^2 + theta * (2 * p1 + a) + p1 + ratio * p2
    d <- -p1 * theta * (1 + theta)
    v <- (b / (3 * a))^3 - b * c / 6 / a^2 + d / 2 / a
    u <- (sign(v) + (v == 0)) * sqrt((b / 3 / a)^2 - c / 3 / a)
    w <- (pi + acos(v / u^3)) / 3
    p1t <- 2 * u * cos(w) - b / 3 / a
    p2t <- p1t - theta
    p1t[d0] <- (p1[d0] + ratio[d0] * p2[d0]) / (1 + ratio[d0])
    p2t[d0] <- p1t[d0]
    sigmat <- sqrt((p1t * (1 - p1t) + p2t * (1 - p2t) / ratio) * (ratio + 1))
    sigma <- sqrt((p1 * (1 - p1) + p2 * (1 - p2) / ratio) * (ratio + 1))
    return(tibble::tibble(N1 = N1, N2 = N2, ratio = ratio, p1 = p1, p2 = p2, theta = theta, infot = 1/sigmat, info = 1 / sigma))
  }
```

Now we test this:

```{r}
info_rd()
```

Try 3 different $\theta_{rd}$ values:

```{r}
info_rd(theta=c(0,.1,.05))
```


and $\theta_{rd}$ represents the risk difference. We further define the
risk ratio $\theta_{rr}=p_2/p_1=2/3.$ We denote the alternate hypothesis
as
$$, \theta_{rd}^1=p_1^1-p_2^2=0.05, \theta_{rr}^1=\theta_2/\theta_1=2/3.$$

For the null hypothesis, we let

$$H_0: \theta_{rd} = \theta^0_{rd} = p_1^0 - p_2^0$$

$$H_0: p^0_1=p^0_2=(p_1^1+p_2^2)/2= 0.125, \theta_{rd} = \theta^0_{rd}=p_1^0-p_2^0=0 $$
as laid out in @LachinBook.

```{r}
library(gsdmvn)
p0 <- 0.15 # assumed failure rate in control group
p1 <- 0.10 # assumed failure rate in experimental group
alpha <- 0.025 # Design Type I error
beta <- 0.2 # Design Type II error for 80% power
```

We note that had we considered a success outcome such as objective
response in an oncology study, we would let $p_1$ denote the
experimental group and $p_2$ the control group response rate. Thus, we
always set up the notation so the $p_1>p_2$ represents superiority for
the experimental group.

## Notation and Statistical Testing

We let $X_{ij}\sim \hbox{Bernoulli}(p_i),$ $j=1,2,\ldots,n_{ik}$ denote
independent random variables in the control ($i=1$) and experimental
($i=2$) groups through analysis $k=1,2,\ldots,K$ where
$n_{i1} < n_{i2}<\ldots<n_K$. We let $Y_{ik}=\sum_{j=1}^{n_k}X_{ij}$. At
analysis $k=1,2,\ldots$ and for $j=1,2$ we denote the proportion of
failures in group $i$ at analysis $k$

$$ \hat{p}_{ik}=Y_{ik}/n_{ik}.$$ Estimating under null hypothesis
$H_0: p_1=p_2\equiv p_0$ we estimate

$$ \hat{p}_{0k}=\frac{Y_{1k}+ Y_{2k}}{n_{1k}+ n_{2k}}=
\frac{n_{1k}\hat p_{1k} + n_{2k}\hat p_{2k}}{n_{1k} + n_{2k}}. $$ We
note

$$\hbox{Var}(\hat p_{ik})=\frac{p_{i}(1-p_i)}{n_{ik}},$$ and its
consistent estimatator
$$\widehat{\hbox{Var}}(\hat p_{ik})=\frac{\hat p_{ik}(1-\hat p_{ik})}{n_{ik}},$$
$i=1,2$, $k=1,2,\ldots,K$. Letting
$\hat\theta_k=\hat p_{1k}-\hat p_{2k},$ we also have

$$\sigma^2_k\equiv \hbox{Var}(\hat\theta_i)=\frac{p_1(1-p_1)}{n_{1k}}+\frac{p_2(1-p_2)}{n_{2k}},$$
its consistent estimator
$$\hat\sigma^2_k=\frac{\hat p_{1k}(1-\hat p_{1k})}{n_{1k}}+\frac{\hat p_{2k}(1-\hat p_{2k})}{n_{2k}},$$
and the corresponding null hypothesis estimator
$$\hat\sigma^2_{0k}=\hat p_{0k}(1-\hat p_{0k})\left(\frac{1}{n_{1k}}+ \frac{1}{n_{2k}}\right),$$
$k=1,2,\ldots,K$. Statistical information for each of these quantities
and their corresponding estimators are denoted by

$$\begin{align}
\mathcal{I}_k = &1/\sigma^2_k,\\
\mathcal{\hat I}_k = &1/\hat \sigma^2_k,\\
\mathcal{I}_{0k} =& 1/ \sigma^2_{0k},\\
\mathcal{\hat I}_{0k} =& 1/\hat \sigma^2_{0k},
\end{align}$$ $k=1,2,\ldots,K$. Testing, as recommended by @LachinBook,
is done with the large sample test with the null hypothesis variance
estimate and without continuity correction:
$$Z_k = \hat\theta_k/\hat\sigma_{0k}=\frac{\hat p_{1k} - \hat p_{2k}}{\sqrt{(1/n_{1k}+ 1/n_{2k})\hat p_{0k}(1-\hat p_{0k})} },$$
which is asymptotically Normal(0,1) if $p_1=p_2$ and Normal(0,
$\sigma_{0k}^2/\sigma_k^2$) more generally for any $p_1, p_2$,
$k=1,2,\ldots,K$. We note that $\chi^2=Z^2_k$ is the $\chi^2$ test
without continuity correction as recommended by @CCmyth. Note finally
that this extends in a straightforward way the non-inferiority test of
@FarringtonManning if the null hypothesis is
$\theta = p_1 - p_2 - \delta = 0$ for some non-inferiority margin
$\delta > 0$; $\delta < 0$ would correspond to what is referred to as
super-superiority @Chan2002, requiring that experimental therapy has
been shown to be superior to control by at least a margin $-\delta>0$.

## Power Calculations

We begin with a fixed design to simplify. We follow the approach of
@LachinBook to compute power accounting for the different variance
(statistical information) computations under the null hypothesis noted
in the previous section. Noting the asymptotic equivalence

$$Z_k\approx \hat\theta_k/\sigma_{0k}=\frac{\hat p_{1k} - \hat p_{2k}}{\sqrt{(1/n_{1k}+ 1/n_{2k})p_{0}(1- p_0)} }.$$
We denote $n_k=n_{1k}+n_{2k},$ $k=1,2,\ldots, K$ and assume a constant
proportion $\xi_i$ randomized to each group $i=1,2.$ Thus,

$$Z_k\approx \frac{\sqrt{n_k}(\hat p_{1k} - \hat p_{2k})}{\sqrt{(1/\xi_1+ 1/\xi_2)p_{0}(1- p_0)} }.$$

we have the asymptotic distribution

$$Z_k\sim\hbox{Normal}\left(\sqrt{n_k}\frac{p_1 - p_2}{\sqrt{(1/\xi_1+ 1/\xi_2) p_0(1- p_0)} },\sigma^2_{0k}/\sigma^2_{1k}\right).$$
We note that

$$ \sigma^2_{0k}/\sigma^2_{1k} = \frac{ p_0(1-p_0)\left(1/\xi_1+ 1/\xi_2\right)}{p_1(1-p_1)/\xi_1+p_2(1-p_2)/\xi_2}.$$
We also note by definition that
$\sigma^2_{0k}/\sigma^2_{1k}=\mathcal I_k/\mathcal I_{0k}.$ Based on an
input $p_1, p_2, \xi_1, \xi_2 = 1-\xi_1, n_k$ we will compute
$\theta, \mathcal{I}_k, \mathcal{I}_{0k}$, $k=1,2,\ldots,K$.

```{r}
library(tibble)

gs_info_binomial <- function(p1, p2, xi1, n, delta = NULL){
  if (is.null(delta)) delta <- p1 - p2
  # Compute (constant) effect size at each analysis theta
  theta <- rep(p1 - p2, length(n))
  # compute null hypothesis rate, p0
  p0 <- xi1 * p1 + (1 - xi1) * p2
  # compute information based on p1, p2
  info <-  n / (p1 * (1 - p1) / xi1 + p2 * (1 - p2) / (1 - xi1))
  # compute information based on null hypothesis rate of p0
  info0 <- n / (p0 * (1 - p0)*(1 / xi1 + 1 / (1 - xi1)))
  # compute information based on H1 rates of p1star, p2star
  p1star <- p0 + delta * xi1
  p2star <- p0 - delta * (1 - xi1)
  info1 <-  n / (p1star * (1 - p1star) / xi1 + p2star * (1 - p2star) / (1 - xi1))
  return(tibble(Analysis = 1:length(n),
                n = n,
                theta = theta,
                theta1 = rep(delta, length(n)),
                info = info,
                info0 = info0,
                info1 = info1))
}
```

For the CAPTURE trial, we have

```{r}
h1 <- gs_info_binomial(p1 = .15, p2 = .1, xi1 = .5, n = c(350, 700, 1400))
h1
```

Now we examine information for a smaller assumed treatment difference
than the alternative:

```{r}
h <- gs_info_binomial(p1 = .15, p2 = .12, xi1 = .5, delta = .05, n = c(350, 700, 1400))
h
```

We can plug these into `gs_power_npe()` with the intended spending
functions. We begin with power under the alternate hypothesis

```{r}
gs_power_npe(theta = h1$theta, theta1 = h1$theta, info = h1$info,
             info0 = h1$info0, info1 = h1$info1,
             upper = gs_spending_bound,
             upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
             lower = gs_spending_bound,
             lpar = list(sf = gsDesign::sfHSD, param = -2, total_spend = 0.2)
)
```

```{r}
gs_power_npe(theta = h$theta, theta1 = h$theta1, info = h$info,
             info0 = h$info0, info1 = h$info1,
             upper = gs_spending_bound,
             upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
             lower = gs_spending_bound,
             lpar = list(sf = gsDesign::sfHSD, param = -2, total_spend = 0.2)
)
```

## References
